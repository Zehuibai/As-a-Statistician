# Logistic Regression 


## Introduction

Use probability-based linear models to predict qualitative response variables, three methods:

1. Logistic regression
2. Linear discriminant analysis  
3. Multivariate adaptive regression spline 


### Violation of assumptions of Ordinary least squares (OLS) 

The basic assumptions of OLS regression

1. $y_{i}=\alpha+\beta x_{i}+\varepsilon_{i} \mid$
2. $\mathrm{E}\left(\varepsilon_{i}\right)=0$
3. $\operatorname{var}\left(\varepsilon_{i}\right)=\sigma^{2}$
4. $\operatorname{cov}\left(\varepsilon_{i}, \varepsilon_{j}\right)=0$
5. $\varepsilon_{i} \sim$ Normal

**Normal residuals assumption**

Assuming y is a dichotomy, the possible values are 1 or 0. Assume yi = 1. Then hypothesis 1 means ​​εi = 1–α–βxi. On the other hand, if yi = 0, we have εi = –α–βxi. Since εi can only take two values, it is impossible to have a normal distribution


**Consistant variance assumption**

$$E\left(y_{i}\right)=1 \times \operatorname{Pr}\left(y_{i}=1\right)+0 \times \operatorname{Pr}\left(y_{i}=0\right)$$

If we define $p i=\operatorname{Pr}(y i=1)$,Then
$$E\left(y_{i}\right)=p_{i}$$

$$
\begin{array}{c}
E\left(y_{i}\right)=E\left(\alpha+\beta x_{i}+\varepsilon_{i}\right) 
=E(\alpha)+E\left(\beta x_{i}\right)+E\left(\varepsilon_{i}\right) 
=\alpha+\beta x_{i}
\end{array}
$$
Putting these two results together, we get
$$
\begin{array}{c}
p_{i}=\alpha+\beta x_{i} \\
\operatorname{var}\left(\varepsilon_{i}\right)=p_{i}\left(1-p_{i}\right)=\left(\alpha+\beta x_{i}\right)\left(1-\alpha-\beta x_{i}\right)
\end{array}
$$
For different observations, the variance of $ε_i$ must be different, especially as it changes with changes in x. When pi = 0.5, the disturbance variance is the largest, and when pi is close to 1 or 0, the disturbance variance becomes smaller.

**Problems**

If the sample is quite large, the normality assumption is not required. The **central limit theorem** assures us that even if ε is not normally distributed, the coefficient estimates will have an approximately normal distribution. This means that we can still use ordinary tables to calculate p-values and confidence intervals. However, if the sample is small, these approximations may be poor.

<!-- 如果样本相当大，则不需要正态假设。中心极限定理向我们保证，即使ε不呈正态分布，系数估计也将具有近似正态的分布。这意味着我们仍然可以使用普通表来计算p值和置信区间。但是，如果样本较小，则这些近似值可能会很差。 -->

**Violation of the homoscedasticity assumption has two undesirable consequences.**

1. First, the coefficient estimates are no longer **effective**. In statistical terms, this means that there are other selection methods with smaller standard errors.
2. the standard error estimates are no longer consistent estimates of the true standard errors. That means that the estimated standard errors could be biased (either upward or downward) to unknown degrees. And because the standard errors are used in calculating test statistics, the test statistics could also be problematic.

<!-- 首先，系数估计不再有效。用统计术语来说，这意味着存在其他选择方法，它们的标准误差较小。 -->
<!-- 其次，更严重的是，标准误差估计不再是真实标准误差的一致估计。这意味着估计的标准误差可能会偏向（向上或向下）到未知程度。并且由于标准误差用于计算测试统计信息，因此测试统计信息也可能会出现问题。幸运的是，可以轻松解决标准错误和测试统计信息的潜在问题。 -->

**Heteroscedasticity consistent covariance estimator “sandwich”**

Even if the homogeneity assumption is violated, this method will produce a consistent estimate of the standard error. To implement this method in PROC REG, just put the option HCC on the MODEL statement

```
PROC REG DATA=penalty;
  MODEL death=blackd whitvic serious / HCC; 
RUN;
```

<!-- 尽管HCC标准误差很容易解决，但是请注意，它们固有地具有比常规标准误差更大的采样变异性（Kauermann和Carroll 2001），并且在小样本中可能尤其不可靠。但是，对于大样本，它们应该是令人满意的. -->


### More fundamental problem outside [0,1]

For Linear probability model $p_{i}=\alpha+\beta x_{i}$, If x has no upper or lower limit, then for any value of β, there is a value of x whose pi is greater than 1 or less than 0.

### Logistic Regression Model 

Probability is bounded by 0 and 1, while linear functions are inherently unbounded. The solution is to convert the probability so that it is no longer restricted. Converting probabilities to odds eliminates the upper limit. For k explanatory variables
$$\log \left[\frac{p_{i}}{1-p_{i}}\right]=\alpha+\beta_{1} x_{i 1}+\beta_{2} x_{i 2}+\ldots+\beta_{k} x_{i k}$$
$$p_{i}=\frac{\exp \left(\alpha+\beta_{1} x_{i 1}+\beta_{2} x_{i 2}+\ldots+\beta_{k} x_{i k}\right)}{1+\exp \left(\alpha+\beta_{1} x_{i 1}+\beta_{2} x_{i 2}+\ldots+\beta_{k} x_{i k}\right)}$$
$$p_{i}=\frac{1}{1+\exp \left(-\alpha-\beta_{1} x_{i 1}-\beta_{2} x_{i 2}-\ldots-\beta_{k} x_{i k}\right)}$$

<!-- 逻辑模型的方程式中没有随机扰动项。这并不意味着该模型是确定性的，因为pi和yi之间的概率关系仍然存在随机变化的空间。-->

There is **no random disturbance term** in the equation of the logic model. This does not mean that the model is deterministic, because there is still room for random variation in the probability relationship between pi and yi.



### Estimation of the Logistic Model

* ordinary least squares, 
* weighted least squares, 
* maximum likelihood.

> 假设分析单位是商业公司，并且因变量是员工是全职员工的概率。设Pi为在i公司中全职工作的可观察员工比例。
要通过OLS估计逻辑模型，我们可以简单地采用P的logit变换，即log [P/（1-P）]，然后将结果回归到公司特征和员工平均特征上。
加权最小二乘（WLS）分析将类似，不同之处在于将对数据进行加权以针对异方差进行调整。
最大似然（ML）是为分组数据估算逻辑模型的第三种方法，也是一般用于单个级别数据的唯一方法。
利用个人数据，我们只需观察每个人的二分因变量以及该人的测量特征即可。


**ML**


> 最大似然受欢迎程度有两个原因。
1. ML估计量是一致的，渐近有效的并且渐近正态的。
2. 在没有其他明显候选者的情况下，通常很容易得出ML估计量. 为此，有两个步骤：
（1）写下数据概率作为未知参数的函数的表达式，以及（2）找到使该表达式的值尽可能大的未知参数值。


1. **Consistency** means that as the sample size gets larger the probability that the estimate is within some small distance of the true value also gets larger. No matter how small the distance or how high the specified probability, there is always a sample size that yields an even higher probability that the estimator is within that distance of the true value. One implication of consistency is that the ML estimator is approximately unbiased in large samples.
2. **Asymptotic efficiency** means that, in large samples, the estimates will have standard errors that are, approximately, at least as small as those for any other estimation method. And, finally, the sampling distribution of the estimates will be approximately normal in large samples, which means that you can use the normal and chi-square distributions to compute confidence intervals and p-values. All these approximations get better as the sample size gets larger. The fact that these desirable properties have only been proven for large samples does not mean that ML has bad properties for small samples. It simply means that we usually don’t know exactly what the small-sample properties are. And in the absence of attractive alternatives, researchers routinely use ML estimation for both large and small samples.

<!-- 一致性意味着，随着样本数量的增加，估计值在真实值的一小段距离内的可能性也会随之增加。无论距离有多小或指定概率有多高，总有一个样本大小会产生更高的概率，即估计量在真实值的该距离内。一致性的一个暗示是，在大样本中ML估计量几乎是无偏的。 -->

<!-- 渐近效率意味着，在大样本中，估计将具有大约至少与任何其他估计方法一样小的标准误差。最后，在大样本中，估计值的采样分布将近似于正态，这意味着您可以使用正态分布和卡方分布来计算置信区间和p值。随着样本数量的增加，所有这些近似值都会变得更好。这些合意的特性仅在大样本中得到证明的事实并不意味着ML对于小样本具有不良的特性。这仅表示我们通常不确切知道小样本属性是什么。而且，在没有有吸引力的替代方案的情况下，研究人员通常对大型和小型样本都使用ML估计。 -->


**Maximum Likelihood Estimation **

We have data for n individuals (i = 1, ..., n), and these individuals are considered statistically independent. For each i, the data consists of yi and xi, where yi is a random variable with possible values 0 and 1, and xi = [1 xi1...xik]' is a vector of explanatory variables (1 is the intercept).) Let pi The probability of yi = 1
$$p_{i}=\frac{1}{1+e^{-\boldsymbol{\beta} \mathbf{x}_{i}}}$$

The likelihood of observing the values of $y$ for all the observations can be written as
$$L=\operatorname{Pr}\left(y_{1}, y_{2,} \ldots, y_{n}\right)$$

Because we are assuming that observations are independent, the overall probability of observing all the $y_{i}, \mathrm{~s}$ can be factored into the product of the individual probabilities:
$$
L=\operatorname{Pr}\left(y_{1}\right) \operatorname{Pr}\left(y_{2}\right) \ldots \operatorname{Pr}\left(y_{n}\right)=\prod_{i=1}^{n} \operatorname{Pr}\left(y_{i}\right)
$$
By definition, $\operatorname{Pr}\left(y_{i}=1\right)=p_{i}$ and $\operatorname{Pr}\left(y_{i}=0\right)=1-p_{i} .$ That implies that we can write
$$
\begin{array}{c}
\operatorname{Pr}\left(y_{i}\right)=p_{i}^{y_{i}}\left(1-p_{i}\right)^{1-y_{i}} \\
L=\prod_{i=1}^{n} p_{i}^{y_{i}}\left(1-p_{i}\right)^{1-y_{i}}=\prod_{i=1}^{n}\left(\frac{p_{i}}{1-p_{i}}\right)^{y_{i}}\left(1-p_{i}\right) .
\end{array}
$$
At this point we take the logarithm of both sides of the equation to get
$$
\log L=\sum_{i} y_{i} \log \left(\frac{p_{i}}{1-p_{i}}\right)+\sum_{i} \log \left(1-p_{i}\right)
$$
And for equation
$$
\log L=\sum_{i} \boldsymbol{\beta} \mathbf{x}_{i} y_{i}-\sum_{i} \log \left(1+e^{\boldsymbol{\beta} \mathbf{x}_{i}}\right)
$$
Taking the derivative of equation and setting it equal to 0 gives us:
$$
\begin{aligned}
\frac{\partial \log L}{\partial \boldsymbol{\beta}} &=\sum_{i} \mathbf{x}_{i} y_{i}-\sum_{i} \mathbf{x}_{i}\left(1+e^{-\boldsymbol{\beta} \mathbf{x}_{i}}\right)^{-1} \\
&=\sum_{i} \mathbf{x}_{i} y_{i}-\sum_{i} \mathbf{x}_{i} \hat{y}_{i}=0
\end{aligned}
$$
$$\hat{y}_{i}=\frac{1}{1+e^{-\beta \mathbf{x}_{i}}}$$
**Newton-Raphson iterative methods**

There is no clear solution to the equation. Instead, we must rely on iterative methods, which are equivalent to successive approximations to the solution until the approximation "converges" to the solution. Until the approximation "converges" to the correct value. Again, there are many different ways to do this. All methods produce the same solution, but they differ in factors such as convergence speed, sensitivity to initial values, and computational difficulty of each iteration. The Newton-Raphson algorithm is one of the most widely used iterative methods.
$$
\begin{array}{l}
\mathbf{U}(\boldsymbol{\beta})=\frac{\partial \log L}{\partial \boldsymbol{\beta}}=\sum_{i} \mathbf{x}_{i} y_{i}-\sum_{i} \mathbf{x}_{i} \hat{y}_{i} \\
\mathbf{I}(\boldsymbol{\beta})=\frac{\partial^{2} \log L}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^{\prime}}=-\sum_{i} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime} \hat{y}_{i}\left(1-\hat{y}_{i}\right)
\end{array}
$$

<!-- 方程没有明确的解决方案。 取而代之的是，我们必须依靠迭代方法，这些方法等于对解的逐次逼近，直到逼近“收敛”到解。直到逼近“收敛”到正确的值为止。同样，有许多不同的方法可以执行此操作。所有方法都产生相同的解决方案，但是它们在诸如收敛速度，对初始值的敏感性以及每次迭代的计算难度等因素方面有所不同。 牛顿-拉夫森（Newton-Raphson）算法是最广泛使用的迭代方法之一 -->


The Newton-Raphson algorithm is then $$\boldsymbol{\beta}_{j+1}=\boldsymbol{\beta}_{j}-\mathbf{I}^{-1}\left(\boldsymbol{\beta}_{j}\right) \mathbf{U}\left(\boldsymbol{\beta}_{j}\right)$$

We need a set of initial values $\beta_0$. PROC LOGISTIC starts by setting all slope coefficients to 0. Set the intercept to be equal to log [p /(1-p)], where p is the total proportion of events. These initial values are substituted into the right side of the equation, resulting in the result of the first iteration $\beta_1$. Then substitute these values into the right side, recalculate the first and second derivatives, and the result is $\beta_2$ Repeat this process until you get "convergence".

This means that what is inserted on the right is obtained on the left. In fact, you will never get exactly the same thing, so it is necessary to adopt a convergence criterion to judge whether the proximity is close enough. But since every successful run of PROC LOGISTIC reports “Convergence criterion (GCONV=1E-8) satisfied,” 
$$\frac{\mathbf{U}\left(\boldsymbol{\beta}_{j}\right)^{\prime} \mathbf{I}^{-1}\left(\boldsymbol{\beta}_{j}\right) \mathbf{U}\left(\boldsymbol{\beta}_{j}\right)}{\left|\log L\left(\boldsymbol{\beta}_{j}\right)\right|+.000001}$$

If the number is less than .00000001, convergence is declared and the algorithm stops.


### Convergence Problems

> 逻辑模型的最大似然估计是逐次逼近的迭代过程。通常，该过程会顺利进行，无需特别注意。很少需要超过10次迭代才能达到收敛。但是，有时迭代过程会中断，因此无法实现收敛。处理收敛失败可能是逻辑回归用户遇到的更令人沮丧的问题之一。 LOGISTIC的默认限制为25次迭代。如果算法尚未达到此限制，则LOGISTIC会发出警告消息，并在最后一次迭代时打印出结果。尽管可以提高迭代限制（使用MODEL语句中的MAXITER =选项），但这很少能解决问题。未进行25次迭代收敛的模型通常永远不会收敛。在大多数收敛失败的情况下，最大似然估计根本不存在。

**Quasi-complete separation of data points detected.**

> 准完全分离的最常见原因是虚拟预测变量具有以下属性：在虚拟变量的一个级别上，每种情况下因变量都为1或每种情况下都为0。查看任何分类自变量与因变量的交叉分类也非常有帮助。如果您在这些表中的任何一个中发现单元频率为0，则说明了造成准完全分离的原因。找到问题变量后，如何处理

* Recode the problem variables 
* Collapse categories
* Exclude cases from the model

**Retain the model with quasi-complete separation but use likelihood-ratio tests.**

The reported standard error and Wald's chi-square of this variable are also of no avail. Nevertheless, it is still possible to obtain a valid likelihood ratio test, *Profile Likelihood confidence interval.*

```
PROC LOGISTIC DATA=penalty;
 WHERE blackd=0;
 CLASS culp /PARAM=REF;
 MODEL death(EVENT='1') = culp serious / CLPARM=PL ALPHA=.01;
RUN;

##########################################
Parameter Estimate   99% Confidence Limits
culp 1    -15.5467          . -3.2344
```

### Use exact methods. 

> 尽管最大似然具有许多吸引人的属性，但请务必记住，最大似然产生的标准误差和p值是大样本近似值。 在小样本或分离的情况下，准确性可能不如我们想要的那样好。 具有良好的小样本属性的另一种估算方法是“精确逻辑回归”。 这种方法可以看作是费雪（Fisher）对双向列联表的精确检验的概括。 在这种方法中，p值是通过在原假设下枚举所有可能的样本结果来计算的。 即使在完全分离或准完全分离的情况下，精确方法也会产生有效的p值。

```
PROC LOGISTIC DATA=penalty;
 WHERE blackd=0;
 CLASS culp /PARAM=REF;
 MODEL death(EVENT='1') = culp serious;
 EXACT culp serious / ESTIMATE=BOTH;
RUN; 
```


These tests are conditional in the sense that they are based on the conditional distribution of the sufficient statistic for each parameter, conditioning on the sufficient statistics for all the other parameters. (The sufficient statistics are the sums of cross products for each x and the binary dependent variable y.) The tests are exact in the same sense that t-statistics are exact in normal-theory linear regression. That is, they are not large sample approximations, and they give the correct probability of getting a result that is at least as extreme as the one observed in the sample, under the null hypothesis that a variable has no effect.

### Use penalized likelihood

处理拟完全分离的最简单，最有效的方法之一是一种被称为惩罚似然估计的方法，该方法由Firth（1993）引入，因此通常被称为Firth方法。 众所周知，传统的最大似然估计可能会在小样本中产生偏差。 惩罚似然法旨在减少这种偏差，适用于最大似然的广泛应用。 Heinze和Schemper（2002）表明，这种方法在处理准完全分离的情况下特别有效。 运作方式如下。 在牛顿-拉夫森算法（方程3.7）中，一阶导数U（β）

$$\mathbf{U}(\boldsymbol{\beta})=\frac{\partial \log L}{\partial \boldsymbol{\beta}}=\sum_{i} \mathbf{x}_{i} y_{i}-\sum_{i} \mathbf{x}_{i} \hat{y}_{i}-\sum_{i} h_{i} \mathbf{x}_{i}\left(.5-\hat{y}_{i}\right)$$
In PROC LOGISTIC, the method is implemented with the FIRTH option on the MODEL statement

```
PROC LOGISTIC DATA=penalty;
 WHERE blackd=0;
 CLASS culp /PARAM=REF;
 MODEL death(EVENT='1') = culp serious / FIRTH
 CLPARM=PL;
RUN;
```

## Binary Regression

The probability $\pi{i}=\mathrm{P}\left(y{i}=1 \mid x{i 1}, \ldots, x{i k}\right)$ and the linear predictor $\eta_{i}=\beta_{0}+\beta_{1} x_{i 1}+\ldots+\beta_{k} x_{i k}=\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}$ are linked by a response function $\pi_{i}=h\left(\eta_{i}\right) $:

For Logit-Modell:
$$\pi=\frac{\exp (\eta)}{1+\exp (\eta)} \Longleftrightarrow \log \frac{\pi}{1-\pi}=\eta$$
For Probit-Modell:
$$\pi=\Phi(\eta) \Longleftrightarrow \Phi^{-1}(\pi)=\eta$$
For Complementary log-log model:
$$\pi=1-\exp (-\exp (\eta)) \quad \Longleftrightarrow \quad \log (-\log (1-\pi))=\eta$$

## Logit Modell

### Introduction

$$\frac{\mathrm{P}\left(y_{i}=1 \mid \boldsymbol{x}_{i}\right)}{\mathrm{P}\left(y_{i}=0 \mid \boldsymbol{x}_{i}\right)}=\exp \left(\beta_{0}\right) \cdot \exp \left(x_{i 1} \beta_{1}\right) \cdot \ldots \cdot \exp \left(x_{i k} \beta_{k}\right)$$

$$\frac{\mathrm{P}\left(y_{i}=1 \mid x_{i 1}, \ldots\right)}{\mathrm{P}\left(y_{i}=0 \mid x_{i 1}, \ldots\right)} / \frac{\mathrm{P}\left(y_{i}=1 \mid x_{i 1}+1, \ldots\right)}{\mathrm{P}\left(y_{i}=0 \mid x_{i 1}+1, \ldots\right)}=\exp \left(\beta_{1}\right)$$

$$
\begin{array}{l}
\beta_{1}>0: \text {Chance} $\mathrm{P}\left(y_{i}=1\right) / \mathrm{P}\left(y_{i}=0\right) \text {wird größer},\\
\beta_{1}<0: \text { Chance } \mathrm{P}\left(y_{i}=1\right) / \mathrm{P}\left(y_{i}=0\right) \text { wird kleiner, } \\
\beta_{1}=0: \text { Chance } \mathrm{P}\left(y_{i}=1\right) / \mathrm{P}\left(y_{i}=0\right) \text { bleibt gleich. }
\end{array}
$$


### SAS Implementation

```
PROC LOGISTIC DATA=penalty;
 MODEL death(EVENT='1')=blackd whitvic serious;
RUN; 
```

> 在MODEL语句中指定的一个选项是因变量之后的EVENT ='1'。 LOGISTIC中的默认值是估计一个预测因变量最低值的模型。因此，如果我省略了EVENT ='1'，则结果将是一个逻辑模型，预测因变量DEATH等于0的概率。EVENT='1'选项将其反转，以便模型预测因变量等于1。

> 一种等效的（流行的）方法是使用选项DEATH （DESCENDING），它告诉LOGISTIC对DEATH的“较高”值进行建模，而不是对较低值进行建模。但是，较高而不是较低的值取决于所选的其他选项，因此明确建模哪个因变量值较为安全。如果您忘记了EVENT ='1'选项，则唯一的结果就是更改系数的符号。

```
*** For Multiplicative Terms in the MODEL Statement;

MODEL y = x|x|x;
MODEL y = x x*x x*x*x;
```

**class**

> 当CLASS变量作为解释变量包含在MODEL语句中时，LOGISTIC自动创建一组“设计变量”来表示CLASS变量的级别。当预测变量是指示变量（虚拟变量）时，例如仅具有0或1的值，则无需将其声明为CLASS变量。实际上，将指示符变量放在CLASS语句上可能会产生误导性的结果。这是因为CLASS语句可能会以意想不到的方式重新编码变量，正如我们将看到的那样。因此，CLASS语句应保留给具有两个以上类别的分类变量，或保留具有字符值（例如“是”和“否”）的二分变量。

```
PROC LOGISTIC DATA=penalty;
 CLASS culp /PARAM=REF;
 MODEL death(EVENT='1') = blackd whitvic culp ;
RUN;
```

Change the default reference category (5 in this example) and hope it is the minimum value of CULP instead of the maximum value

`CLASS culp / PARAM=REF DESCENDING;`

Particular value, say 3

`CLASS culp(REF='3') / PARAM=REF;`


**Confidence Intervals**

* Wald CI: `CLPARM = WALD`
* Profile likelihood CI: Can produce better approximations, especially in smaller samples using `CLPARM = PL`
* Two confidence intervals: 

```
PROC LOGISTIC DATA=penalty;
 MODEL death(EVENT='1') = blackd whitvic culp / CLPARM=BOTH;
RUN;
```

**Marginal effect**

For each variable, we obtain the predicted change in the probability of death penalty for each additional unit of the variable according to the predicted probability of the person.
Get them easily with PROC QLIM

```
PROC LOGISTIC DATA=penalty;
 MODEL death(EVENT='1')=blackd whitvic serious;
RUN; 

PROC QLIM DATA=penalty;
 ENDOGENOUS death~DISCRETE(DIST=LOGISTIC);
 MODEL death = blackd whitvic serious;
 OUTPUT OUT=a MARGINAL;
PROC PRINT DATA=a(OBS=10);
 VAR meff_p2_blackd meff_p2_whitvic meff_p2_serious;
RUN; 
```

### Multicollinearity
