# Correlation and Regression

## Correlation

The correlation measures the strength of a linear relationship. 

### Pearson correlation coefficient 

用于量度两个变量X和Y之间的线性相关。它具有+1和-1之间的值，其中1是总正线性相关性，0是非线性相关性，并且-1是总负线性相关性。 Pearson相关系数的一个关键数学特性是它在两个变量的位置和尺度的单独变化下是不变的。也就是说，我们可以将X变换为a+bX并将Y变换为c+dY，而不改变相关系数,其中a，b，c和d是常数，b，d >0。 请注意，更一般的线性变换确实会改变相关性。

$${\displaystyle \rho _{X,Y}=\operatorname {corr} (X,Y)={\operatorname {cov} (X,Y) \over \sigma _{X}\sigma _{Y}}={\operatorname {E} [(X-\mu _{X})(Y-\mu _{Y})] \over \sigma _{X}\sigma _{Y}}}$$

$${\displaystyle \rho _{X,Y}={\operatorname {E} (XY)-\operatorname {E} (X)\operatorname {E} (Y) \over {\sqrt {\operatorname {E} (X^{2})-\operatorname {E} (X)^{2}}}\cdot {\sqrt {\operatorname {E} (Y^{2})-\operatorname {E} (Y)^{2}}}}}$$


积差相关系数的适用条件： 在相关分析中首先要考虑的问题就是两个变量是否可能存在相关关系，如果得到了肯定的结论，那才有必要进行下一步定量的分析。另外还必须注意以下几个问题 (前两者的要求最严，第三条比较宽松，违反时系数的结果也是比较稳健的)：

1. 积差相关系数适用于线性相关的情形，对于曲线相关等更为复杂的情形，积差相关系数的大小并不能代表相关性的强弱
2. 样本中存在的极端值对Pearson积差相关系数的影响极大，因此要慎重考虑和处理，必要时可以对其进行剔出，或者加以变量变换，以避免因为一两个数值导致出现错误的结论。 
3. Parson积差相关系数要求相应的变量呈双变量正态分布，注意双变量正态分布并非简单的要求x变量和y变量各自服从正态分布，而是要求服从一个联合的双变量正态分布。
4. Pearson相关系数是在原始数据的方差和协方差基础上计算得到，所以对离群值比较敏感，它度量的是线性相关。因此，即使pearson相关系数为0，也只能说明变量之间不存在线性相关，但仍有可能存在曲线相关。


### Spearman's rank correlation coefficient 

Spearman相关系数和kendall相关系数都是建立在秩和观测值的相对大小的基础上得到，是一种更为一般性的非参数方法，对离群值的敏感度较低，因而也更具有耐受性，度量的主要是变量之间的联系。

Spearman's rank correlation利用两变量的秩次大小作线性相关分析，对原始变量的分布不做要求，属于非参数统计方法。 因此它的适用范围比Pearson相关系数要广的多。即使原始数据是等级资料也可以计算Spearman相关系数。对于服从Pearson相关系数的数据也可以计算Spearman相关系数，但统计效能比Pearson相关系数要低一些（不容易检测出两者事实上存在的相关关系）。

如果数据中没有重复值， 并且当两个变量完全单调相关时，斯皮尔曼相关系数则为+1或−1。Spearman相关系数即使出现异常值，由于异常值的秩次通常不会有明显的变化（比如过大或者过小，那要么排第一，要么排最后），所以对Spearman相关性系数的影响也非常小。

$${\displaystyle r_{s}=\rho _{\operatorname {rg} _{X},\operatorname {rg} _{Y}}={\frac {\operatorname {cov} (\operatorname {rg} _{X},\operatorname {rg} _{Y})}{\sigma _{\operatorname {rg} _{X}}\sigma _{\operatorname {rg} _{Y}}}}}$$


### Kendall rank correlation coefficient

Kendall秩相关系数: 是一种秩相关系数，用于反映分类变量相关性的指标，适用于两个变量均为有序分类的情况.
用希腊字母$\tau$表示其值.Kendall相关系数的取值范围在-1到1之间，当$\tau$为1时，表示两个随机变量拥有一致
的等级相关性；当$\tau$为-1时，表示两个随机变量拥有完全相反的等级相关性；当$\tau$为0时，表示两个随机变量
是相互独立的。

计算公式：Kendall系数是基于协同的思想。对于X,Y的两对观察值$Xi$,$Yi$和$X_j$,$Y_j$,如果$X_j>Y_j$,则称这
两对观察值是和谐的,否则就是不和谐。(和谐的观察值对减去不和谐的观察值对的数量,除以总的观察值对数.), kendall相关系数的计算公式如下:

$${\displaystyle \tau ={\frac {({\text{number of concordant pairs}})-({\text{number of discordant pairs}})}{n \choose 2}}}$$

${\displaystyle {n \choose 2}={n(n-1) \over 2}}$ is the binomial coefficient for the number of ways to 
choose two items from n items.


### Intraclass correlation

当对组织成组的单元进行定量测量时，可以使用该描述性统计。 它描述了同一组中的单元彼此相似的程度。 尽管它被视为一种相关类型，但与大多数其他相关度量不同的是，它对以组为结构的数据进行操作，而不是对以
成对观测值进行结构化的数据。组内相关系数(Intraclass correlation coefficient,ICC) 常用于评价具有某种确定亲属关系的个体间某定量属性的相似程度，另一方面主要应用于评价不同测定方法或
评定者对同一定量测量结果的一致性或可靠性。
 
就其代数形式而言，费舍尔最初的ICC是最类似于Pearson相关系数的ICC。两种统计数据之间的主要区别在于，
在ICC中，数据使用合并的平均值和标准偏差进行居中和缩放。
对于ICC来说，这种合并缩放是有意义的，因为所有测量的数量都是相同的（尽管在不同组中的单位上）。

$${\displaystyle r={\frac {1}{Ns^{2}}}\sum _{n=1}^{N}(x_{n,1}-{\bar {x}})(x_{n,2}-{\bar {x}}),}$$
$${\displaystyle {\bar {x}}={\frac {1}{2N}}\sum _{n=1}^{N}(x_{n,1}+x_{n,2}),}$$
$${\displaystyle s^{2}={\frac {1}{2N}}\left\{\sum _{n=1}^{N}(x_{n,1}-{\bar {x}})^{2}+\sum _{n=1}^{N}(x_{n,2}-{\bar {x}})^{2}\right\}.}$$
在Pearson相关中，每个变量均通过其自身的平均值和标准偏差来居中和缩放。
$$r_{x y}=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}}}$$


### Visualize the correlation in R

```{r Corrplot,echo = T,message = FALSE, error = FALSE, warning = FALSE}
bank<- data.frame(
  y=c(1018.4,1258.9,1359.4,1545.6,1761.6,1960.8),
  x1=c(159,42,95,102,104,108),
  x2=c(223.1,269.4,297.1,330.1,337.9,400.5),
  x3=c(500,370,430,390,330,310),
  x4=c(112.3,146.4,119.9,117.8,122.3,167.0),
  w=c(5,6,8,3,6,8)
)
cor(bank[,c(2,3,4,5)],method = "pearson")

## Visualize the correlation matrix
library(corrplot)
bank.cor <- cor(bank)
corrplot(bank.cor, method = "ellipse")


## devtools::install_github("kassambara/ggcorrplot")
library(ggplot2)
library(ggcorrplot)

## Correlation matrix
data(mtcars)
corr <- round(cor(mtcars), 1)

## Plot
ggcorrplot(corr, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlogram of mtcars", 
           ggtheme=theme_bw)
```


## Ordinary least squares (OLS) Assumpions

$${\displaystyle \mathbf {y} =\mathbf {X} {\boldsymbol {\beta }}+{\boldsymbol {\varepsilon }}\;}  \ \ \ \ {\displaystyle \;{\boldsymbol {\varepsilon }}\sim {\mathcal {N}}(\mathbf {0} ,\sigma ^{2}\mathbf {I} _{T})}.$$
1. $y_{i}=\alpha+\beta x_{i}+\varepsilon_{i}$
2. $\mathrm{E}\left(\varepsilon_{i}\right)=0$
3. $\operatorname{var}\left(\varepsilon_{i}\right)=\sigma^{2}$
4. $\operatorname{cov}\left(\varepsilon_{i}, \varepsilon_{j}\right)=0$
5. $\varepsilon_{i} \sim$ Normal Distribution

Interpretation

* Linearity: The relationship between the predictor variable and the response variable is linear. If the linear relationship cannot be clearly presented, **data conversion** (logarithmic conversion, polynomial conversion, exponential conversion, etc.) can be performed on the variable X or Y
* Residuals are uncorrelated
* Homoscedasticity: The errors are normally distributed and have the same variance. This means that for different input values, the variance of the error is a fixed value. If this hypothesis is violated, the parameter estimation may be biased, leading to the statistical test result of significance is too high or too low, and thus get wrong conclusions. This situation is called heteroscedasticity.
    * 同方差性:误差是正态分布的，并具有相同的方差。这意味着对于不同的输入值，误差 的方差是个固定值。如果违背了这个假设，参数估计就有可能产生偏差，导致对显著性 的统计检验结果过高或者过低，从而得到错误的结论。这种情况就称为异方差性。 
* on-collinearity: There is no linear relationship between the two predictors, that is, there should be no correlation between the features. Similarly, collinearity can also cause estimation bias.
* Outliers: outliers will seriously affect parameter estimation. Ideally, outliers must be removed before using linear regression to fit the model.



## Model Statistics

### Residuals Standard Error

```
## Build Model
y <- c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68)
x1 <- c(10,8,13,9,11,14,6,4,12,7,5)
set.seed(15)
x2 <- sqrt(y)+rnorm(length(y))
model <- lm(y~x1+x2)

## Residual Standard error (Like Standard Deviation)
k <- length(model$coefficients)-1  # Subtract one to ignore intercept
SSE <- sum(model$residuals**2)
n <- length(model$residuals)
Residual_Standard_Error <- sqrt(SSE/(n-(1+k)))
```

### R-Squared and Adjusted R-Squared

**R-Squared Calculation**

R^2^ is a statistic that will give some information about the goodness of fit of a model. In regression, the R^2^ coefficient of determination is a statistical measure of how well the regression predictions approximate the real data points. An R^2^ of 1 indicates that the regression predictions perfectly fit the data.

The total sum of squares
$${\displaystyle SS_{\text{tot}}=\sum _{i}(y_{i}-{\bar {y}})^{2}}$$
The sum of squares of residuals
$${\displaystyle SS_{\text{res}}=\sum _{i}(y_{i}-f_{i})^{2}=\sum _{i}e_{i}^{2}\,}$$
$${\displaystyle R^{2}=1-{SS_{\rm {res}} \over SS_{\rm {tot}}}\,}$$

```
## Build Model
y <- c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68)
x1 <- c(10,8,13,9,11,14,6,4,12,7,5)
set.seed(15)
x2 <- sqrt(y)+rnorm(length(y))
model <- lm(y~x1+x2)

## Multiple R-Squared
SSyy <- sum((y-mean(y))**2)
SSE <- sum(model$residuals**2)

(SSyy-SSE)/SSyy
 
# Alternatively
1-SSE/SSyy
```

**Adjusted R-Squared**

Adjusted R-Squared normalizes Multiple R-Squared by taking into account how many samples you have and how many variables you’re using. 

$${\displaystyle {\bar {R}}^{2}=1-(1-R^{2}){n-1 \over n-p-1}}$$
```
## Adjusted R-Squared
n <- length(y)
k <- length(model$coefficients)-1  # Subtract one to ignore intercept
SSE <- sum(model$residuals**2)
SSyy <- sum((y-mean(y))**2)
1-(SSE/SSyy)*(n-1)/(n-(k+1))
```

### T Statistic  

Null Hypothesis is that the coefficients associated with the variables is equal to zero. The alternate hypothesis is that the coefficients are not equal to zero (i.e. there exists a relationship between the independent variable in question and the dependent variable).

We can interpret the t-value something like this. A larger t-value indicates that it is less likely that the coefficient is not equal to zero purely by chance. So, higher the t-value, the better.

Pr(>|t|) or p-value is the probability that you get a t-value as high or higher than the observed value when the Null Hypothesis (the β coefficient is equal to zero or that there is no relationship) is true. So if the Pr(>|t|) is low, the coefficients are significant (significantly different from zero). If the Pr(>|t|) is high, the coefficients are not significant.

$$t−Statistic = {β−coefficient \over Std.Error}$$

### F Statistic

$\mathrm{H}_{0}: \beta_{1}=\ldots \beta_{\mathrm{p}-1}=0$

$$Std. Error = \sqrt{MSE} = \sqrt{\frac{SSE}{n-q}}$$
$$MSR=\frac{\sum_{i}^{n}\left( \hat{y_{i} - \bar{y}}\right)}{q-1} = \frac{SST - SSE}{q - 1}$$
$$F-statistic = \frac{MSR}{MSE}$$
```
linearMod <- lm(dist ~ speed, data=cars)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
beta.estimate <- modelCoeffs["speed", "Estimate"]  # get beta estimate for speed
std.error <- modelCoeffs["speed", "Std. Error"]  # get std.error for speed

t_value <- beta.estimate/std.error  # calc t statistic

p_value <- 2*pt(-abs(t_value), df=nrow(cars)-ncol(cars))  # calc p Value

f_statistic <- modelSummary$fstatistic[1]  # fstatistic

f <- summary(linearMod)$fstatistic  # parameters for model p-value calc
model_p <- pf(f[1], f[2], f[3], lower=FALSE)


## For Calculation
data(savings)
g < - 1m (sr ˜ pop15 + pop75 + dpi + ddpi, savings)
summary (g)

## Test Beta1 = Beta2 = Beta3 = Beta4 = 0
(tss < - sum((savings$sr-mean (savings$sr))^2))
(rss < - deviance(g))
(fstat < - ((tss-rss)/4)/(rss/df.residual(g)))
## F Test
1-pf (fstat, 4, df.residual (g)) 
```

**Model Comparasion**

```
g2 < - 1m (sr ˜ pop75 + dpi + ddpi, savings)

## d compute the RSS and the F-statistic:
(rss2 < - deviance (g2))
(fstat < - (deviance (g2)-deviance (g))/(deviance (g)/df.residual(g)))
## P value
1-pf (fstat, l, df.residual(g))

## relate this to the t-based test and p-value by:
sqrt (fstat) 
(tstat < - summary(g)$coef[2, 3])
2 * (l-pt (sqrt (fstat), 45))

## more convenient way to compare two nested models is:
anova (g2, g)

## Analysis of Variance Table
## Model 1: sr ˜ pop75 + dpi + ddpi
## Model 2: sr ˜ pop15 + pop75 + dpi + ddpi
```


### Confidence Intervals 

**Confidence Intervals for $\beta$**

$$\hat{\beta}_{i} \pm t_{n-p}^{(\alpha / 2)} \hat{\sigma} \sqrt{\left(X^{T} X\right)_{i i}^{-1}}$$

Alternatively, a $100(1-\alpha) \%$ confidence region for $\beta$ satisfies:

$$(\hat{\beta}-\beta)^{T} X^{T} X(\hat{\beta}-\beta) \leq p \hat{\sigma}^{2} F_{p, n-p}^{(\alpha)}$$

**Confidence Intervals for Predictions**

必须区分对未来均值响应的预测和对未来观测值的预测。

1. Suppose a specific house comes on the market with characteristics $x_{0}$. Its selling price will be $x_{0}^{T} \beta+\varepsilon$ since $\mathrm{E} \varepsilon=0$, the predicted price is $x_{0}^{T} \hat{\beta}$ but in assessing the variance of this
prediction, we must include the variance of $\varepsilon .$ (评估此预测的方差时，我们必须包括 $\varepsilon$ 的方差 $_{\circ}$ )
2. Suppose we ask the question - What would a house with characteristics $x_{0}$ sell for on average?" This selling price is $x_{0}^{T} \beta$ and is again predicted by $x_{0}^{T} \hat{\beta}$ but now only the variance in $\hat{\beta}$ needs to be taken into account.(平均售价只需要考虑 $\hat{\beta}$ 的方差)


A 100(1–%) % CI for a single future response is:$$\hat{y}_{0} \pm t_{n-p}^{(\alpha / 2)} \hat{\sigma} \sqrt{1+x_{0}^{T}\left(X^{T} X\right)^{-1} x_{0}}$$


A CI for the mean response for given $x_0$ is $$\hat{y}_{0} \pm t_{n-p}^{(\alpha / 2)} \hat{\sigma} \sqrt{x_{0}^{T}\left(X^{T} X\right)^{-1} x_{0}}$$


### Likelihood-ratio test

likelihood-ratio test assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods, specifically one found by maximization over the entire parameter space and another found after imposing some constraint. If the constraint (i.e., the null hypothesis) is supported by the observed data, the two likelihoods should not differ by more than sampling error.

Suppose that we have a statistical model with parameter space ${\displaystyle \Theta }$. 

* A null hypothesis is often stated by saying that the parameter ${\displaystyle \theta }$ is in a specified subset ${\displaystyle \Theta _{0}}$ of ${\displaystyle \Theta }$. 
* The alternative hypothesis is thus that ${\displaystyle \theta }$ is in the complement of ${\displaystyle \Theta _{0}}$

$${\displaystyle \lambda _{\text{LR}}=-2\ln \left[{\frac {~\sup _{\theta \in \Theta _{0}}{\mathcal {L}}(\theta )~}{~\sup _{\theta \in \Theta }{\mathcal {L}}(\theta )~}}\right]}$$

Often the likelihood-ratio test statistic is expressed as a difference between the log-likelihoods
$${\displaystyle \lambda _{\text{LR}}=-2\left[~\ell (\theta _{0})-\ell ({\hat {\theta }})~\right]}$$
$${\displaystyle \ell ({\hat {\theta }})\equiv \ln \left[~\sup _{\theta \in \Theta }{\mathcal {L}}(\theta )~\right]~}$$





### Accuracy

**Accuracy**: A simple correlation between the actuals and predicted values can be used as a form of accuracy measure. A higher correlation accuracy implies that the actuals and predicted values have similar directional movement, i.e. when the actuals values increase the predicteds also increase and vice-versa.
$$\text{Min Max Accuracy} = mean \left( \frac{min\left(actuals, predicteds\right)}{max\left(actuals, predicteds \right)} \right)$$
$$\text{Mean Absolute Percentage Error \ (MAPE)} = mean\left( \frac{abs\left(predicteds−actuals\right)}{actuals}\right)$$

```
Step 1: Create the training (development) and test (validation) data samples from original data.
Step 2: Develop the model on the training data and use it to predict the distance on test data
Step 3: Review diagnostic measures.
Step 4: Calculate prediction accuracy and error rates

# Create Training and Test data -
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))  # row indices for training data
trainingData <- cars[trainingRowIndex, ]  # model training data
testData  <- cars[-trainingRowIndex, ]   # test data

# Build the model on training data -
lmMod <- lm(dist ~ speed, data=trainingData)  # build the model
distPred <- predict(lmMod, testData)  # predict distance

# Review diagnostic measures.
summary (lmMod)  # model summary
AIC (lmMod)  # Calculate akaike information criterion

# Calculate prediction accuracy and error rates
actuals_preds <- data.frame(cbind(actuals=testData$dist, predicteds=distPred))  # make actuals_predicteds dataframe.
correlation_accuracy <- cor(actuals_preds)  
correlation_accuracy

# Now lets calculate the Min Max accuracy and MAPE:
# 计算最小最大精度和MAPE：
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))  
min_max_accuracy

# Mean Absolute Percentage Error 
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)  
mape
```


## Model Assumptions Checks



## SAS implementation Proc Reg

### Options

|  Options         |  Descriptation       |
|------------------|-----------------------------------------------------------------------------------|
| STB              | 输出标准化偏回归系数矩阵     |
| CORRB            | 输出参数估计矩阵        |
| COLLINOINT       | 自变量进行共线性分析       |
| P                | 输出个体观测值、预测值及残差        |
| R                | 输出每个个体观测值、残差及标准误差         |
| CLM              | 输出因变量均值95％的置信界限的上下限      |
| CLI              | 对各预测值输出95％的置信界限的上下限     |
| MSE              | 要求输出随机扰动项方差           |
| VIF              | 输出变量间相关性的方差膨胀系数，VIF越大，说明由于共线性存在，使方差变大  |
| TOL              | 表示共线性水平的容许值，TOL越小说明其可用别的自变量解释的部分多，
                            自然可能与别的自变量存在共线性关系； |
| DW               | 输出Durbin-Watson统计量|
| influence        | 对异常点进行诊断，对每一观测点输出统计量(Cook's D > 50％, 
                            defits/debetas > 2说明该点影响较大） |
| Plot Options     |                                              |
| FITPLOT          | 带回归线、置信预测带的散点图       |
| RESIDUALS        | 自变量的残差                       |
| DIAGNOSTICS      | 诊断图（包括下面各图）             |
| COOKSD           | CookIs D统计量图                   |
| OBSERVEDBYPREDICTED     | 根据预测值的因变量图        |
| QQPLOT           | 检验残差正态性的图                 |
| RESIDUALBYPREDICTED     | 根据预测值的残差图          |
| RESIDUALHISTOGRAM       | 残差的直方图                |
| RFPLOT                  | 残差拟合图                  |
| RSTUDENTBYLEVERAGE      | 杠杆比率的学生化残差图      |
| RSTUDENTBYPREDICTED     | 预测值的学生化残差图        |





## R implementation 


|  Title             | Function                               |
|--------------------|----------------------------------------|
| 拟合模型           | bank.lm <- lm(y~x1+x2+x3+x4,data=bank) |
| 预测值             | fitted(bank.lm)                        |
| 求模型系数         | coef(bank.lm)                          |
| 计算残差平方和     | deviance(bank.lm)                      |
| 计算残差           | residuals(bank.lm)                     |
| 方差分析表         | anova(bank.lm)                         |
| Konfidenzintervall | confint(bank.lm, level=0.95)           |



