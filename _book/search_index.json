[["time-series-analysis.html", "Chapter 29 Time Series Analysis 29.1 Fundational Concepts 29.2 Autoregressive moving average (ARMA) models 29.3 Stationary Models", " Chapter 29 Time Series Analysis 29.1 Fundational Concepts The purpose of time series analysis is generally twofold: to understand or model the stochastic mechanism that gives rise to an observed series and to predict or forecast the future values of a series based on the history of that series and, possibly, other related series or factors. 29.1.1 Means, Variances, and Covariances For a stochastic process \\[\\left\\{Y_{t}: t=0, \\pm 1, \\pm 2, \\pm 3, \\ldots\\right\\},\\] The autocovariance function, \\(\\gamma_{t, s}\\), is defined as \\[ \\gamma_{t, s}=\\operatorname{Cov}\\left(Y_{t}, Y_{s}\\right) \\quad \\text { for } t, s=0, \\pm 1, \\pm 2, \\ldots \\] where \\[\\operatorname{Cov}\\left(Y_{t}, Y_{s}\\right)=E\\left[\\left(Y_{t}-\\mu_{t}\\right)\\left(Y_{s}-\\mu_{s}\\right)\\right]=E\\left(Y_{t} Y_{s}\\right)-\\mu_{t} \\mu_{s}\\] The autocorrelation function, \\(\\rho_{t, s}\\), is given by \\[ \\rho_{t, s}=\\operatorname{Corr}\\left(Y_{t}, Y_{s}\\right) \\quad \\text { for } t, s=0, \\pm 1, \\pm 2, \\ldots \\] where \\[ \\operatorname{Corr}\\left(Y_{t}, Y_{s}\\right)=\\frac{\\operatorname{Cov}\\left(Y_{t}, Y_{s}\\right)}{\\sqrt{\\operatorname{Var}\\left(Y_{t}\\right) \\operatorname{Var}\\left(Y_{s}\\right)}}=\\frac{\\gamma_{t, s}}{\\sqrt{\\gamma_{t, t} \\gamma_{s, s}}} \\] The following important properties follow from known results and our definitions: \\[ \\left.\\begin{array}{ll} \\gamma_{t, t}=\\operatorname{Var}\\left(Y_{t}\\right) &amp; \\rho_{t, t}=1 \\\\ \\gamma_{t, s}=\\gamma_{s, t} &amp; \\rho_{t, s}=\\rho_{s, t} \\\\ \\left|\\gamma_{t, s}\\right| \\leq \\sqrt{\\gamma_{t, t} \\gamma_{s, s}} &amp; \\left|\\rho_{t, s}\\right| \\leq 1 \\end{array}\\right\\} \\] Values of \\(\\rho_{t, s}\\) near \\(\\pm 1\\) indicate strong (linear) dependence, whereas values near zero indicate weak (linear) dependence. If \\(\\rho_{t, s}=0\\), we say that \\(Y_{t}\\) and \\(Y_{s}\\) are uncorrelated. 29.1.2 Properties of covariance To investigate the covariance properties of various time series models, the following result will be used repeatedly: If \\(c_{1}, c_{2}, \\ldots, c_{m}\\) and \\(d_{1}, d_{2}, \\ldots, d_{n}\\) are constants and \\(t_{1}\\), \\(t_{2}, \\ldots, t_{m}\\)$ and $\\(s_{1}, s_{2}, \\ldots, s_{n}\\) are time points, then \\[ \\operatorname{Cov}\\left[\\sum_{i=1}^{m} c_{i} Y_{t_{i}}, \\sum_{j=1}^{n} d_{j} Y_{s_{j}}\\right]=\\sum_{i=1}^{m} \\sum_{j=1}^{n} c_{i} d_{j} \\operatorname{Cov}\\left(Y_{t_{i}}, Y_{s_{j}}\\right) \\] As a special case, we obtain the well-known result \\[ \\operatorname{Var}\\left[\\sum_{i=1}^{n} c_{i} Y_{t_{i}}\\right]=\\sum_{i=1}^{n} c_{i}^{2} \\operatorname{Var}\\left(Y_{t_{i}}\\right)+2 \\sum_{i=2}^{n} \\sum_{j=1}^{i-1} c_{i} c_{j} \\operatorname{Cov}\\left(Y_{t_{i}}, Y_{t_{j}}\\right) \\] Forthermore: \\[ \\begin{array}{c} \\operatorname{Cov}(a+b X, c+d Y)=b d \\operatorname{Cov}(X, Y) \\\\ \\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)+2 \\operatorname{Cov}(X, Y) \\\\ \\operatorname{Cov}(X+Y, Z)=\\operatorname{Cov}(X, Z)+\\operatorname{Cov}(Y, Z) \\\\ \\operatorname{Cov}(X, X)=\\operatorname{Var}(X) \\\\ \\operatorname{Cov}(X, Y)=\\operatorname{Cov}(Y, X) \\end{array} \\] If \\(X\\) and \\(Y\\) are independent, \\[ \\operatorname{Cov}(X, Y)=0 \\] The correlation coefficient of \\(X\\) and \\(Y\\), denoted by \\(\\operatorname{Cor}(X, Y)\\) or \\(\\rho\\), is defined as \\[ \\rho=\\operatorname{Corr}(X, Y)=\\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)}} \\] Alternatively, if \\(X^{*}\\) is a standardized \\(X\\) and \\(Y^{*}\\) is a standardized \\(Y\\), then \\(\\rho=E\\left(X^{*} Y^{*}\\right) .\\) 29.1.3 Properties of Expectation If \\(h(x)\\) is a function such that \\(\\int_{-\\infty}^{\\infty}|h(x)| f(x) d x&lt;\\infty\\), it may be shown that \\[ E[h(X)]=\\int_{-\\infty}^{\\infty} h(x) f(x) d x \\] Similarly, if \\(\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty}|h(x, y)| f(x, y) d x d y&lt;\\infty\\), it may be shown that \\[ E[h(X, Y)]=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} h(x, y) f(x, y) d x d y \\] As a corollary, we easily obtain the important result \\[ E(a X+b Y+c)=a E(X)+b E(Y)+c \\] We also have \\[ E(X Y)=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y f(x, y) d x d y \\] The variance of a random variable \\(X\\) is defined as \\[ \\operatorname{Var}(X)=E\\left\\{[X-E(X)]^{2}\\right\\} \\] (provided \\(E\\left(X^{2}\\right)\\) exists). The variance of \\(X\\) is often denoted by \\(\\sigma^{2}\\) or \\(\\sigma_{X}^{2}\\) 29.1.4 Properties of Variance \\[ \\begin{array}{c} \\operatorname{Var}(X) \\geq 0 \\\\ \\operatorname{Var}(a+b X)=b^{2} \\operatorname{Var}(X) \\end{array} \\] If \\(X\\) and \\(Y\\) are independent, then \\[ \\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y) \\] In general, it may be shown that \\[ \\operatorname{Var}(X)=E\\left(X^{2}\\right)-[E(X)]^{2} \\] The positive square root of the variance of \\(X\\) is called the standard deviation of \\(X\\) and is often denoted by \\(\\sigma\\) or \\(\\sigma_{X}\\). The random variable \\(\\left(X-\\mu_{X}\\right) / \\sigma_{X}\\) is called the standardized version of \\(X .\\) The mean and standard deviation of a standardized variable are always zero and one, respectively. The covariance of \\(X\\) and \\(Y\\) is defined as \\(\\operatorname{Cov}(X, Y)=E\\left[\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right]\\). 29.1.5 The Random Walk Let \\(e_{1}, e_{2}, \\ldots\\) be a sequence of independent, identically distributed random variables each with zero mean and variance \\(\\sigma_{e}^{2} .\\) The observed time series, \\(\\left\\{Y_{t}: t=1,2, \\ldots\\right\\}\\), is constructed as follows: \\[ \\left.\\begin{array}{rl} Y_{1} &amp; =e_{1} \\\\ Y_{2} &amp; =e_{1}+e_{2} \\\\ &amp; \\vdots \\\\ Y_{t} &amp; =e_{1}+e_{2}+\\cdots+e_{t} \\end{array}\\right\\} \\] Alternatively, we can write \\[ Y_{t}=Y_{t-1}+e_{t} \\] with initial condition \\(Y_{1}=e_{1} .\\) 29.1.5.1 The mean function is \\[ \\begin{aligned} \\mu_{t} &amp;=E\\left(Y_{t}\\right)=E\\left(e_{1}+e_{2}+\\cdots+e_{t}\\right)=E\\left(e_{1}\\right)+E\\left(e_{2}\\right)+\\cdots+E\\left(e_{t}\\right) \\\\ &amp;=0+0+\\cdots+0 \\end{aligned} \\] so that \\[ \\mu_{t}=0 \\quad \\text { for all } t \\] 29.1.5.2 The variance function is \\[ \\begin{aligned} \\operatorname{Var}\\left(Y_{t}\\right) &amp;=\\operatorname{Var}\\left(e_{1}+e_{2}+\\cdots+e_{t}\\right)=\\operatorname{Var}\\left(e_{1}\\right)+\\operatorname{Var}\\left(e_{2}\\right)+\\cdots+\\operatorname{Var}\\left(e_{t}\\right) \\\\ &amp;=\\sigma_{e}^{2}+\\sigma_{e}^{2}+\\cdots+\\sigma_{e}^{2} \\end{aligned} \\] so that \\[ \\operatorname{Var}\\left(Y_{t}\\right)=t \\sigma_{e}^{2} \\] 29.1.5.3 The covariance function is Notice that the process variance increases linearly with time. To investigate the covariance function, suppose that \\(1 \\leq t \\leq s .\\) Then we have \\[ \\gamma_{t, s}=\\operatorname{Cov}\\left(Y_{t}, Y_{s}\\right)=\\operatorname{Cov}\\left(e_{1}+e_{2}+\\cdots+e_{t}, e_{1}+e_{2}+\\cdots+e_{t}+e_{t+1}+\\cdots+e_{s}\\right) \\] From Equation above, we have \\[ \\gamma_{t, s}=\\sum_{i=1}^{s} \\sum_{j=1}^{t} \\operatorname{Cov}\\left(e_{i}, e_{j}\\right) \\] However, these covariances are zero unless \\(i=j\\), in which case they equal \\(\\operatorname{Var}\\left(e_{i}\\right)=\\sigma_{e}^{2}\\). There are exactly \\(t\\) of these so that \\(\\gamma_{t, s}=t \\sigma_{e}^{2}\\). 29.1.5.4 The autocorrelation function Since \\(\\gamma_{t, s}=\\gamma_{s, t}\\), this specifies the autocovariance function for all time points \\(t\\) and \\(s\\) and we can write \\[ \\gamma_{t, s}=t \\sigma_{e}^{2} \\] for \\(1 \\leq t \\leq s\\) The autocorrelation function for the random walk is now easily obtained as \\[ \\rho_{t, s}=\\frac{\\gamma_{t, s}}{\\sqrt{\\gamma_{t, t} \\gamma_{s, s}}}=\\sqrt{\\frac{t}{s}} \\] 29.1.6 A Moving Average As a second example, suppose that \\(\\left\\{Y_{t}\\right\\}\\) is constructed as \\[ Y_{t}=\\frac{e_{t}+e_{t-1}}{2} \\] where (as always throughout this book) the \\[e\\] s are assumed to be independent and identically distributed with zero mean and variance \\(\\sigma_{e}^{2} .\\) Here \\[ \\begin{aligned} \\mu_{t} &amp;=E\\left(Y_{t}\\right)=E\\left\\{\\frac{e_{t}+e_{t-1}}{2}\\right\\}=\\frac{E\\left(e_{t}\\right)+E\\left(e_{t-1}\\right)}{2} =0 \\end{aligned} \\] \\[ \\begin{aligned} \\operatorname{Var}\\left(Y_{t}\\right) &amp;=\\operatorname{Var}\\left\\{\\frac{e_{t}+e_{t-1}}{2}\\right\\}=\\frac{\\operatorname{Var}\\left(e_{t}\\right)+\\operatorname{Var}\\left(e_{t-1}\\right)}{4} \\\\ &amp;=0.5 \\sigma_{e}^{2} \\end{aligned} \\] \\[ \\begin{aligned} \\operatorname{Cov}\\left(Y_{t}, Y_{t-1}\\right)=&amp; \\operatorname{Cov}\\left\\{\\frac{e_{t}+e_{t-1}}{2}, \\frac{e_{t-1}+e_{t-2}}{2}\\right\\} \\\\ =&amp; \\frac{\\operatorname{Cov}\\left(e_{t}, e_{t-1}\\right)+\\operatorname{Cov}\\left(e_{t}, e_{t-2}\\right)+\\operatorname{Cov}\\left(e_{t-1}, e_{t-1}\\right)}{4} +\\frac{\\operatorname{Cov}\\left(e_{t-1}, e_{t-2}\\right)}{4} \\\\ =&amp; \\frac{\\operatorname{Cov}\\left(e_{t-1}, e_{t-1}\\right)}{4} \\quad \\text { (as all the other covariances are zero) } \\\\ =&amp; 0.25 \\sigma_{e}^{2} \\end{aligned} \\] Or \\[ \\gamma_{t, t-1}=0.25 \\sigma_{e}^{2} \\] for all \\(t\\). Furthermore, \\[ \\begin{aligned} \\operatorname{Cov}\\left(Y_{t}, Y_{t-2}\\right) &amp;=\\operatorname{Cov}\\left\\{\\frac{e_{t}+e_{t-1}}{2}, \\frac{e_{t-2}+e_{t-3}}{2}\\right\\} \\\\ &amp;=0 \\quad \\text { since the } e^{\\prime} \\text { s are independent } \\end{aligned} \\] Similarly, \\(\\operatorname{Cov}\\left(Y_{t}, Y_{t-k}\\right)=0\\) for \\(k&gt;1\\), so we may write \\[ \\gamma_{t, s}=\\left\\{\\begin{array}{cc} 0.5 \\sigma_{e}^{2} &amp; \\text { for }|t-s|=0 \\\\ 0.25 \\sigma_{e}^{2} &amp; \\text { for }|t-s|=1 \\\\ 0 &amp; \\text { for }|t-s|&gt;1 \\end{array}\\right. \\] 29.1.7 Strictly Stationarity assumption is that of stationarity. The basic idea of stationarity is that the probability laws that govern the behavior of the process do not change over time. In a sense, the process is in statistical equilibrium. Specifically, a process \\(\\left\\{Y_{t}\\right\\}\\) is said to be strictly stationary if the joint distribution of \\(Y_{t_{1}}, Y_{t_{2}}, \\ldots, Y_{t_{n}}\\) is the same as the joint distribution of \\(Y_{t_{1}-k} Y_{t_{2}-k}, \\ldots, Y_{t_{n}-k}\\) for all choices of time points \\(t_{1}, t_{2}, \\ldots, t_{n}\\) and all choices of time lag \\(k\\). Thus, when \\(n=1\\) the (univariate) distribution of \\(Y_{t}\\) is the same as that of \\(Y_{t-k}\\) for all \\(t\\) and \\(k\\); in other words, the \\(Y\\) s are (marginally) identically distributed. It then follows that \\(E\\left(Y_{t}\\right)=E\\left(Y_{t-k}\\right)\\) for all \\(t\\) and \\(k\\) so that the mean function is constant for all time. Additionally, \\(\\operatorname{Var}\\left(Y_{t}\\right)=\\operatorname{Var}\\left(Y_{t-k}\\right)\\) for all \\(t\\) and \\(k\\) so that the variance is also constant over time. Setting \\(n=2\\) in the stationarity definition we see that the bivariate distribution of \\(Y_{t}\\) and \\(Y_{s}\\) must be the same as that of \\(Y_{t-k}\\) and \\(Y_{s-k}\\) from which it follows that \\(\\operatorname{Cov}\\left(Y_{t}, Y_{s}\\right)\\) \\(=\\operatorname{Cov}\\left(Y_{t-k}, Y_{s-k}\\right)\\) for all \\(t, s\\), and \\(k\\). Putting \\(k=s\\) and then \\(k=t\\), we obtain \\[ \\begin{aligned} \\gamma_{t, s} &amp;=\\operatorname{Cov}\\left(Y_{t-s}, Y_{0}\\right) \\\\ &amp;=\\operatorname{Cov}\\left(Y_{0}, Y_{s-t}\\right) \\\\ &amp;=\\operatorname{Cov}\\left(Y_{0}, Y_{|t-s|}\\right) \\\\ &amp;=\\gamma_{0,|t-s|} \\end{aligned} \\] That is, the covariance between \\(Y_{t}\\) and \\(Y_{s}\\) depends on time only through the time difference \\(|t-s|\\) and not otherwise on the actual times \\(t\\) and \\(s\\). Thus, for a stationary process, we can simplify our notation and write \\[ \\gamma_{k}=\\operatorname{Cov}\\left(Y_{t}, Y_{t-k}\\right) \\quad \\text { and } \\quad \\rho_{k}=\\operatorname{Corr}\\left(Y_{t}, Y_{t-k}\\right) \\] Note also that \\[ \\rho_{k}=\\frac{\\gamma_{k}}{\\gamma_{0}} \\] The general properties given in Equation now become \\[ \\left.\\begin{array}{ll} \\gamma_{0}=\\operatorname{Var}\\left(Y_{t}\\right) &amp; \\rho_{0}=1 \\\\ \\gamma_{k}=\\gamma_{-k} &amp; \\rho_{k}=\\rho_{-k} \\\\ \\left|\\gamma_{k}\\right| \\leq \\gamma_{0} &amp; \\left|\\rho_{k}\\right| \\leq 1 \\end{array}\\right\\} \\] If a process is strictly stationary and has finite variance, then the covariance function must depend only on the time lag. 29.1.8 Weakly (or second-order) stationary A stochastic process \\[\\left\\{Y_{t}\\right\\}\\]** is said to be weakly (or second-order) stationary if The mean function is constant over time, and for all time \\[t\\] and lag \\(k\\) \\[ \\gamma_{t, t-k}=\\gamma_{0, k} \\] 29.1.9 White Noise A very important example of a stationary process is the so-called white noise process, which is defined as a sequence of independent, identically distributed random variables \\(\\left\\{e_{t}\\right\\}\\). Its importance stems not from the fact that it is an interesting model itself but from the fact that many useful processes can be constructed from white noise. The fact that \\(\\left\\{e_{t}\\right\\}\\) is strictly stationary is easy to see since \\(\\operatorname{Pr}\\left(e_{t_{1}} \\leq x_{1}, e_{t_{2}} \\leq x_{2}, \\ldots, e_{t_{n}} \\leq x_{n}\\right)\\) \\(=\\operatorname{Pr}\\left(e_{t_{1}} \\leq x_{1}\\right) \\operatorname{Pr}\\left(e_{t_{2}} \\leq x_{2}\\right) \\cdots \\operatorname{Pr}\\left(e_{t_{n}} \\leq x_{n}\\right) \\quad\\) (by independence) \\(=\\operatorname{Pr}\\left(e_{t_{1}-k} \\leq x_{1}\\right) \\operatorname{Pr}\\left(e_{t_{2}-k} \\leq x_{2}\\right) \\cdots \\operatorname{Pr}\\left(e_{t_{n}-k} \\leq x_{n}\\right)\\) \\(=\\operatorname{Pr}\\left(e_{t_{1}-k} \\leq x_{1}, e_{t_{2}-k} \\leq x_{2}, \\ldots, e_{t_{n}-k} \\leq x_{n}\\right)\\) (identical distributions) (by independence) as required. Also, \\(\\mu_{t}=E\\left(e_{t}\\right)\\) is constant and \\[ \\gamma_{k}=\\left\\{\\begin{array}{cl} \\operatorname{Var}\\left(e_{t}\\right) &amp; \\text { for } k=0 \\\\ 0 &amp; \\text { for } k \\neq 0 \\end{array}\\right. \\] Alternatively, we can write \\[ \\rho_{k}=\\left\\{\\begin{array}{ll} 1 &amp; \\text { for } k=0 \\\\ 0 &amp; \\text { for } k \\neq 0 \\end{array}\\right. \\] The term white noise arises from the fact that a frequency analysis of the model shows that, in analogy with white light, all frequencies enter equally. We usually assume that the white noise process has mean zero and denote \\(\\operatorname{Var}\\left(e_{t}\\right)\\) by \\(\\sigma_{e}^{2}\\). The moving average example, \\(Y_{t}=\\left(e_{t}+e_{t-1}\\right) / 2\\), is another example of a stationary process constructed from white noise. In our new notation, we have for the moving average process that \\[ \\rho_{k}=\\left\\{\\begin{array}{ll} 1 &amp; \\text { for } k=0 \\\\ 0.5 &amp; \\text { for }|k|=1 \\\\ 0 &amp; \\text { for }|k| \\geq 2 \\end{array}\\right. \\] 29.1.10 Deterministic Versus Stochastic Trends In a general time series, the mean function is a totally arbitrary function of time. In a stationary time series, the mean function must be constant in time. Frequently we need to take the middle ground and consider mean functions that are relatively simple (but not constant) functions of time. We might assume that \\(X_{t}\\), the unobserved variation around \\(\\mu_{t}\\), has zero mean for all \\(t\\) so that indeed \\(\\mu_{t}\\) is the mean function for the observed series \\(Y_{t} .\\) We could describe this model as having a deterministic trend as opposed to the stochastic trend considered earlier. In other situations we might hypothesize a deterministic trend that is linear in time (that is, \\(\\left.\\mu_{t}=\\beta_{0}+\\beta_{1} t\\right)\\) or perhaps a quadratic time trend, \\(\\mu_{t}=\\beta_{0}+\\beta_{1} t+\\beta_{2} t^{2}\\). Note that an implication of the model \\(Y_{t}=\\mu_{t}+X_{t}\\) with \\(E\\left(X_{t}\\right)=0\\) for all \\(t\\) is that the deterministic trend \\(\\mu_{t}\\) applies for all time. Thus, if \\(\\mu_{t}=\\beta_{0}+\\beta_{1} t\\), we are assuming that the same linear time trend applies forever. We should therefore have good reasons for assuming such a model-not just because the series looks somewhat linear over the time period observed. 29.2 Autoregressive moving average (ARMA) models 29.2.1 General Linear Processes A general linear process, \\(\\left\\{Y_{t}\\right\\}\\), is one that can be represented as a weighted linear combination of present and past white noise terms as \\[ Y_{t}=e_{t}+\\psi_{1} e_{t-1}+\\psi_{2} e_{t-2}+\\cdots \\] If the right-hand side of this expression is truly an infinite series, then certain conditions must be placed on the \\(\\psi\\)-weights for the right-hand side to be meaningful mathematically. For our purposes, it suffices to assume that \\[ \\sum_{i=1}^{\\infty} \\psi_{i}^{2}&lt;\\infty \\] We should also note that since \\(\\left\\{e_{t}\\right\\}\\) is unobservable, there is no loss in the generality of Equation (4.1.2) if we assume that the coefficient on \\(e_{t}\\) is 1 ; effectively, \\(\\psi_{0}=1\\) \\(\\left\\{Y_{t}\\right\\}\\) denote the observed time series. \\(\\left\\{e_{t}\\right\\}\\) represent an unobserved white noise series, that is, a sequence of identically distributed, zero-mean, independent random variables. the assumption of independence could be replaced by the weaker assumption that the \\(\\left\\{e_{t}\\right\\}\\) are uncorrelated random variables An important nontrivial example to which we will return often is the case where the \\(\\psi\\) is form an exponentially decaying sequence \\[ \\psi_{j}=\\phi^{j} \\] where \\(\\phi\\) is a number strictly between \\(-1\\) and \\(+1\\). Then \\[ Y_{t}=e_{t}+\\phi e_{t-1}+\\phi^{2} e_{t-2}+\\cdots \\] For this example, \\[ E\\left(Y_{t}\\right)=E\\left(e_{t}+\\phi e_{t-1}+\\phi^{2} e_{t-2}+\\cdots\\right)=0 \\] so that \\(\\left\\{Y_{t}\\right\\}\\) has a constant mean of zero. Also, \\[ \\begin{aligned} \\operatorname{Var}\\left(Y_{t}\\right) &amp;=\\operatorname{Var}\\left(e_{t}+\\phi e_{t-1}+\\phi^{2} e_{t-2}+\\cdots\\right) \\\\ &amp;=\\operatorname{Var}\\left(e_{t}\\right)+\\phi^{2} \\operatorname{Var}\\left(e_{t-1}\\right)+\\phi^{4} \\operatorname{Var}\\left(e_{t-2}\\right)+\\cdots \\\\ &amp;=\\sigma_{e}^{2}\\left(1+\\phi^{2}+\\phi^{4}+\\cdots\\right) \\\\ &amp;=\\frac{\\sigma_{e}^{2}}{1-\\phi^{2}} \\text { (by summing a geometric series) } \\end{aligned} \\] Furthermore, \\[ \\begin{aligned} \\operatorname{Cov}\\left(Y_{t}, Y_{t-1}\\right) &amp;=\\operatorname{Cov}\\left(e_{t}+\\phi e_{t-1}+\\phi^{2} e_{t-2}+\\cdots, e_{t-1}+\\phi e_{t-2}+\\phi^{2} e_{t-3}+\\cdots\\right) \\\\ &amp;=\\operatorname{Cov}\\left(\\phi e_{t-1}, e_{t-1}\\right)+\\operatorname{Cov}\\left(\\phi^{2} e_{t-2}, \\phi e_{t-2}\\right)+\\cdots \\\\ &amp;=\\phi \\sigma_{e}^{2}+\\phi^{3} \\sigma_{e}^{2}+\\phi^{5} \\sigma_{e}^{2}+\\cdots \\\\ &amp;=\\phi \\sigma_{e}^{2}\\left(1+\\phi^{2}+\\phi^{4}+\\cdots\\right) \\\\ &amp;=\\frac{\\phi \\sigma_{e}^{2}}{1-\\phi^{2}} \\text { (again summing a geometric series) } \\end{aligned} \\] Thus \\[ \\operatorname{Corr}\\left(Y_{t}, Y_{t-1}\\right)=\\left[\\frac{\\phi \\sigma_{e}^{2}}{1-\\phi^{2}}\\right] /\\left[\\frac{\\sigma_{e}^{2}}{1-\\phi^{2}}\\right]=\\phi \\] In a similar manner, we can find \\(\\operatorname{Cov}\\left(Y_{t}, Y_{t-k}\\right)=\\frac{\\phi^{k} \\sigma_{e}^{2}}{1-\\phi^{2}}\\) and thus \\[ \\operatorname{Corr}\\left(Y_{t}, Y_{t-k}\\right)=\\phi^{k} \\] The process defined in this way is stationarythe autocovariance structure depends only on time lag and not on absolute time 29.2.2 Moving Average Processes \\[ Y_{t}=e_{t}-\\theta_{1} e_{t-1}-\\theta_{2} e_{t-2}-\\cdots-\\theta_{q} e_{t-q} \\] We call such a series a moving average of order \\(q\\) and abbreviate the name to \\(\\operatorname{MA}(q)\\). \\(Y_{t}\\) is obtained by applying the weights \\(1,-\\theta_{1},-\\theta_{2}, \\ldots,-\\theta_{q}\\) to the variables \\(e_{t}, e_{t-1}, e_{t-2}, \\ldots, e_{t-q}\\) and then moving the weights and applying them to \\(e_{t+1}, e_{t}, e_{t-1}, \\ldots, e_{t-q+1}\\) to obtain \\(Y_{t+1}\\) and so on. Moving average models were first considered by Slutsky (1927) and Wold (1938). 29.2.2.1 MA(1): The First-Order Moving Average Process It is instructive to rederive the results. The model is \\[ Y_{t}=e_{t}-\\theta e_{t-1} \\] only one \\(\\theta\\) is involved, so drop the redundant subscript 1. Clearly \\(E\\left(Y_{t}\\right)=0\\) and \\(\\operatorname{Var}\\left(Y_{t}\\right)=\\sigma_{e}^{2}\\left(1+\\theta^{2}\\right) .\\) Now \\[ \\begin{aligned} \\operatorname{Cov}\\left(Y_{t}, Y_{t-1}\\right) &amp;=\\operatorname{Cov}\\left(e_{t}-\\theta e_{t-1}, e_{t-1}-\\theta e_{t-2}\\right) \\\\ &amp;=\\operatorname{Cov}\\left(-\\theta e_{t-1}, e_{t-1}\\right)=-\\theta \\sigma_{e}^{2} \\end{aligned} \\] and \\[ \\begin{aligned} \\operatorname{Cov}\\left(Y_{t}, Y_{t-2}\\right) &amp;=\\operatorname{Cov}\\left(e_{t}-\\theta e_{t-1}, e_{t-2}-\\theta e_{t-3}\\right) \\\\ &amp;=0 \\end{aligned} \\] since there are no \\(e\\) s with subscripts in common between \\(Y_{t}\\) and \\(Y_{t-2}\\). Similarly, \\(\\operatorname{Cov}\\left(Y_{t}, Y_{t-k}\\right)=0\\) whenever \\(k \\geq 2\\); the process has no correlation beyond lag 1. In summary for an MA(1) model \\(Y_{t}=e_{t}-\\theta e_{t-1}\\) \\[ \\left.\\begin{array}{rl} E\\left(Y_{t}\\right) &amp; =0 \\\\ \\gamma_{0} &amp; =\\operatorname{Var}\\left(Y_{t}\\right)=\\sigma_{e}^{2}\\left(1+\\theta^{2}\\right) \\\\ \\gamma_{1} &amp; =-\\theta \\sigma_{e}^{2} \\\\ \\rho_{1} &amp; =(-\\theta) /\\left(1+\\theta^{2}\\right) \\\\ \\gamma_{k} &amp; =\\rho_{k}=0 \\quad \\text { for } k \\geq 2 \\end{array}\\right\\} \\] library(TSA) data(tempdub) har.=harmonic(tempdub,1) ## har.: cos(2*pi*t) sin(2*pi*t) model4=lm(tempdub~har.) summary(model4) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 46.2660 0.3088 149.816 &lt; 2e-16 *** har.cos(2*pi*t) -26.7079 0.4367 -61.154 &lt; 2e-16 *** har.sin(2*pi*t) -2.1697 0.4367 -4.968 1.93e-06 *** plot(ts(fitted(model4),freq=12,start=c(1964,1)), ylab=&#39;Temperature&#39;,type=&#39;l&#39;, ylim=range(c(fitted(model4),tempdub))) points(tempdub) 29.2.2.2 Reliability and Efficiency of Regression Estimates To comparing the least squares estimates with the so-called best linear unbiased estimates (BLUE) or the generalized least squares (GLS) estimates. If the stochastic component {Xt } is not white noise, estimates of the unknown parameters in the trend function may be made; they are linear functions of the data, are unbiased, and have the smallest variances among all such estimatesthe so-called BLUE or GLS estimates. BLUEGLS{Xt}BLUEGLS . {Xt} 29.2.2.3 Interpreting Regression Output  {Xt} {Xt}{Xt} {Xt}(residual standard deviation)Xt \\[ s=\\sqrt{\\frac{1}{n-p} \\sum_{t=1}^{n}\\left(Y_{t}-\\hat{\\mu}_{t}\\right)^{2}} \\] 29.2.3 Residual Analysis As we have already noted, the unobserved stochastic component \\[\\left\\{X_{t}\\right\\}\\] can be estimated, or predicted, by the residual \\[ \\hat{X}_{t}=Y_{t}-\\hat{\\mu}_{t} \\] s()     , QQ  QQ Shapiro-WilkW = 0.9929p0.6954  (Sample Autocorrelation Function) acf(rstudent(model3)) 29.3 Stationary Models A broad class of parametric time series modelsthe autoregressive moving average (ARMA) models. 29.3.0.1 Model-Building Strategy model specification (or identification)  model fitting, and  model diagnostics  29.3.1 General Linear Processes {Yt} {et} {et} {Yt} \\[ Y_{t}=e_{t}+\\psi_{1} e_{t-1}+\\psi_{2} e_{t-2}+\\cdots \\]  (exponentially decaying sequence) \\[ \\psi_{j}=\\phi^{j} \\] where \\[\\phi\\] is a number strictly between \\[-1\\] and \\[+1\\]. Then \\[ Y_{t}=e_{t}+\\phi e_{t-1}+\\phi^{2} e_{t-2}+\\cdots \\] For this example, \\[ E\\left(Y_{t}\\right)=E\\left(e_{t}+\\phi e_{t-1}+\\phi^{2} e_{t-2}+\\cdots\\right)=0 \\] so that \\[\\left\\{Y_{t}\\right\\}\\] has a constant mean of zero. Also, \\[ \\begin{aligned} \\operatorname{Var}\\left(Y_{t}\\right) &amp;=\\operatorname{Var}\\left(e_{t}+\\phi e_{t-1}+\\phi^{2} e_{t-2}+\\cdots\\right) \\\\ &amp;=\\operatorname{Var}\\left(e_{t}\\right)+\\phi^{2} \\operatorname{Var}\\left(e_{t-1}\\right)+\\phi^{4} \\operatorname{Var}\\left(e_{t-2}\\right)+\\cdots \\\\ &amp;=\\sigma_{e}^{2}\\left(1+\\phi^{2}+\\phi^{4}+\\cdots\\right)\\\\ &amp;=\\frac{\\sigma_{e}^{2}}{1-\\phi^{2}} \\text{(by summing a geometric series)} \\end{aligned} \\] Furthermore, \\[ \\begin{aligned} \\operatorname{Cov}\\left(Y_{t}, Y_{t-1}\\right) &amp;=\\operatorname{Cov}\\left(e_{t}+\\phi e_{t-1}+\\phi^{2} e_{t-2}+\\cdots, e_{t-1}+\\phi e_{t-2}+\\phi^{2} e_{t-3}+\\cdots\\right) \\\\ &amp;=\\operatorname{Cov}\\left(\\phi e_{t-1}, e_{t-1}\\right)+\\operatorname{Cov}\\left(\\phi^{2} e_{t-2}, \\phi e_{t-2}\\right)+\\cdots \\\\ &amp;=\\phi \\sigma_{e}^{2}+\\phi^{3} \\sigma_{e}^{2}+\\phi^{5} \\sigma_{e}^{2}+\\cdots \\\\ &amp;=\\phi \\sigma_{e}^{2}\\left(1+\\phi^{2}+\\phi^{4}+\\cdots\\right)\\\\ &amp;=\\frac{\\phi \\sigma_{e}^{2}}{1-\\phi^{2}} \\text{(again summing a geometric series)} \\end{aligned} \\] Thus \\[ \\operatorname{Corr}\\left(Y_{t}, Y_{t-1}\\right)=\\left[\\frac{\\phi \\sigma_{e}^{2}}{1-\\phi^{2}}\\right] /\\left[\\frac{\\sigma_{e}^{2}}{1-\\phi^{2}}\\right]=\\phi \\] In a similar manner, we can find \\[ \\operatorname{Cov}\\left(Y_{t}, Y_{t-k}\\right)=\\frac{\\phi^{k} \\sigma_{e}^{2}}{1-\\phi^{2}} \\] and thus \\[ \\operatorname{Corr}\\left(Y_{t}, Y_{t-k}\\right)=\\phi^{k} \\] -(Time Lag)  \\[ E\\left(Y_{t}\\right)=0 \\quad \\gamma_{k}=\\operatorname{Cov}\\left(Y_{t}, Y_{t-k}\\right)=\\sigma_{e}^{2} \\sum_{i=0}^{\\infty} \\psi_{i} \\psi_{i+k} \\quad k \\geq 0 \\] with \\[\\psi_{0}=1\\]. 29.3.2 Moving Average Processes qMAq Yt1-1-2-qetet-1et-2et-q et + 1etet-1et-q + 1Yt + 1 \\[ Y_{t}=e_{t}-\\theta_{1} e_{t-1}-\\theta_{2} e_{t-2}-\\cdots-\\theta_{q} e_{t-q} \\] 29.3.2.1 The First-Order Moving Average Process MA(1) series: \\[ Y_{t}=e_{t}-\\theta e_{t-1} \\] \\[k  2\\], The process has no correlation beyond lag 1 \\[ \\left.\\begin{array}{rl} E\\left(Y_{t}\\right) &amp; =0 \\\\ \\gamma_{0} &amp; =\\operatorname{Var}\\left(Y_{t}\\right)=\\sigma_{e}^{2}\\left(1+\\theta^{2}\\right) \\\\ \\gamma_{1} &amp; =-\\theta \\sigma_{e}^{2} \\\\ \\rho_{1} &amp; =(-\\theta) /\\left(1+\\theta^{2}\\right) \\\\ \\gamma_{k} &amp; =\\rho_{k}=0 \\quad \\text { for } k \\geq 2 \\end{array}\\right\\} \\] 29.3.2.2 The Second-Order Moving Average Process \\[ Y_{t}=e_{t}-\\theta_{1} e_{t-1}-\\theta_{2} e_{t-2} \\] \\[ \\begin{aligned} \\gamma_{0} &amp;=\\operatorname{Var}\\left(Y_{t}\\right)=\\operatorname{Var}\\left(e_{t}-\\theta_{1} e_{t-1}-\\theta_{2} e_{t-2}\\right)=\\left(1+\\theta_{1}^{2}+\\theta_{2}^{2}\\right) \\sigma_{e}^{2} \\\\ \\gamma_{1} &amp;=\\operatorname{Cov}\\left(Y_{t}, Y_{t-1}\\right)=\\operatorname{Cov}\\left(e_{t}-\\theta_{1} e_{t-1}-\\theta_{2} e_{t-2}, e_{t-1}-\\theta_{1} e_{t-2}-\\theta_{2} e_{t-3}\\right) \\\\ &amp;=\\operatorname{Cov}\\left(-\\theta_{1} e_{t-1}, e_{t-1}\\right)+\\operatorname{Cov}\\left(-\\theta_{1} e_{t-2},-\\theta_{2} e_{t-2}\\right) \\\\ &amp;=\\left[-\\theta_{1}+\\left(-\\theta_{1}\\right)\\left(-\\theta_{2}\\right)\\right] \\sigma_{e}^{2} \\\\ &amp;=\\left(-\\theta_{1}+\\theta_{1} \\theta_{2}\\right) \\sigma_{e}^{2} \\\\ \\gamma_{2} &amp;=\\operatorname{Cov}\\left(Y_{t}, Y_{t-2}\\right)=\\operatorname{Cov}\\left(e_{t}-\\theta_{1} e_{t-1}-\\theta_{2} e_{t-2}, e_{t-2}-\\theta_{1} e_{t-3}-\\theta_{2} e_{t-4}\\right) \\\\ &amp;=\\operatorname{Cov}\\left(-\\theta_{2} e_{t-2}, e_{t-2}\\right) \\\\ &amp;=-\\theta_{2} \\sigma_{e}^{2} \\end{aligned} \\] Thus, for an MA(2) process, \\[ \\begin{aligned} \\rho_{1} &amp;=\\frac{-\\theta_{1}+\\theta_{1} \\theta_{2}}{1+\\theta_{1}^{2}+\\theta_{2}^{2}} \\\\ \\rho_{2} &amp;=\\frac{-\\theta_{2}}{1+\\theta_{1}^{2}+\\theta_{2}^{2}} \\\\ \\rho_{k} &amp;=0 \\text { for } k=3,4, \\ldots \\end{aligned} \\] 29.3.2.3 The General MA(q) Process For the general \\[\\operatorname{MA}(q)\\] process \\[Y_{t}=e_{t}-\\theta_{1} e_{t-1}-\\theta_{2} e_{t-2}-\\cdots-\\theta_{q} e_{t-q}\\], similar calcu- lations show that \\[ \\gamma_{0}=\\left(1+\\theta_{1}^{2}+\\theta_{2}^{2}+\\cdots+\\theta_{q}^{2}\\right) \\sigma_{e}^{2} \\] \\[ \\rho_{k}=\\left\\{\\begin{array}{l} \\frac{-\\theta_{k}+\\theta_{1} \\theta_{k+1}+\\theta_{2} \\theta_{k+2}+\\cdots+\\theta_{q-k} \\theta_{q}}{1+\\theta_{1}^{2}+\\theta_{2}^{2}+\\cdots+\\theta_{q}^{2}} \\\\ 0 \\quad \\text { for } k&gt;q \\end{array}\\right. \\] 29.3.3 Autoregressive Processes Autoregressive processes are as their name suggests - regressions on themselves. Specifically, a \\[p\\] th-order autoregressive process \\[\\left\\{Y_{t}\\right\\}\\] satisfies the equation \\[ Y_{t}=\\phi_{1} Y_{t-1}+\\phi_{2} Y_{t-2}+\\cdots+\\phi_{p} Y_{t-p}+e_{t} \\] 29.3.3.1 The First-Order Autoregressive Process \\[ Y_{t}=\\phi Y_{t-1}+e_{t} \\] 29.3.3.2 The Second-Order Autoregressive Process \\[ Y_{t}=\\phi_{1} Y_{t-1}+\\phi_{2} Y_{t-2}+e_{t} \\] 29.3.4 ARMA Model If we assume that the series is partly autoregressive and partly moving average, we obtain a quite general time series model. \\[ \\begin{array}{r} Y_{t}=\\phi_{1} Y_{t-1}+\\phi_{2} Y_{t-2}+\\cdots+\\phi_{p} Y_{t-p}+e_{t}-\\theta_{1} e_{t-1}-\\theta_{2} e_{t-2} \\\\ -\\cdots-\\theta_{q} e_{t-q} \\end{array} \\] we say that \\[\\left\\{Y_{t}\\right\\}\\] is a mixed autoregressive moving average process of orders \\[p\\] and \\[q\\], respectively; we abbreviate the name to ARMA \\[(p, q)\\]. 29.3.4.1 The ARMA(1,1) Model \\[ Y_{t}=\\phi Y_{t-1}+e_{t}-\\theta e_{t-1} \\] To derive Yule-Walker type equations, we first note that \\[ \\begin{aligned} E\\left(e_{t} Y_{t}\\right) &amp;=E\\left[e_{t}\\left(\\phi Y_{t-1}+e_{t}-\\theta e_{t-1}\\right)\\right] \\\\ &amp;=\\sigma_{e}^{2} \\end{aligned} \\] "]]
