# ANOVA

```{r mind map,echo = F,message = FALSE, error = FALSE, warning = FALSE}
library('mindr')
### text -> widget
### input <- c("# Chapter 1", "## Section 1.1", "### Section 1.1.1", "## Section 1.2", "# Chapter 2")
### mm(from = input, type = "text", root = "mindr")
filename <- rstudioapi::getSourceEditorContext()$path
widget <- mm(from = filename, type = "file", root = "")
widget
```

## Unstructured Models

These are models where **little is assumed about distributions, correlations**, etc. Nonparametric procedures fall in this class. The models for the actual data in this case may be quite complicated, but the assumption is that the analysis has been distilled down to a collection of pvalues. Multiple inference methods in this class consist essentially of adjusting these p-values for the purposes of making tests. Such methods work reasonably well for a variety of models, and if you have a model that is not contained in one of the major classes given below, then you can choose an MCP that assumes little structure. In particular, these methods are valid, though typically conservative when there are correlations. Generalized Bonferroni methods and standard false discovery rate controlling methods are in this group. 

> 在这些模型中，假设关于分布，相关性等假设很小。非参数过程属于此类。 在这种情况下，用于实际数据的模型可能非常复杂，但是假设是分析已精简为p值的集合。 此类中的多种推理方法主要包括为了进行测试而调整这些p值。 这样的方法对于各种模型都相当有效，可以选择结构很少的MCP。这些方法是有效的，尽管在存在关联时通常是保守的。 广义Bonferroni方法和标准错误发现率控制方法在该组中。

## Balanced One-Way Analysis-of-Variance (ANOVA) 

These are models for data from experiments where several groups are compared, and where the sample sizes are equal for all groups. Independence of data values is a crucial assumption for these models. If they are not independent, then you might be able to use one of the alternatives. Other assumptions strictly needed for these models are homogeneity of error variance and normality of the observations within each group. But these are not as important as the independence assumption (unless severely violated).  

> 来自实验数据的模型，其中比较了几个组，并且所有组的样本大小均相等。 
数据值的独立性是这些模型的关键假设。 如果它们不是独立的，那么您也许可以使用其他方法之一。 这些模型严格需要的其他假设是误差方差的均质性和各组内观测值的正态性。 但是这些并不像独立性假设那么重要（除非严重违反）。


### Modeling Assumptions and Basic Analysis

The model for the data throughout this chapter is assumed to be
$$
y_{i j}=\mu_{i}+\varepsilon_{i j}
$$
where the $y_{i j}$ are the observed data, with $i=1, \ldots, g$ indicating group and $j=1, \ldots, n$ indicating measurement observed within a group. This is a full-rank parameterization, unlike the default PROC GLM parameterization, which is not of full rank since it includes the "intercept" term $\gamma$. 

The $\mu_{i}$ are assumed to be fixed, unknown population mean values, and the errors $\varepsilon_{i j}$ are assumed to be random variables that

- have mean zero,
- have constant variance $\sigma^{2}$,
- are independent, and
- are normally distributed.

**Constant Variance**

The assumption of constant variance is also called homoscedasticity, and its violation is called
heteroscedasticity. As it turns out, at least in the balanced one-way model, heteroscedasticity is
not necessarily much of a problem, and inferences can still be approximately valid with mild
violations of this assumption. 

**Levene's Test for Homogeneity**

There are also formal statistical tests for homoscedasticity,
available with the HOVTEST option in the GLM MEANS statement; you can use this in
conjunction with the informal descriptive and graphical assessments.

```
proc glm data=Wloss;
 class Diet;
 model Wloss=Diet;
 means Diet / hovtest;
run; 
```

**Independence**

The assumption that the measurements are independent is crucial. In the extreme, its violation
can lead to estimates and inferences that are effectively based on much less information than it
might appear that you have, based on the sample size of your data set. Common ways for this
assumption to be violated include 

1. there are repeated measurements on the subjects (measurements on the same subject are usually correlated),
2. subjects are “paired” in some fashion, such as the husband/wife
3. the data involve time series or spatial autocorrelation. 

As with heteroscedasticity, autocorrelation can be diagnosed with informal graphical and formal
inferential measures, but the other two violations (which are probably more common in
ANOVA) require knowledge of the design for the data—how it was collected. You can check
for the various types of dependence structure using hypothesis tests, but, again, testing methods
should not be used exclusively to diagnose seriousness of the problem. 

**Normality**

It is usually not critical that the distribution of the response be precisely normal: the Central
Limit Theorem states that estimated group means are approximately normally distributed even
if the observations have non-normal distributions. This happy fact provides approximate largesample justification for the methods described in this chapter, as long as the other assumptions
are valid. However, if the sample sizes are small and the distributions are not even close to
normal, then the Central Limit Theorem may not apply. 

**Testing the Normality Assumption in ANOVA**

```
proc glm data=Wloss;
 class Diet;
 model Wloss=Diet;
 output out=wlossResid r=wlossResid;
run;
proc univariate data=wlossResid normal;
 var wlossResid;
 ods select TestsForNormality;
run; 
```


### Parameter Estimates

**Means and SD**

The estimated population means are the individual sample means for each group,
$$
\hat{\mu}_{i}=\bar{y}_{i}=\frac{\sum_{j=1}^{n} y_{i j}}{n},
$$
and the estimated common variance of the errors is the pooled mean squared error (MSE),
$$
\hat{\sigma}^{2}=\mathrm{MSE}=\frac{\sum_{i=1}^{g} \sum_{j=1}^{n}\left(y_{i j}-\bar{y}_{i}\right)^{2}}{g(n-1)}
$$
These formulas are special cases of the general formulas $\hat{\boldsymbol{\beta}}=\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{Y}$ and $\hat{\sigma}^{2}=(\mathbf{Y}-\mathbf{X} \hat{\boldsymbol{\beta}})^{\prime}(\mathbf{Y}-\mathbf{X} \hat{\boldsymbol{\beta}}) / d f$; here the $\mathbf{X}$ matrix is full rank.


**Simultaneous Confidence Intervals**

The general form of the simultaneous confidence interval
$$
\mathbf{c}^{\prime} \hat{\boldsymbol{\beta}} \pm c_{\alpha} s . e .\left(\mathbf{c}^{\prime} \hat{\boldsymbol{\beta}}\right)
$$
produces intervals for the difference of means $\mu_{i}-\mu_{i^{\prime}}$ having the form
$$
\bar{y}_{i}-\bar{y}_{i^{\prime}} \pm c_{\alpha} \hat{\sigma} \sqrt{2 / n}
$$
where $c_{\alpha}$ is a critical value that is selected to make the $\mathrm{FWE}=\alpha .$ The term $\hat{\sigma} \sqrt{2 / n}$ is the square root of the estimated variance of the difference, also called the standard error of the estimated difference.

In the case of non-multiplicity-adjusted confidence intervals, you set $c_{\alpha}$ equal to the $1-\alpha / 2$ quantile of the $t$ distribution, $t_{1-\alpha / 2, g(n-1)} .$ Each confidence interval thus constructed will contain the true difference $\mu_{i}-\mu_{i^{\prime}}$ with confidence $100(1-\alpha) \%$. 

However, when you look at many intervals (say, $k$ of them) then all $k$ intervals will contain their respective true differences simultaneously with much lower confidence. The Bonferroni inequality gives a pessimistic estimate of the simultaneous confidence of these $k$ non multiplicity-adjusted intervals as $100 \times(1-k \alpha) \%$. This implies that you can construct Bonferroni-adjusted confidence intervals by setting $c_{\alpha}=t_{1-\alpha^{\prime} / 2, g(n-1)}$, where $\alpha^{\prime}=\alpha / k$. However, the Bonferroni method is conservative:
the value $c_{\alpha}=t_{1-\alpha^{\prime} / 2, g(n-1)}$ is larger than it needs to be, in the sense that the actual simultaneous confidence level will be somewhat larger than the nominal level $100(1-\alpha) \%$.



### R Implementation

* `df.residual` 返回残差的自由度
* `coef` 返回被估计的系数(有时还包括他们的标准差)
* `residuals` 返回残差
* `deviance` 返回方差
* `fitted` 返回拟合值
* `logLik` 计算对数似然值和返回参数数目
* `AIC` 计算Akaike信息准则(Akaike information criterion, AIC) (依赖于logLik())

```{r One-Way ANOVA Test,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Date exploration

Intelligenz <- data.frame(IQ.Werte=c(99, 131, 118, 112, 128, 136, 120, 107,
                                     134, 122,134, 103, 127, 121, 139, 114, 
                                     121, 132,120, 133, 110, 141, 118, 124, 
                                     111, 138, 120,117, 125, 140, 109, 128, 
                                     137, 110, 138, 127, 141, 119, 148),
                             Fach=c(rep(1,10),rep(2,8),rep(3,9),rep(4,12)))
Intelligenz$Fach <- factor(Intelligenz$Fach,labels=c("I", "II", "III","IV"))

## Compute summary statistics
library(dplyr)
group_by(Intelligenz, Fach) %>%
  summarise(
    count = n(),
    mean = mean(IQ.Werte, na.rm = TRUE),
    sd = sd(IQ.Werte, na.rm = TRUE)
  )

## Visualize your data with ggpubr
library("ggpubr")
ggboxplot(Intelligenz, x = "Fach", y = "IQ.Werte", 
          color = "Fach", 
          palette = c("#00AFBB", "#E7B800", "#FC4E07","#E7298A"),
          order = c("I","II","III","IV"),
          ylab = "IQ.Werte", xlab = "Fach")

## Model Fitting
Intelligenz.aov <- aov(IQ.Werte~Fach, data=Intelligenz)
summary(Intelligenz.aov)
```


### SAS Implementation

Comparing Weight Loss for Five Different Regimens Ott (1988) reports an experiment undertaken to evaluate the effectiveness of five weightreducing agents. There are 10 male subjects in each group who have been randomly assigned to one of the regimens A, B, C, D, or E. This is a classic example of the balanced one-way ANOVA setup. After a fixed length of time, the weight loss of each of the 50 subjects is measured. The goal of the study is to rank the treatments, to the extent possible, using the observed weight loss data for the 50 subjects. 
These box plots are convenient for depicting and comparing the distributions of the data in each treatment group.



```
data Wloss;
 do Diet = 'A','B','C','D','E';
 do i = 1 to 10;
 input Wloss @@;
 output;
 end;
 end;
datalines;
12.4 10.7 11.9 11.0 12.4 12.3 13.0 12.5 11.2 13.1
 9.1 11.5 11.3 9.7 13.2 10.7 10.6 11.3 11.1 11.7
 8.5 11.6 10.2 10.9 9.0 9.6 9.9 11.3 10.5 11.2
 8.7 9.3 8.2 8.3 9.0 9.4 9.2 12.2 8.5 9.9
12.7 13.2 11.8 11.9 12.2 11.2 13.7 11.8 11.5 11.7
;
proc sgplot data=Wloss;
 vbox Wloss/category=Diet;
run;
```

### Model Diagnosis

1. Homogeneity Test
    + Homogeneity of variance assumption using Plot
    + Homogeneity of variance assumption using Levene's test, which is less sensitive to departures from normal distribution. (对偏离正态分布不敏感)
    + Homogeneity of variance assumption with no assumption of equal variances (放宽方差假设的同质性)
2. Normality assumption
3. Multiple Test

#### Homogeneity Test

```{r Homogeneity Test,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Check the homogeneity of variance assumption
plot(Intelligenz.aov, 1)

## Levene’s test,  Homogeneity of variance assumption
library(car)
leveneTest(IQ.Werte ~ Fach, data = Intelligenz)

## With no assumption of equal variances
oneway.test(IQ.Werte~Fach, data=Intelligenz)
```

#### Normality assumption

Using QQ Plot or Shapiro–Wilk test to test residuals

```{r Normality assumption,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Check the normality assumption
plot(Intelligenz.aov, 2)

## Shapiro–Wilk test to test residuals
## Extract the residuals and run Shapiro-Wilk test
aov_residuals <- residuals(object = Intelligenz.aov )
shapiro.test(x = aov_residuals )
```


#### Violation of assumptions

**Random Effects Models**

$$Y_{ij} = \mu + \alpha_i + \epsilon_{ij},$$
We assume $\alpha_i \; \textrm{i.i.d.} \sim N(0, \sigma_{\alpha}^2).$ and 
$$E[Y_{ij}] = \mu$$
$$\text{Var}(Y_{ij}) = \sigma_{\alpha}^2 + \sigma^2$$
$$\text{Cor}(Y_{ij}, Y_{kl}) =    \left\{   \begin{array}{cc}     0 & i \neq k \\     \sigma_{\alpha}^2 / (\sigma_{\alpha}^2 + \sigma^2) & i = k, j \neq l (\text{intraclass correlation (ICC)})\\     1 & i = k, j = l   \end{array}   \right.$$

MLE is not usable for parameter estimation for the variance components $\sigma_a^2$ and $\sigma^2$, **Restricted maximum likelihood (REML)** is applied here. REML is less biased. The parameter $\mu$ is estimated with maximum-likelihood assuming that the variances are known.

**Non-Parametric Test**

Non-parametric alternative to one-way ANOVA test, **Kruskal-Wallis rank sum test**, which can be used when ANOVA assumptions are not met. 

```
kruskal.test(IQ.Werte~Fach, data=Intelligenz)
```



## Unbalanced One-Way ANOVA and Analysis-of-Covariance (ANCOVA)

These data are similar to the balanced ANOVA except that sample sizes may be unbalanced, or the comparisons between means might be done while controlling one or more covariates (e.g., confounding variables, pre-experimental measurements). The distributional assumptions are identical to those of the ANOVA, with the exception that for ANCOVA, the **normality assumption must be evaluated by using residuals** and not actual data values. 

> 这些数据类似于平衡方差分析，不同之处在于样本数量可能不平衡，或者在控制一个或多个协变量（例如，混杂变量，实验前测量）的同时进行均值之间的比较。 分布假设与ANOVA相同，但对于ANCOVA，正态性假设必须使用残差而不是实际数据值进行评估。




## Two-Ways ANOVA Test

You consider the effects of two or more factors, with possibly unbalanced sample sizes and/or covariates. The distributional assumptions are the same as for the unbalanced oneway ANOVA or ANCOVA (if there are covariates). 

> 您考虑两个或多个因素的影响，可能会导致样本数量和/或协变量不平衡。 分布假设与非平衡单向方差分析或ANCOVA（如果存在协变量）相同。

### Introduction

$$
Y_{i j k}=\mu+\alpha_{i}+\beta_{j}+(\alpha \beta)_{i j}+\epsilon_{i j k}
$$
- ai is the main effect of factor $A$ at leveli
- $\beta \mathrm{j}$ is the main effect of factor $\mathrm{B}$ at level $\mathrm{j}$ 
- $(aß)ij$ is the interaction effect between $A$ and $B$ for the level combination i,j (it is not the product ai\betaj)

Test: the total sum of squares
$$
S S_{T}=S S_{A}+S S_{B}+S S_{A B}+S S_{E}
$$


* $SS_A = \sum_{i=1}^a b n (\widehat{\alpha}_i)^2$ “between rows”
* $SS_B = \sum_{j=1}^b a n (\widehat{\beta}_j)^2$ “between columns”
* $SS_{AB} = \sum_{i=1}^a \sum_{j=1}^b n (\widehat{\alpha\beta})_{ij}^2$ “correction”
* $SS_E = \sum_{i=1}^a \sum_{j=1}^b \sum_{k=1}^n (y_{ijk} - \overline{y}_{ij\cdot})^2$ Error “within cells”
* $SS_T = \sum_{i=1}^a \sum_{j=1}^b \sum_{k=1}^n (y_{ijk} - \overline{y}_{\cdot\cdot\cdot})^2$ “total”


### R implementation

```{r Two-Ways ANOVA Test,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Date exploration
my_data <- ToothGrowth

# Show a random sample
set.seed(1234)
dplyr::sample_n(my_data, 10)

# Convert dose as a factor and recode the levels
my_data$dose <- factor(my_data$dose, 
                  levels = c(0.5, 1, 2),
                  labels = c("D0.5", "D1", "D2"))
                  
## Compute mean and SD by groups using dplyr R package:
require("dplyr")
group_by(my_data, supp, dose) %>%
  summarise(
    count = n(),
    mean = mean(len, na.rm = TRUE),
    sd = sd(len, na.rm = TRUE)
  )
  
                  
# Visualize data
library("ggpubr")
ggboxplot(my_data, x = "dose", y = "len", color = "supp",
          palette = c("#00AFBB", "#E7B800"))

## Two-way interaction plot
library("ggpubr")
ggline(my_data, x = "dose", y = "len", color = "supp",
       add = c("mean_se", "dotplot"),
       palette = c("#00AFBB", "#E7B800"))

## Model Fitting
## Compute two-way ANOVA test
res.aov2 <- aov(len ~ supp + dose, data = my_data)
summary(res.aov2)

## Two-way ANOVA with interaction effect
res.aov3 <- aov(len ~ supp * dose, data = my_data)
res.aov3 <- aov(len ~ supp + dose + supp:dose, data = my_data)
summary(res.aov3)

  ## dose的p值<2e-16（显着），表明剂量水平与显着不同的牙齿长度len 有关。
  ## supp * dose之间相互作用的p值为0.02（显着），表明dose与牙齿长度len之间的关系取决于supp方法
  ## 在交互作用不明显的情况下，应使用加性模型。

## Diagnosis
## 1. Compute mean and SD by groups using dplyr R package:
require("dplyr")
group_by(my_data, supp, dose) %>%
  summarise(
    count = n(),
    mean = mean(len, na.rm = TRUE),
    sd = sd(len, na.rm = TRUE)
  )
model.tables(res.aov3, type="means", se = TRUE)

## 2. Multiple Test
pairwise.t.test(my_data$len, my_data$dose,
                p.adjust.method = "BH")

## Multiple pairwise-comparison between the means of groups
TukeyHSD(res.aov3, which = "dose")

library(multcomp)
summary(glht(res.aov2, linfct = mcp(dose = "Tukey")))

## 3. Homogeneity and normality
## Check the homogeneity of variance assumption (outliers)
plot(res.aov3, 1)

## Levene’s test to check the homogeneity of variances.
library(car)
leveneTest(len ~ supp*dose, data = my_data)

## Check the normality assumpttion
plot(res.aov3, 2)

# Extract the residuals, Shapiro-Wilk test
aov_residuals <- residuals(object = res.aov3)
shapiro.test(x = aov_residuals )
```

### SAS Implementation

```
data Waste;
 do Temp = 1 to 3;
 do Envir = 1 to 5;
 do rep=1 to 2;
 input Waste @@;
 output;
 end;
 end;
 end;
datalines;
7.09 5.90 7.94 9.15 9.23 9.85 5.43 7.73 9.43 6.90
7.01 5.82 6.18 7.19 7.86 6.33 8.49 8.67 9.62 9.07
7.78 7.73 10.39 8.78 9.27 8.90 12.17 10.95 13.07 9.76
;
run;
ods graphics on;
proc glm data=Waste;
 class Temp Envir;
 model Waste = Temp Envir Temp*Envir;
run;
quit;
ods graphics off;

```




### Unbalanced design

With unbalanced designs, LS-means typically are more relevant than arithmetic means for quantifying general population characteristics, since the LS-means estimate the marginal means over a balanced population, whether or not the design itself is balanced; the arithmetic means only estimate the marginal means for a population whose margins match those of the design. In particular, the arithmetic means estimate balanced population margins only when the design itself is balanced. Moreover, the LS-means match the arithmetic means when the design is balanced. 

```
## unequal numbers of subjects in each group.
library(car)
my_anova <- aov(len ~ supp * dose, data = my_data)
Anova(my_anova, type = "III")
```

```
*** Comparisons of LS-Means with Unbalanced Data;
data Drug;
 input Drug Disease @;
 do i=1 to 6;
 input Response @;
 output;
 end;
cards;
1 1 42 44 36 13 19 22
1 2 33 . 26 . 33 21
1 3 31 -3 . 25 25 24
2 1 28 . 23 34 42 13
2 2 . 34 33 31 . 36
2 3 3 26 28 32 4 16
3 1 . . 1 29 . 19
3 2 . 11 9 7 1 -6
3 3 21 1 . 9 3 .
4 1 24 . 9 22 -2 15
4 2 27 12 12 -5 16 15
4 3 22 7 25 5 12 .
;
ods graphics on;
proc glm;
 class Drug Disease;
 model Response = Drug Disease Drug*Disease/ss3;
 lsmeans Drug/ pdiff cl adjust=simulate(seed=121211 acc=.0002
report);
run; quit;
ods graphics off; 
```

 
```
*** Computing LS-Means by Hand;
data Balanced;
 do Drug = 1 to 4;
 do Disease = 1 to 3;
 output;
 end;
 end;
data DrugPlus; set Drug(where=(Response ^= .)) Balanced;
proc glm data=DrugPlus;
 class Drug Disease;
 model Response = Drug Disease Drug*Disease;
 output out=PredBal(where=(Response = .)) p=pResponse;
proc means data=PredBal;
 class Drug;
 ways 1;
 var pResponse;
run; 
```




## Heteroscedastic Responses

If the error variances are not constant, then the ordinary methods might be biased (in the sense of providing higher error rates than advertised) or inefficient (in the sense that the method lacks power to detect real differences). 

## Repeated Measures ANOVA Data 

When there are repeated measures on the same experimental unit, the crucial independence assumption that is used for the previous models no longer applies. For example, the data may contain repeated measures on blood pressure for an individual. In such cases, you can **model the dependence** of blood pressure measurements by using a variety of possible dependence structure models, and perform multiplicity-adjusted analyses within the context of such models. **Normality (or at least approximate normality) remains an important assumption** for these models. 

> 当在同一实验单元上重复测量时，先前模型中使用的关键独立性假设将不再适用。例如，数据可以包含针对个体的血压的重复测量。在这种情况下，您可以使用各种可能的依存关系结构模型对血压测量的依存关系进行建模，并在此类模型的上下文中执行经过多重调整的分析。正态性（或至少近似正态性）仍然是这些模型的重要假设。


## Multivariate Responses with Normally Distributed Data 

In these models, there are multiple measurements on the same individual. While repeated measures models usually assume that the measurements are taken on the same characteristic (like blood pressure), the multivariate response models allow completely different scales of measurement. For example, blood pressure and self-rated anxiety level form a multivariate response vector. Multiple inferences from such data are improved by incorporating the correlations among such measurements. In addition to the normality assumption, the multivariate observation vectors also are assumed independent, with constant covariance matrices. Our suggested method of analysis will allow covariates as well, so you can perform multiple comparisons with **multivariate analysis of covariance (MANCOVA)** data.

> 同一个人有多个测量值。虽然重复测量模型通常假定测量是在相同特征（如血压）下进行的，但多元响应模型却允许完全不同的测量范围。例如，血压和自我评估的焦虑水平形成多元响应向量。通过合并此类测量之间的相关性，可以改善来自此类数据的多个推断。除了正态性假设外，还假设多元观察向量是独立的，具有恒定的协方差矩阵。我们建议的分析方法也将允许使用协变量，因此您可以使用协方差（MANCOVA）数据的多变量分析执行多个比较。


## Independent Observations from Parametric Nonnormal Distributions

As an example, suppose you know that the observations are counts of defects on a manufactured item, and you wish to compare shifts A, B, and C. The model used may be Poisson, and you still wish to perform multiple comparisons. In this case, you can use any of several SAS procedures to fit the Poisson model, and can perform adjustments for multiple comparisons easily using the fitted results from such models.

> 假设您知道观察结果是制造品上的缺陷计数，并且您希望比较班次A，B和C。使用的模型可能是Poisson，并且您仍然希望执行多次比较。 在这种情况下，您可以使用几种SAS过程中的任何一种来拟合Poisson模型，并且可以使用此类模型的拟合结果轻松地对多个比较进行调整。


## Dependent Observations from Parametric Nonnormal Distributions

Following the previous example, suppose you know that the counts of defects on manufactured items are associated with different machines. You still wish to compare shifts A, B, and C, but you want to account for the machine effect. In this case, you may model the observations on a common machine as dependent, using a random effects model, where the machine effect is considered random. Again the model may be Poisson, but with a repeated measures component. In this case, you can use PROC GLIMMIX both to perform the repeated measures modeling and to perform the multiple comparisons.


 
