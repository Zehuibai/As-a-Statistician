# Advanced Linear Regression


## Model Selection





## Practical Difficulties using OLS

**1. Nonrandom Samples**

数据的收集方式直接影响我们可以得出的结论。假设检验的一般理论提出了一个从中抽取样本的总体。样本就是我们的数据，我们想使用从样本数据中获得的估计值来说明未知的总体值！此外，我们要求生成数据d使用总体的简单随机样本。该样本的大小是有限的，而总体的大小是无限的，或者至少如此之大，以至于样本量在整体中所占的比例可以忽略不计。当数据根本不是随机样本时，统计推断背后的逻辑还取决于样本是随机的。这并不是说这样的研究是毫无价值的，而是应用描述性的统计技术之外的其他任何东西都是不合理的。对此类数据得出的结论的置信度肯定值得怀疑

**2. Choice and Range of Predictors**

如果未观察到重要的预测因素，则预测可能会很差，或者我们可能会误解预测因素与响应之间的关系。数据收集的范围和条件可能会限制有效的预测。 推断太多是不安全的。

**3. Model Misspecification**

我们对模型的结构部分和随机部分进行假设。 $\varepsilon \sim \mathrm{N}\left(0, \sigma^{2} I\right)$ 但这可能不正确. 线性模型 $E y=X \beta$ 的结构部分也可能不正确。

**4. Practical and Statistical Significance**

统计意义不等于实际意义。样本越大，您的p值将越小，因此请不要将p值与重要的预测器效果混淆。对于大型数据集，将很容易获得具有统计意义的结果，但实际效果可能并不重要。

参数估计值上的CIS是评估效果大小的更好方法。即使在不拒绝零假设的情况下，它们也很有用，因为它们告诉我们，我们对真实效果或价值接近零的信心有多强。同样重要的是要记住，模型通常只是基础现实的近似，这至少使参数的确切含义值得商榷. 此外，我们知道，拥有的数据越多，测试的功能就越强大。即使是零样本，即使有很大的差异，也会被大量样本检测到。现在，如果我们不能拒绝原假设，我们可能会简单地得出结论，即我们没有足够的数据来获得重要的结果。根据这种观点，假设检验只是对样本量的检验。因此，我们更喜欢CIs



## Skewness

## Scale

单位的更改可能有助于解释, 但是不会影响参数估计。
当所有预测变量的比例都相似时，估计的数值稳定性会增强。
一种相当彻底的缩放方法是使用scale（）命令将所有变量转换为标准单位（均值0和方差1）。 这样的缩放比例具有将所有预测变量和响应置于可比较的比例尺上的优势，这使比较更加容易。
它还避免了当变量的比例非常不同时可能出现的一些数字问题。 这种缩放的不利之处在于，回归系数现在表示预测变量中标准单位增加对标准单位响应的影响-
这可能并不总是很容易解释。

```
data(savings)
g < - 1m (sr ˜ popl5+pop75+dpi+ddpi, savings)
summary (g)

Coefficients:
 Estimate Std. Error t value Pr(>|t|)
(Intercept) 28.566087 7.354516 3.88 0.00033
      pop15  0.461193 0.144642 !3.19 0.00260
      pop75  1.691498 1.083599 !1.56 0.12553
        dpi  0.000337 0.000931 !0.36 0.71917
        ddpi 0.409695 0.196197 2.09 0.04247
        
Residual standard error: 3.8 on 45 degrees of freedom
Multiple R-Squared: 0.338, Adjusted R-squared: 0.28
F-statistic: 5.76 on 4 and 45 DF, p-value: 0.00079


 g < 1m (sr ˜ pop15+pop75+I(dpi/1000)+ddpi, savings)
 summary (g)
 
Estimate Std. Error t value Pr(>|t|)
(Intercept) 28.566 7.355 3.88 0.00033
pop15 !0.461 0.145 !3.19 0.00260
pop75 !1.691 1.084 !1.56 0.12553
I(dpi/1000) !0.337 0.931 !0.36 0.71917
ddpi 0.410 0.196 2.09 0.04247

Residual standard error: 3.8 on 45 degrees of freedom
Multiple R-Squared: 0.338, Adjusted R-squared: 0.28
F-statistic: 5.76 on 4 and 45 DF, p-value: 0.00079
 
 
scsav < - data.frame(scale(savings))
g < - lm (sr , scsav) 
summary (g) Coefficients:

 Estimate Std Error t value Pr(>|t|)
(Intercept) 4.0e–16 0.1200 3.3e–15 1.0000
pop15 !0.9420 0.2954 !3.19 0.0026
pop75 !0.4873 0.3122 !1.56 0.1255
dpi !0.0745 0.2059 !0.36 0.7192
ddpi 0.2624 0.1257 2.09 0.0425

Residual standard error: 0.849 on 45 degrees of freedom
Multiple R-Squared: 0.338, Adjusted R-squared: 0.28
F-statistic: 5.76 on 4 and 45 DF, p-value: 0.00079
```


## Interaction


## Collinearity

When some predictors are linear combinations of others, then $X^{T} X$  is singular, and we have (exact) collinearity. There is no unique least squares estimate of $\beta$ If $X^{T} X$ is close to singular, we have collinearity (some call it multicollinearity). This causes serious problems with the estimation of $\beta$ and associated quantities, as well as the interpretation.

共线性可以通过以下几种方式检测：

1. 检查预测变量的相关矩阵可能会显示较大的成对共线性。
2. 将 $x_{i}$ 对所有其他预测变量进行回归得到 $R_{i}^{2}$ 。 对所有预测变量重复上述步骤。 $R_{i}^{2}$ 接近1表示有问 题。 令人讨厌的线性组合可以通过检查这些回归系数来发现。
3. 检查 $X^{T} X$, 的特征值，其中 $\lambda_{1}$ 是最大特征值，其他特征值递减。特征值较小表示存在问题。


```
data (seatpos)
g < - lm (hipcenter ˜ . , seatpos)
summary (g)
round(cor(seatpos), 3)

## check the eigendecomposition:
x < - model.matrix(g)[,-1] 
e < - eigen (t(x) %*% x)
e$val

sqrt(e$val[1]/e$val)
1.000 13.042 20.100 110.551 156.912 212.156 261.667 707.549


## check the variance inflation factors (VIFs). 
   For the first variable this is:
summary (lm (x [, 1] ~ x[,-1]))$r.squared
[1] 0.49948
1/(1!0.49948)
[1] 1.9979

vif (x)
Age Weight HtShoes Ht Seated Arm Thigh  Leg
1.9979 3.6470 307.4294 333.1378 8.9511 4.4964 2.7629 6.6943
```


## Problems with the Error

关于误差项 $\varepsilon$ 的标准假设是，它根据情况是独立且均匀分布的
（independent and identically distributed iid) 。即 $\varepsilon=\sigma^{2} I$ 。
此外，我们还假定误差是正态分布的，以便执行 通常的统计推断。我们已经看到,
这些假设经常会被违反，因此我们必须考虑其他选择。

* 当误差不是i.i.d.时，我们考虑使用广义最小二乘（GLS）。 
* 当误差是独立的，但分布不相同时，我们可以使用加权最小二乘（WLS），这是GLS的特例。 
* 当误差不是正态分布时，我们可以使用鲁棒回归

### Generalized Least Squares

到现在为止，我们假设var $\varepsilon=\sigma^{2} I,$, 但有时错误具有非恒定方差或相关。假设var $\varepsilon=\sigma^{2} \Sigma$, 其中
$\sigma^{2}$ 是未知的，而 $\Sigma$ 是已知的-换句话说，我们知道误差之间的相关性和相对方差，但我们不知道绝对比
例。
我们可以写 $\Sigma=S S^{T},$, 其中 $S$ 是使用Choleski分解的三角矩阵（triangular matrix) 。现在我们可以按
如下方式转换回归模型：
$$
\begin{array}{c}
y=X \beta+\varepsilon \\
S^{-1} y=S^{-1} X \beta+S^{-1} \varepsilon \\
y^{\prime}=X^{\prime} \beta+\varepsilon^{\prime} \\
\operatorname{var} \varepsilon^{\prime}=\operatorname{var}\left(S^{-1} \varepsilon\right)=S^{-1}(\operatorname{var} \varepsilon) S^{-T}=S^{-1} \sigma^{2} S S^{T} S^{-T}=\sigma^{2} I
\end{array}
$$
因此，我们可以将GLS 简化为OLS, 通过 $y^{\prime}=S^{-1} y$ on $S^{-1} X$,误差为 $\varepsilon^{\prime}$ 即iid. 我们只是将问题简化为 我们已经解决的问题。在此转换后的模型中，平方和为：
$$
\left(S^{-1} y-S^{-1} X \beta\right)^{T}\left(S^{-1} y-S^{-1} X \beta\right)=(y-X \beta)^{T} S^{-T} S^{-1}(y-X \beta)=(y-X \beta)^{T} \Sigma^{-1}(y-X \beta)
$$
which is minimized by:
$$
\hat{\boldsymbol{\beta}}=\left(X^{T} \Sigma^{-1} X\right)^{-1} X^{T} \Sigma^{-1} y
$$
We find that:
$$
\operatorname{var} \hat{\boldsymbol{\beta}}=\left(X^{T} \Sigma^{-1} X\right)^{-1} \sigma^{2}
$$
Since $\varepsilon^{\prime}=S^{-1} \varepsilon$, diagnostics should be applied to the residuals, $S^{-1} \hat{\varepsilon} .$ If we have the right $\Sigma$, then
these should be approximately i.i.d.
在Pinheiro和Bates（2000）的nlme软件包中找到更方便的方法，该软件包包含GLS拟合函数。我们可以 使用它来拟合此模型：

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
library (nlme)
fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
           correlation = corAR1(form = ~ 1 | Mare))
summary(fm1)

## check the confidence intervals
intervals (fm1) 
```

### Weighted Least Squares

有时误差是不相关的，但方差不相等。当 $\Sigma$ 是对角矩阵时，误差是不相关的，但不一定具有相等的方
差。在这种情况下可以使用WLS。
$$
\Sigma=\operatorname{diag}(1 / \mathrm{w} 1, \ldots, 1 / \mathrm{w} n)
$$
其中 $w_{i}$ 是权重, $S=\operatorname{diag}(\sqrt{1 / w 1}, \ldots, \sqrt{1 / w n})$ 因此我们可以将 $\sqrt{w_{i}} y_{i}$ 上的 $\sqrt{w_{i}} x_{i}$ 回归（尽
管需要替换 $X$ -matrix中的一列 与 )。变异性低的案例应获得较高的权重，变异性高的案例应获
得较低的权重。
Das Modell
$$
\boldsymbol{y}=\boldsymbol{X} \boldsymbol{\beta}+\boldsymbol{\varepsilon}
$$

is called a general linear regression model if the following assumptions apply:

1. $\mathrm{E}(\varepsilon)=\mathbf{0}$
2. $\operatorname{Cov}(\varepsilon)=\mathrm{E}\left(\varepsilon \varepsilon^{\prime}\right)=\sigma^{2} \boldsymbol{W}$, wobei $\boldsymbol{W}$ eine bekannte positiv definite Matrix sei.
3. Die Designmatrix $\boldsymbol{X}$ besitzt vollen Spaltenrang, d.h. $\operatorname{rg}(\boldsymbol{X})=p$.
4. $\varepsilon \sim \mathrm{N}\left(\mathbf{0}, \sigma^{2} \boldsymbol{W}\right)$
$\mathrm{lm}_{\mathrm{A} \mathrm{A} 2}^{\sim} \mathrm{A}+\mathrm{B}+\mathrm{C}+\mathrm{D}+\mathrm{E}+\mathrm{F}+\mathrm{G}+\mathrm{H}+\mathrm{J}+\mathrm{K}+\mathrm{N} ! \mathrm{l}$, fpe, weights $\left.=\mathrm{l} / \mathrm{EI}\right)$

**weights options**
	
an optional vector of weights to be used in the fitting process. Should be NULL or a numeric vector. If non-NULL, weighted least squares is used with weights weights (that is, minimizing sum(w*e^2)); otherwise ordinary least squares is used. 

### Robust Regression

When the error is normal, the least squares regression is obviously the best, but when the error is non-normal, other methods can be considered. Of particular interest are long-tailed error distributions. Robust regression provides an alternative. Robust regression is related to WLS. 

1. 健壮的估计器可防止长尾误差，但无法克服模型选择及其方差结构方面的问题。
2. 健壮的估算仅能为您提供 $\hat{\beta}$ 以及可能的标准错误，而没有相关的推论方法。
3. 除最小二乘法外，还可以使用鲁棒的方法作为确认方法。如果这两个估计值相距甚远，则有理由担 心。差异的来源应进行调查。


## Shrinkage Methods

### Principal Components Analzsis

### Partial Least Squares

Partial Least Squares (PLS) is a method used to associate a set of input variables X1,...Xm with outputs Y1,...,Y1. PLS regression is comparable to PCR because both use a certain number of linear combinations of predictors to predict the response. The difference is that although PCR ignores Y when determining linear combinations, PLS regression explicitly selects them to predict Y as much as possible. We will only consider univariate PLS

$$\hat{y}=\beta_{1} T_{1}+\cdots+\beta_{p} T_{p}$$

<!-- 偏最小二乘（PLS）是一种用于关联一组输入变量X1，... Xm和输出Y1，...，Y1的方法。PLS回归可与PCR媲美，因为两者均使用一定数量的预测变量线性组合来预测响应。 区别在于，尽管PCR在确定线性组合时忽略了Y，但PLS回归显式选择了它们来尽可能地预测Y。 我们将仅考虑单变量PLS -->

```{r, echo=FALSE, fig.align="center", out.width = '100%',fig.cap="Figure: Partial Least Squares"}
knitr::include_graphics("/Users/zehuibai/Documents/GitHub/As-a-Statistician/02_Plots/Partial Least Squares.png")
```

 
### Ridge Regression

$$
\hat{\boldsymbol{\beta}}=\left(X^{T} X+\lambda I\right)^{-1} X^{T} y
$$
We demonstrate the method on the meat spectroscopy data; $\lambda=0$ corresponds to least
squares while we find that as $\lambda \rightarrow \infty: \hat{\beta} \rightarrow 0$

The Ridge regression estimates of coefficients are biased. Deviation is undesirable, but it is not the only consideration. The mean square error (MSE) can be decomposed in the following ways:

$$E(\hat{\beta}-\beta)^{2}=(E(\hat{\beta}-\beta))^{2}+E(\hat{\beta}-E \hat{\beta})^{2}$$

因此，估计的MSE可以表示为偏差的平方加方差。 有时，可以以增加偏差为代价获得方差的大幅减少。 如果结果导致MSE降低，那么我们可能愿意接受一些偏见。 这就是岭回归所要做出的权衡—以偏差增加为代价减少方差。 这是一个常见的难题。 Frank and Friedman（1993）比较了PCR，PLS和岭回归，发现岭回归的最佳结果。 当然，对于任何给定的数据集，任何一种方法都可能被证明是最好的，因此很难选择一个获胜者






