<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Multiple-Comparison | As a Statistician</title>
  <meta name="description" content="This book involves different statistical principles and methods, including the application of SAS and R, and aims to accumulate personal statistical knowledge." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Multiple-Comparison | As a Statistician" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book involves different statistical principles and methods, including the application of SAS and R, and aims to accumulate personal statistical knowledge." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Multiple-Comparison | As a Statistician" />
  
  <meta name="twitter:description" content="This book involves different statistical principles and methods, including the application of SAS and R, and aims to accumulate personal statistical knowledge." />
  

<meta name="author" content="Zehui Bai" />


<meta name="date" content="2021-05-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="anova.html"/>
<link rel="next" href="non-parametric-test.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="input-and-format.html"><a href="input-and-format.html"><i class="fa fa-check"></i><b>2</b> Input and Format</a>
<ul>
<li class="chapter" data-level="2.1" data-path="input-and-format.html"><a href="input-and-format.html#import-in-sas"><i class="fa fa-check"></i><b>2.1</b> Import in SAS</a></li>
<li class="chapter" data-level="2.2" data-path="input-and-format.html"><a href="input-and-format.html#import-in-r"><i class="fa fa-check"></i><b>2.2</b> Import in R</a></li>
<li class="chapter" data-level="2.3" data-path="input-and-format.html"><a href="input-and-format.html#package-readr"><i class="fa fa-check"></i><b>2.3</b> Package readr</a></li>
<li class="chapter" data-level="2.4" data-path="input-and-format.html"><a href="input-and-format.html#package-strings"><i class="fa fa-check"></i><b>2.4</b> Package Strings</a></li>
<li class="chapter" data-level="2.5" data-path="input-and-format.html"><a href="input-and-format.html#package-forcats"><i class="fa fa-check"></i><b>2.5</b> Package forcats</a></li>
<li class="chapter" data-level="2.6" data-path="input-and-format.html"><a href="input-and-format.html#package-lubridate"><i class="fa fa-check"></i><b>2.6</b> Package lubridate</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-management-in-sas.html"><a href="data-management-in-sas.html"><i class="fa fa-check"></i><b>3</b> Data Management in SAS</a></li>
<li class="chapter" data-level="4" data-path="data-management-in-r.html"><a href="data-management-in-r.html"><i class="fa fa-check"></i><b>4</b> Data Management in R</a></li>
<li class="chapter" data-level="5" data-path="sql.html"><a href="sql.html"><i class="fa fa-check"></i><b>5</b> SQL</a></li>
<li class="chapter" data-level="6" data-path="space.html"><a href="space.html"><i class="fa fa-check"></i><b>6</b> ^_^ Space</a></li>
<li class="chapter" data-level="7" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>7</b> ggplot2</a></li>
<li class="chapter" data-level="8" data-path="proc-sqplot.html"><a href="proc-sqplot.html"><i class="fa fa-check"></i><b>8</b> Proc Sqplot</a>
<ul>
<li class="chapter" data-level="8.1" data-path="proc-sqplot.html"><a href="proc-sqplot.html#scatter-plot"><i class="fa fa-check"></i><b>8.1</b> Scatter Plot</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="proc-sqplot.html"><a href="proc-sqplot.html#vector-in-scatter-plot"><i class="fa fa-check"></i><b>8.1.1</b> Vector in scatter Plot</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="space-1.html"><a href="space-1.html"><i class="fa fa-check"></i><b>9</b> ^_^ Space</a></li>
<li class="chapter" data-level="10" data-path="descriptive-statistics-in-sas.html"><a href="descriptive-statistics-in-sas.html"><i class="fa fa-check"></i><b>10</b> Descriptive Statistics in SAS</a>
<ul>
<li class="chapter" data-level="10.1" data-path="descriptive-statistics-in-sas.html"><a href="descriptive-statistics-in-sas.html#proc-freq"><i class="fa fa-check"></i><b>10.1</b> Proc Freq</a></li>
<li class="chapter" data-level="10.2" data-path="descriptive-statistics-in-sas.html"><a href="descriptive-statistics-in-sas.html#proc-means"><i class="fa fa-check"></i><b>10.2</b> Proc Means</a></li>
<li class="chapter" data-level="10.3" data-path="descriptive-statistics-in-sas.html"><a href="descriptive-statistics-in-sas.html#proc-tabulatte"><i class="fa fa-check"></i><b>10.3</b> Proc Tabulatte</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="descriptive-statistics-in-r.html"><a href="descriptive-statistics-in-r.html"><i class="fa fa-check"></i><b>11</b> Descriptive Statistics in R</a>
<ul>
<li class="chapter" data-level="11.1" data-path="descriptive-statistics-in-r.html"><a href="descriptive-statistics-in-r.html#package-pape"><i class="fa fa-check"></i><b>11.1</b> Package pape</a></li>
<li class="chapter" data-level="11.2" data-path="descriptive-statistics-in-r.html"><a href="descriptive-statistics-in-r.html#package-summarytools"><i class="fa fa-check"></i><b>11.2</b> Package summarytools</a></li>
<li class="chapter" data-level="11.3" data-path="descriptive-statistics-in-r.html"><a href="descriptive-statistics-in-r.html#package-comparegroups"><i class="fa fa-check"></i><b>11.3</b> Package compareGroups</a></li>
<li class="chapter" data-level="11.4" data-path="descriptive-statistics-in-r.html"><a href="descriptive-statistics-in-r.html#package-kableextra"><i class="fa fa-check"></i><b>11.4</b> Package kableExtra</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="descriptive-statistics-in-r.html"><a href="descriptive-statistics-in-r.html#kable_styling"><i class="fa fa-check"></i><b>11.4.1</b> kable_styling</a></li>
<li class="chapter" data-level="11.4.2" data-path="descriptive-statistics-in-r.html"><a href="descriptive-statistics-in-r.html#columnrow-specification"><i class="fa fa-check"></i><b>11.4.2</b> Column/Row Specification</a></li>
<li class="chapter" data-level="11.4.3" data-path="descriptive-statistics-in-r.html"><a href="descriptive-statistics-in-r.html#grouping-columnsrows"><i class="fa fa-check"></i><b>11.4.3</b> Grouping columns/rows</a></li>
<li class="chapter" data-level="11.4.4" data-path="descriptive-statistics-in-r.html"><a href="descriptive-statistics-in-r.html#add-footnote"><i class="fa fa-check"></i><b>11.4.4</b> Add Footnote</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html"><i class="fa fa-check"></i><b>12</b> CI and Sample Size Calculation</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#distribution"><i class="fa fa-check"></i><b>12.1</b> Distribution</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#quantile-function-in-sas"><i class="fa fa-check"></i><b>12.1.1</b> Quantile Function in SAS</a></li>
<li class="chapter" data-level="12.1.2" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#binomial-distribution"><i class="fa fa-check"></i><b>12.1.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="12.1.3" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>12.1.3</b> Negative binomial distribution</a></li>
<li class="chapter" data-level="12.1.4" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#multinomial-distribution"><i class="fa fa-check"></i><b>12.1.4</b> Multinomial distribution</a></li>
<li class="chapter" data-level="12.1.5" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#normal-distribution"><i class="fa fa-check"></i><b>12.1.5</b> Normal Distribution</a></li>
<li class="chapter" data-level="12.1.6" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>12.1.6</b> Multivariate normal distribution</a></li>
<li class="chapter" data-level="12.1.7" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#poisson-distribution"><i class="fa fa-check"></i><b>12.1.7</b> Poisson distribution</a></li>
<li class="chapter" data-level="12.1.8" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#exponential-distribution"><i class="fa fa-check"></i><b>12.1.8</b> Exponential distribution</a></li>
<li class="chapter" data-level="12.1.9" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#gamma-distribution"><i class="fa fa-check"></i><b>12.1.9</b> Gamma distribution</a></li>
<li class="chapter" data-level="12.1.10" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#weibull-distribution"><i class="fa fa-check"></i><b>12.1.10</b> Weibull Distribution</a></li>
<li class="chapter" data-level="12.1.11" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#beta-distribution"><i class="fa fa-check"></i><b>12.1.11</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#point-estimates"><i class="fa fa-check"></i><b>12.2</b> Point Estimates</a></li>
<li class="chapter" data-level="12.3" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#binomial-ci"><i class="fa fa-check"></i><b>12.3</b> Binomial CI</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#binomial-ci-for-small-samples"><i class="fa fa-check"></i><b>12.3.1</b> Binomial CI for Small Samples</a></li>
<li class="chapter" data-level="12.3.2" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#package-binom"><i class="fa fa-check"></i><b>12.3.2</b> Package binom</a></li>
<li class="chapter" data-level="12.3.3" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#ci_single_proportion"><i class="fa fa-check"></i><b>12.3.3</b> %CI_Single_Proportion</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#incidence-rate-ci"><i class="fa fa-check"></i><b>12.4</b> Incidence rate CI</a></li>
<li class="chapter" data-level="12.5" data-path="ci-and-sample-size-calculation.html"><a href="ci-and-sample-size-calculation.html#package-pwr"><i class="fa fa-check"></i><b>12.5</b> Package: pwr</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="parametric-test.html"><a href="parametric-test.html"><i class="fa fa-check"></i><b>13</b> Parametric Test</a>
<ul>
<li class="chapter" data-level="13.1" data-path="parametric-test.html"><a href="parametric-test.html#proc-freq-1"><i class="fa fa-check"></i><b>13.1</b> Proc FREQ</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="parametric-test.html"><a href="parametric-test.html#ods-table-names"><i class="fa fa-check"></i><b>13.1.1</b> ODS Table Names</a></li>
<li class="chapter" data-level="13.1.2" data-path="parametric-test.html"><a href="parametric-test.html#ods-plots"><i class="fa fa-check"></i><b>13.1.2</b> ODS Plots</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="parametric-test.html"><a href="parametric-test.html#section"><i class="fa fa-check"></i><b>13.2</b> </a></li>
<li class="chapter" data-level="13.3" data-path="parametric-test.html"><a href="parametric-test.html#binomial-test"><i class="fa fa-check"></i><b>13.3</b> Binomial test</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="parametric-test.html"><a href="parametric-test.html#mathematical-formula"><i class="fa fa-check"></i><b>13.3.1</b> Mathematical Formula</a></li>
<li class="chapter" data-level="13.3.2" data-path="parametric-test.html"><a href="parametric-test.html#r-implementation"><i class="fa fa-check"></i><b>13.3.2</b> R implementation</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="parametric-test.html"><a href="parametric-test.html#fishers-exact-test"><i class="fa fa-check"></i><b>13.4</b> Fisher’s Exact Test</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="parametric-test.html"><a href="parametric-test.html#mathematical-formula-1"><i class="fa fa-check"></i><b>13.4.1</b> Mathematical Formula</a></li>
<li class="chapter" data-level="13.4.2" data-path="parametric-test.html"><a href="parametric-test.html#sas-implementation"><i class="fa fa-check"></i><b>13.4.2</b> SAS implementation</a></li>
<li class="chapter" data-level="13.4.3" data-path="parametric-test.html"><a href="parametric-test.html#r-implementation-1"><i class="fa fa-check"></i><b>13.4.3</b> R implementation</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="parametric-test.html"><a href="parametric-test.html#contingency-test"><i class="fa fa-check"></i><b>13.5</b> Contingency test</a></li>
<li class="chapter" data-level="13.6" data-path="parametric-test.html"><a href="parametric-test.html#mcnemars-test"><i class="fa fa-check"></i><b>13.6</b> McNemar’s test</a></li>
<li class="chapter" data-level="13.7" data-path="parametric-test.html"><a href="parametric-test.html#cochranmantelhaenszel-test"><i class="fa fa-check"></i><b>13.7</b> Cochran–Mantel–Haenszel Test</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="parametric-test.html"><a href="parametric-test.html#r-implementation-2"><i class="fa fa-check"></i><b>13.7.1</b> R implementation</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="parametric-test.html"><a href="parametric-test.html#correlation-test"><i class="fa fa-check"></i><b>13.8</b> Correlation Test</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="parametric-test.html"><a href="parametric-test.html#pearson-correlation"><i class="fa fa-check"></i><b>13.8.1</b> Pearson correlation</a></li>
<li class="chapter" data-level="13.8.2" data-path="parametric-test.html"><a href="parametric-test.html#spearman-correlation"><i class="fa fa-check"></i><b>13.8.2</b> Spearman correlation</a></li>
<li class="chapter" data-level="13.8.3" data-path="parametric-test.html"><a href="parametric-test.html#kendall-correlation"><i class="fa fa-check"></i><b>13.8.3</b> Kendall correlation</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="parametric-test.html"><a href="parametric-test.html#two-sample-t-test"><i class="fa fa-check"></i><b>13.9</b> Two Sample T-Test</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="parametric-test.html"><a href="parametric-test.html#sas-implementation-1"><i class="fa fa-check"></i><b>13.9.1</b> SAS implementation</a></li>
<li class="chapter" data-level="13.9.2" data-path="parametric-test.html"><a href="parametric-test.html#r-implementation-3"><i class="fa fa-check"></i><b>13.9.2</b> R implementation</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="parametric-test.html"><a href="parametric-test.html#normality-test"><i class="fa fa-check"></i><b>13.10</b> Normality test</a>
<ul>
<li class="chapter" data-level="13.10.1" data-path="parametric-test.html"><a href="parametric-test.html#sas-implementation-2"><i class="fa fa-check"></i><b>13.10.1</b> SAS implementation</a></li>
<li class="chapter" data-level="13.10.2" data-path="parametric-test.html"><a href="parametric-test.html#r-implementation-4"><i class="fa fa-check"></i><b>13.10.2</b> R implementation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>14</b> ANOVA</a>
<ul>
<li class="chapter" data-level="14.1" data-path="anova.html"><a href="anova.html#unstructured-models"><i class="fa fa-check"></i><b>14.1</b> Unstructured Models</a></li>
<li class="chapter" data-level="14.2" data-path="anova.html"><a href="anova.html#balanced-one-way-analysis-of-variance-anova"><i class="fa fa-check"></i><b>14.2</b> Balanced One-Way Analysis-of-Variance (ANOVA)</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="anova.html"><a href="anova.html#modeling-assumptions-and-basic-analysis"><i class="fa fa-check"></i><b>14.2.1</b> Modeling Assumptions and Basic Analysis</a></li>
<li class="chapter" data-level="14.2.2" data-path="anova.html"><a href="anova.html#parameter-estimates"><i class="fa fa-check"></i><b>14.2.2</b> Parameter Estimates</a></li>
<li class="chapter" data-level="14.2.3" data-path="anova.html"><a href="anova.html#r-implementation-5"><i class="fa fa-check"></i><b>14.2.3</b> R Implementation</a></li>
<li class="chapter" data-level="14.2.4" data-path="anova.html"><a href="anova.html#sas-implementation-3"><i class="fa fa-check"></i><b>14.2.4</b> SAS Implementation</a></li>
<li class="chapter" data-level="14.2.5" data-path="anova.html"><a href="anova.html#model-diagnosis"><i class="fa fa-check"></i><b>14.2.5</b> Model Diagnosis</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="anova.html"><a href="anova.html#unbalanced-one-way-anova-and-analysis-of-covariance-ancova"><i class="fa fa-check"></i><b>14.3</b> Unbalanced One-Way ANOVA and Analysis-of-Covariance (ANCOVA)</a></li>
<li class="chapter" data-level="14.4" data-path="anova.html"><a href="anova.html#two-ways-anova-test"><i class="fa fa-check"></i><b>14.4</b> Two-Ways ANOVA Test</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="anova.html"><a href="anova.html#introduction-1"><i class="fa fa-check"></i><b>14.4.1</b> Introduction</a></li>
<li class="chapter" data-level="14.4.2" data-path="anova.html"><a href="anova.html#r-implementation-6"><i class="fa fa-check"></i><b>14.4.2</b> R implementation</a></li>
<li class="chapter" data-level="14.4.3" data-path="anova.html"><a href="anova.html#unbalanced-design"><i class="fa fa-check"></i><b>14.4.3</b> Unbalanced design</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="anova.html"><a href="anova.html#heteroscedastic-responses"><i class="fa fa-check"></i><b>14.5</b> Heteroscedastic Responses</a></li>
<li class="chapter" data-level="14.6" data-path="anova.html"><a href="anova.html#repeated-measures-anova-data"><i class="fa fa-check"></i><b>14.6</b> Repeated Measures ANOVA Data</a></li>
<li class="chapter" data-level="14.7" data-path="anova.html"><a href="anova.html#multivariate-responses-with-normally-distributed-data"><i class="fa fa-check"></i><b>14.7</b> Multivariate Responses with Normally Distributed Data</a></li>
<li class="chapter" data-level="14.8" data-path="anova.html"><a href="anova.html#independent-observations-from-parametric-nonnormal-distributions"><i class="fa fa-check"></i><b>14.8</b> Independent Observations from Parametric Nonnormal Distributions</a></li>
<li class="chapter" data-level="14.9" data-path="anova.html"><a href="anova.html#dependent-observations-from-parametric-nonnormal-distributions"><i class="fa fa-check"></i><b>14.9</b> Dependent Observations from Parametric Nonnormal Distributions</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="multiple-comparison.html"><a href="multiple-comparison.html"><i class="fa fa-check"></i><b>15</b> Multiple-Comparison</a>
<ul>
<li class="chapter" data-level="15.1" data-path="multiple-comparison.html"><a href="multiple-comparison.html#introduction-2"><i class="fa fa-check"></i><b>15.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="multiple-comparison.html"><a href="multiple-comparison.html#multiplicity-problem"><i class="fa fa-check"></i><b>15.1.1</b> Multiplicity Problem</a></li>
<li class="chapter" data-level="15.1.2" data-path="multiple-comparison.html"><a href="multiple-comparison.html#error-rates"><i class="fa fa-check"></i><b>15.1.2</b> Error Rates</a></li>
<li class="chapter" data-level="15.1.3" data-path="multiple-comparison.html"><a href="multiple-comparison.html#the-adjusted-p"><i class="fa fa-check"></i><b>15.1.3</b> The adjusted P</a></li>
<li class="chapter" data-level="15.1.4" data-path="multiple-comparison.html"><a href="multiple-comparison.html#basic-statistical-concepts"><i class="fa fa-check"></i><b>15.1.4</b> Basic Statistical Concepts</a></li>
<li class="chapter" data-level="15.1.5" data-path="multiple-comparison.html"><a href="multiple-comparison.html#functions-in-glht-package-in-r"><i class="fa fa-check"></i><b>15.1.5</b> Functions in glht package in R</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="multiple-comparison.html"><a href="multiple-comparison.html#bonferroni-and-šidák-methods"><i class="fa fa-check"></i><b>15.2</b> Bonferroni and Šidák Methods</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="multiple-comparison.html"><a href="multiple-comparison.html#lsd-least-significance-difference"><i class="fa fa-check"></i><b>15.2.1</b> LSD (least significance difference)</a></li>
<li class="chapter" data-level="15.2.2" data-path="multiple-comparison.html"><a href="multiple-comparison.html#šidák"><i class="fa fa-check"></i><b>15.2.2</b> Šidák</a></li>
<li class="chapter" data-level="15.2.3" data-path="multiple-comparison.html"><a href="multiple-comparison.html#bonferroni"><i class="fa fa-check"></i><b>15.2.3</b> Bonferroni</a></li>
<li class="chapter" data-level="15.2.4" data-path="multiple-comparison.html"><a href="multiple-comparison.html#schweder-spjøtvoll-p-value-plot"><i class="fa fa-check"></i><b>15.2.4</b> Schweder-Spjøtvoll p-Value Plot</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="multiple-comparison.html"><a href="multiple-comparison.html#mcp-among-treatment-means-in-the-one-way-balanced-anova"><i class="fa fa-check"></i><b>15.3</b> MCP among Treatment Means in the One-Way Balanced ANOVA</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="multiple-comparison.html"><a href="multiple-comparison.html#ls-means"><i class="fa fa-check"></i><b>15.3.1</b> LS-Means</a></li>
<li class="chapter" data-level="15.3.2" data-path="multiple-comparison.html"><a href="multiple-comparison.html#the-multivariate-t-distribution"><i class="fa fa-check"></i><b>15.3.2</b> The Multivariate t Distribution</a></li>
<li class="chapter" data-level="15.3.3" data-path="multiple-comparison.html"><a href="multiple-comparison.html#calculating-the-critical-value-c_alpha"><i class="fa fa-check"></i><b>15.3.3</b> Calculating the Critical Value <span class="math inline">\(c_{\alpha}\)</span></a></li>
<li class="chapter" data-level="15.3.4" data-path="multiple-comparison.html"><a href="multiple-comparison.html#all-pairwise-comparisons-and-studentized-range-distribution"><i class="fa fa-check"></i><b>15.3.4</b> All Pairwise Comparisons and Studentized Range Distribution</a></li>
<li class="chapter" data-level="15.3.5" data-path="multiple-comparison.html"><a href="multiple-comparison.html#tukeys-method-for-all-pairwise-comparisons"><i class="fa fa-check"></i><b>15.3.5</b> Tukey’s Method for All Pairwise Comparisons</a></li>
<li class="chapter" data-level="15.3.6" data-path="multiple-comparison.html"><a href="multiple-comparison.html#displaying-pairwise-comparisons-graphically"><i class="fa fa-check"></i><b>15.3.6</b> Displaying Pairwise Comparisons Graphically</a></li>
<li class="chapter" data-level="15.3.7" data-path="multiple-comparison.html"><a href="multiple-comparison.html#dunnetts-two-sided-comparisons-with-a-control-and-dunnetts-two-sided-range-distribution"><i class="fa fa-check"></i><b>15.3.7</b> Dunnett’s Two-Sided Comparisons with a Control and Dunnett’s Two-Sided Range Distribution</a></li>
<li class="chapter" data-level="15.3.8" data-path="multiple-comparison.html"><a href="multiple-comparison.html#dunnetts-one-sided-comparisons-with-a-control"><i class="fa fa-check"></i><b>15.3.8</b> Dunnett’s One-Sided Comparisons with a Control</a></li>
<li class="chapter" data-level="15.3.9" data-path="multiple-comparison.html"><a href="multiple-comparison.html#maximum-modulus-distribution-multiple-inferences-for-independent-estimates"><i class="fa fa-check"></i><b>15.3.9</b> Maximum Modulus Distribution, Multiple Inferences for Independent Estimates</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="multiple-comparison.html"><a href="multiple-comparison.html#multiple-comparisons-among-treatment-means-in-the-one-way-unbalanced-anova"><i class="fa fa-check"></i><b>15.4</b> Multiple Comparisons among Treatment Means in the One-Way Unbalanced ANOVA</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="multiple-comparison.html"><a href="multiple-comparison.html#the-model-and-estimates"><i class="fa fa-check"></i><b>15.4.1</b> The Model and Estimates</a></li>
<li class="chapter" data-level="15.4.2" data-path="multiple-comparison.html"><a href="multiple-comparison.html#tukey-kramer-method"><i class="fa fa-check"></i><b>15.4.2</b> Tukey-Kramer Method</a></li>
<li class="chapter" data-level="15.4.3" data-path="multiple-comparison.html"><a href="multiple-comparison.html#alternative-simulation-based-method"><i class="fa fa-check"></i><b>15.4.3</b> Alternative Simulation-Based Method</a></li>
<li class="chapter" data-level="15.4.4" data-path="multiple-comparison.html"><a href="multiple-comparison.html#pairwise-comparisons-with-control"><i class="fa fa-check"></i><b>15.4.4</b> Pairwise Comparisons with Control</a></li>
<li class="chapter" data-level="15.4.5" data-path="multiple-comparison.html"><a href="multiple-comparison.html#comparisons-with-the-average-meananalysis-of-means-anom"><i class="fa fa-check"></i><b>15.4.5</b> Comparisons with the Average Mean–Analysis of Means (ANOM)</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="multiple-comparison.html"><a href="multiple-comparison.html#generalizations-for-the-analysis-of-covariance-ancova-model"><i class="fa fa-check"></i><b>15.5</b> Generalizations for the Analysis of Covariance (ANCOVA) model</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="multiple-comparison.html"><a href="multiple-comparison.html#dunnett-hsu-factor-analytic-approximation"><i class="fa fa-check"></i><b>15.5.1</b> Dunnett-Hsu Factor Analytic Approximation</a></li>
<li class="chapter" data-level="15.5.2" data-path="multiple-comparison.html"><a href="multiple-comparison.html#hsu-nelson-simulation-based-approximation-cvadjust-method"><i class="fa fa-check"></i><b>15.5.2</b> Hsu-Nelson Simulation-Based Approximation: CVADJUST Method</a></li>
<li class="chapter" data-level="15.5.3" data-path="multiple-comparison.html"><a href="multiple-comparison.html#comparisons-in-ancova-models-with-interaction"><i class="fa fa-check"></i><b>15.5.3</b> Comparisons in ANCOVA Models with Interaction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="non-parametric-test.html"><a href="non-parametric-test.html"><i class="fa fa-check"></i><b>16</b> Non-Parametric Test</a></li>
<li class="chapter" data-level="17" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html"><i class="fa fa-check"></i><b>17</b> Correlation and Regression</a>
<ul>
<li class="chapter" data-level="17.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#correlation"><i class="fa fa-check"></i><b>17.1</b> Correlation</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#pearson-correlation-coefficient"><i class="fa fa-check"></i><b>17.1.1</b> Pearson correlation coefficient</a></li>
<li class="chapter" data-level="17.1.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#spearmans-rank-correlation-coefficient"><i class="fa fa-check"></i><b>17.1.2</b> Spearman’s rank correlation coefficient</a></li>
<li class="chapter" data-level="17.1.3" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#kendall-rank-correlation-coefficient"><i class="fa fa-check"></i><b>17.1.3</b> Kendall rank correlation coefficient</a></li>
<li class="chapter" data-level="17.1.4" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#intraclass-correlation"><i class="fa fa-check"></i><b>17.1.4</b> Intraclass correlation</a></li>
<li class="chapter" data-level="17.1.5" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#visualize-the-correlation-in-r"><i class="fa fa-check"></i><b>17.1.5</b> Visualize the correlation in R</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>17.2</b> Ordinary least squares (OLS)</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#assumpions"><i class="fa fa-check"></i><b>17.2.1</b> Assumpions</a></li>
<li class="chapter" data-level="17.2.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#interpretation"><i class="fa fa-check"></i><b>17.2.2</b> Interpretation</a></li>
<li class="chapter" data-level="17.2.3" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#matrix-solution"><i class="fa fa-check"></i><b>17.2.3</b> Matrix Solution</a></li>
<li class="chapter" data-level="17.2.4" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>17.2.4</b> Gauss-Markov Theorem</a></li>
<li class="chapter" data-level="17.2.5" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#limitation"><i class="fa fa-check"></i><b>17.2.5</b> limitation</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#model-statistics"><i class="fa fa-check"></i><b>17.3</b> Model Statistics</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#residuals-standard-error"><i class="fa fa-check"></i><b>17.3.1</b> Residuals Standard Error</a></li>
<li class="chapter" data-level="17.3.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#r-squared-and-adjusted-r-squared"><i class="fa fa-check"></i><b>17.3.2</b> R-Squared and Adjusted R-Squared</a></li>
<li class="chapter" data-level="17.3.3" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#t-statistic"><i class="fa fa-check"></i><b>17.3.3</b> T Statistic</a></li>
<li class="chapter" data-level="17.3.4" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#f-statistic"><i class="fa fa-check"></i><b>17.3.4</b> F Statistic</a></li>
<li class="chapter" data-level="17.3.5" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>17.3.5</b> Confidence Intervals</a></li>
<li class="chapter" data-level="17.3.6" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>17.3.6</b> Likelihood-ratio test</a></li>
<li class="chapter" data-level="17.3.7" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#accuracy"><i class="fa fa-check"></i><b>17.3.7</b> Accuracy</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>17.4</b> Model Diagnostics</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#checking-error-assumptions"><i class="fa fa-check"></i><b>17.4.1</b> Checking Error Assumptions</a></li>
<li class="chapter" data-level="17.4.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#finding-unusual-observations"><i class="fa fa-check"></i><b>17.4.2</b> Finding Unusual Observations</a></li>
<li class="chapter" data-level="17.4.3" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#checking-the-structure-of-the-model"><i class="fa fa-check"></i><b>17.4.3</b> Checking the Structure of the Model</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#sas-implementation-proc-reg"><i class="fa fa-check"></i><b>17.5</b> SAS implementation Proc Reg</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#options"><i class="fa fa-check"></i><b>17.5.1</b> Options</a></li>
<li class="chapter" data-level="17.5.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#diagnose"><i class="fa fa-check"></i><b>17.5.2</b> Diagnose</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#r-implementation-7"><i class="fa fa-check"></i><b>17.6</b> R implementation</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html"><i class="fa fa-check"></i><b>18</b> Advanced Linear Regression</a>
<ul>
<li class="chapter" data-level="18.1" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#model-selection"><i class="fa fa-check"></i><b>18.1</b> Model Selection</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#selection-methods"><i class="fa fa-check"></i><b>18.1.1</b> Selection Methods</a></li>
<li class="chapter" data-level="18.1.2" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#selection-criteria"><i class="fa fa-check"></i><b>18.1.2</b> Selection Criteria</a></li>
<li class="chapter" data-level="18.1.3" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#k--fold-cross-validation"><i class="fa fa-check"></i><b>18.1.3</b> k- Fold Cross validation</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#practical-difficulties-using-ols"><i class="fa fa-check"></i><b>18.2</b> Practical Difficulties using OLS</a></li>
<li class="chapter" data-level="18.3" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#skewness"><i class="fa fa-check"></i><b>18.3</b> Skewness</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#introduction-3"><i class="fa fa-check"></i><b>18.3.1</b> Introduction</a></li>
<li class="chapter" data-level="18.3.2" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#baisc-transformation"><i class="fa fa-check"></i><b>18.3.2</b> Baisc Transformation</a></li>
<li class="chapter" data-level="18.3.3" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#box-cox-power-transformation"><i class="fa fa-check"></i><b>18.3.3</b> Box-Cox Power Transformation</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#scale"><i class="fa fa-check"></i><b>18.4</b> Scale</a></li>
<li class="chapter" data-level="18.5" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#interaction"><i class="fa fa-check"></i><b>18.5</b> Interaction</a>
<ul>
<li class="chapter" data-level="18.5.1" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#simple-slopes-analysis"><i class="fa fa-check"></i><b>18.5.1</b> Simple slopes analysis</a></li>
<li class="chapter" data-level="18.5.2" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#plotting-interactions"><i class="fa fa-check"></i><b>18.5.2</b> Plotting Interactions</a></li>
<li class="chapter" data-level="18.5.3" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#check-linearity-assumption"><i class="fa fa-check"></i><b>18.5.3</b> Check linearity assumption</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#collinearity"><i class="fa fa-check"></i><b>18.6</b> Collinearity</a></li>
<li class="chapter" data-level="18.7" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#problems-with-the-error"><i class="fa fa-check"></i><b>18.7</b> Problems with the Error</a>
<ul>
<li class="chapter" data-level="18.7.1" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>18.7.1</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="18.7.2" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>18.7.2</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="18.7.3" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#robust-regression"><i class="fa fa-check"></i><b>18.7.3</b> Robust Regression</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#shrinkage-methods"><i class="fa fa-check"></i><b>18.8</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="18.8.1" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#principal-components-analzsis"><i class="fa fa-check"></i><b>18.8.1</b> Principal Components Analzsis</a></li>
<li class="chapter" data-level="18.8.2" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#partial-least-squares"><i class="fa fa-check"></i><b>18.8.2</b> Partial Least Squares</a></li>
<li class="chapter" data-level="18.8.3" data-path="advanced-linear-regression.html"><a href="advanced-linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>18.8.3</b> Ridge Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>19</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="19.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introduction-4"><i class="fa fa-check"></i><b>19.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#violation-of-assumptions-of-ordinary-least-squares-ols"><i class="fa fa-check"></i><b>19.1.1</b> Violation of assumptions of Ordinary least squares (OLS)</a></li>
<li class="chapter" data-level="19.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#more-fundamental-problem-outside-01"><i class="fa fa-check"></i><b>19.1.2</b> More fundamental problem outside [0,1]</a></li>
<li class="chapter" data-level="19.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-model"><i class="fa fa-check"></i><b>19.1.3</b> Logistic Regression Model</a></li>
<li class="chapter" data-level="19.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#estimation-of-the-logistic-model"><i class="fa fa-check"></i><b>19.1.4</b> Estimation of the Logistic Model</a></li>
<li class="chapter" data-level="19.1.5" data-path="logistic-regression.html"><a href="logistic-regression.html#convergence-problems"><i class="fa fa-check"></i><b>19.1.5</b> Convergence Problems</a></li>
<li class="chapter" data-level="19.1.6" data-path="logistic-regression.html"><a href="logistic-regression.html#use-exact-methods."><i class="fa fa-check"></i><b>19.1.6</b> Use exact methods.</a></li>
<li class="chapter" data-level="19.1.7" data-path="logistic-regression.html"><a href="logistic-regression.html#use-penalized-likelihood"><i class="fa fa-check"></i><b>19.1.7</b> Use penalized likelihood</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="logistic-regression.html"><a href="logistic-regression.html#binary-regression"><i class="fa fa-check"></i><b>19.2</b> Binary Regression</a></li>
<li class="chapter" data-level="19.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logit-modell"><i class="fa fa-check"></i><b>19.3</b> Logit Modell</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introduction-5"><i class="fa fa-check"></i><b>19.3.1</b> Introduction</a></li>
<li class="chapter" data-level="19.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#sas-implementation-4"><i class="fa fa-check"></i><b>19.3.2</b> SAS Implementation</a></li>
<li class="chapter" data-level="19.3.3" data-path="logistic-regression.html"><a href="logistic-regression.html#multicollinearity"><i class="fa fa-check"></i><b>19.3.3</b> Multicollinearity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="advanced-logistic-regression.html"><a href="advanced-logistic-regression.html"><i class="fa fa-check"></i><b>20</b> Advanced Logistic Regression</a></li>
<li class="chapter" data-level="21" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>21</b> Survival Analysis</a></li>
<li class="chapter" data-level="22" data-path="advanced-survival-analysis.html"><a href="advanced-survival-analysis.html"><i class="fa fa-check"></i><b>22</b> Advanced Survival Analysis</a></li>
<li class="chapter" data-level="23" data-path="count-data-regression.html"><a href="count-data-regression.html"><i class="fa fa-check"></i><b>23</b> Count Data Regression</a></li>
<li class="chapter" data-level="24" data-path="proportion-response-regression.html"><a href="proportion-response-regression.html"><i class="fa fa-check"></i><b>24</b> Proportion Response Regression</a></li>
<li class="chapter" data-level="25" data-path="mixed-model.html"><a href="mixed-model.html"><i class="fa fa-check"></i><b>25</b> Mixed Model</a></li>
<li class="chapter" data-level="26" data-path="generalized-linear-mixed-model.html"><a href="generalized-linear-mixed-model.html"><i class="fa fa-check"></i><b>26</b> Generalized Linear (Mixed) Model</a></li>
<li class="chapter" data-level="27" data-path="generalized-estimating-equation.html"><a href="generalized-estimating-equation.html"><i class="fa fa-check"></i><b>27</b> Generalized Estimating Equation</a></li>
<li class="chapter" data-level="28" data-path="time-series-analysis.html"><a href="time-series-analysis.html"><i class="fa fa-check"></i><b>28</b> Time Series Analysis</a></li>
<li class="chapter" data-level="29" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>29</b> Meta Analysis</a></li>
<li class="chapter" data-level="30" data-path="group-adaptive-sequential-design.html"><a href="group-adaptive-sequential-design.html"><i class="fa fa-check"></i><b>30</b> Group (Adaptive) Sequential Design</a></li>
<li class="chapter" data-level="31" data-path="clinic-study-design.html"><a href="clinic-study-design.html"><i class="fa fa-check"></i><b>31</b> Clinic Study Design</a></li>
<li class="chapter" data-level="32" data-path="propensity-score.html"><a href="propensity-score.html"><i class="fa fa-check"></i><b>32</b> Propensity Score</a></li>
<li class="chapter" data-level="33" data-path="missing-data.html"><a href="missing-data.html"><i class="fa fa-check"></i><b>33</b> Missing Data</a></li>
<li class="chapter" data-level="34" data-path="space-2.html"><a href="space-2.html"><i class="fa fa-check"></i><b>34</b> ^_^ Space</a></li>
<li class="chapter" data-level="35" data-path="space-3.html"><a href="space-3.html"><i class="fa fa-check"></i><b>35</b> ^_^ Space</a></li>
<li class="chapter" data-level="36" data-path="space-4.html"><a href="space-4.html"><i class="fa fa-check"></i><b>36</b> ^_^ Space</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">As a Statistician</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-comparison" class="section level1" number="15">
<h1><span class="header-section-number">Chapter 15</span> Multiple-Comparison</h1>
<div id="introduction-2" class="section level2" number="15.1">
<h2><span class="header-section-number">15.1</span> Introduction</h2>
<div id="multiplicity-problem" class="section level3" number="15.1.1">
<h3><span class="header-section-number">15.1.1</span> Multiplicity Problem</h3>
<p>There are real effects from multiplicity.</p>
<ul>
<li>confounding effects</li>
<li>nonresponse effects</li>
<li>placebo effects</li>
<li>learning effects</li>
<li>carryover effects</li>
</ul>
<p>The problem with all statistical tests is the fact that the (overall) error rate increases with increasing number of tests. <span class="math display">\[1 - (1 - \alpha)^m.\]</span></p>
</div>
<div id="error-rates" class="section level3" number="15.1.2">
<h3><span class="header-section-number">15.1.2</span> Error Rates</h3>
<div id="comparisonwise-error-rate-cer" class="section level4" number="15.1.2.1">
<h4><span class="header-section-number">15.1.2.1</span> Comparisonwise Error Rate (CER)</h4>
<p>Typical inferences are performed using the <span class="math inline">\(95 \%\)</span> confidence level or <span class="math inline">\(5 \%\)</span> significance level. In either case, the comparisonwise error rate (CER) is <span class="math inline">\(5 \%\)</span>. For confidence intervals, CER is defined as
<span class="math display">\[\mathrm{CER}=P(\text{Interval does not contain the parameter}).\]</span>
A typical two-sided confidence interval has the form</p>
<p>(parameter estimate) <span class="math inline">\(\pm\)</span> (critical value) <span class="math inline">\(\times\)</span> (standard error of the estimate).</p>
<p>For example, if the parameter of interest is a population mean <span class="math inline">\(\mu\)</span>, and the data are normally distributed, then the usual two-sided <span class="math inline">\(95 \%\)</span> confidence interval for <span class="math inline">\(\mu\)</span> is
<span class="math display">\[
\bar{y} \pm t_{975, n-1} \times s_{y} / \sqrt{n}
\]</span>
where
- <span class="math inline">\(\bar{y}\)</span> is the estimate of the population mean
- <span class="math inline">\(s_{y}\)</span> is the sample standard deviation
o <span class="math inline">\(n\)</span> is the sample size
- <span class="math inline">\(s_{y} / \sqrt{n}\)</span> is the standard error of the estimated mean.</p>
<p>The critical value is <span class="math inline">\(t_{975, n-1}\)</span>, which is the <span class="math inline">\(1-0.05 / 2\)</span> quantile of the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. A one-sided upper confidence interval for <span class="math inline">\(\mu\)</span> might be all values below
<span class="math display">\[
\bar{y}+t_{.95, n-1} \times s_{y} / \sqrt{n}
\]</span>
For tests of hypotheses, CER is defined as
<span class="math display">\[
\mathrm{CER}=P\left(\text { Reject } H_{0} \mid H_{0}\right. \text { is true). }
\]</span></p>
</div>
<div id="familywise-error-rate-fwe" class="section level4" number="15.1.2.2">
<h4><span class="header-section-number">15.1.2.2</span> Familywise Error Rate (FWE)</h4>
<p><strong>FWE for Simultaneous Confidence Intervals</strong></p>
<p>The FWE is the probability of at least one erroneous inference, defined for simultaneous confidence intervals as
<span class="math display">\[\text{FWE (at least one interval is incorrect) 1 (all intervals are correct).}\]</span></p>
<p><strong>FWE for Multiple Tests of Hypotheses</strong></p>
<p>The family-wise error rate is defined as the probability of rejecting at least one of the true <span class="math inline">\(H_0\)</span></p>
<p>In the case of multiple tests of hypotheses, some of the hypotheses <span class="math inline">\(H_{0 j}\)</span> could be true, and others could be false. Suppose the true state of nature is that the particular null hypotheses corresponding to <span class="math inline">\(j_{1}, \ldots, j_{m}\)</span> are true, and all other null hypotheses are false. In other words, <span class="math inline">\(H_{0 j_{1}}, H_{0 j_{2}}, \ldots, H_{0 j_{m}}\)</span> are true, and the remaining <span class="math inline">\((k-m)\)</span> hypotheses are false. The FWE is then defined as</p>
<p><span class="math display">\[FWE =P( \text{reject at least one of} H_{0 j_{1}}, H_{0 j_{2}}, \ldots, H_{0 j_{m}} \mid H_{0 j_{1}}, H_{0 j_{2}}, \ldots, H_{0 j_{m}} \text{all are true})\]</span>.</p>
</div>
<div id="control-of-the-fwe-weak-and-strong" class="section level4" number="15.1.2.3">
<h4><span class="header-section-number">15.1.2.3</span> Control of the FWE: Weak and Strong</h4>
<p>An MCP is said to control the FWE in the weak sense if it controls the FWE under the complete null configuration, but not under all other configurations. Despite the fact that the terms “weak control” and “strong control” are used in conjunction with FWE, you should note that they really refer to different error rates. <strong>Weak control refers only to controlling the probability that the complete null hypothesis is rejected</strong>, and allows Type I errors in excess of the usual 5% value (for example, for the component hypotheses).</p>
<p>A method that controls the FWE in the strong sense will result in a <strong>Type I error for any component hypothesis no more than 5% of the time</strong>.</p>
<blockquote>
<p>如果MCP在完全空配置下而不是在所有其他配置下控制FWE，则说它在较弱的意义上控制FWE。 尽管事实上“弱控制”和“强控制”与FWE结合使用，但您应注意，它们实际上指的是不同的错误率。 弱控制仅是指控制拒绝完全无效假设的可能性，并且允许类型I错误超过通常的5％值（例如，对于分量假设）。
从严格意义上讲，控制FWE的方法将在不超过5％的时间内针对任何组件假设导致I型错误。</p>
</blockquote>
</div>
<div id="directional-decisions-and-type-iii-error-rates" class="section level4" number="15.1.2.4">
<h4><span class="header-section-number">15.1.2.4</span> Directional Decisions and Type III Error Rates</h4>
<p>A directional error (sometimes called a Type III error) is defined as the probability of misclassifying the sign of an effect. If you reject the hypothesis H0 : μ = 0 in favor of the (twosided) alternative HA : μ ≠ 0 using a CER= 0.05 level test, can you then claim that the sign of the true mean μ is the same as the sign of the estimated mean y ?</p>
<p>A type III error is where you correctly reject the null hypothesis, but it’s rejected for the wrong reason. This compares to a Type I error (incorrectly rejecting the null hypothesis) and a Type II error (not rejecting the null when you should). Type III errors are not considered serious, as they do mean you arrive at the correct decision. They usually happen because of random chance and are a rare occurrence.</p>
<p>You can also think of a Type III error as giving the right answer (i.e. correctly rejecting the null) to the wrong question. In other words, both your null and alternate hypotheses may be poorly worded or completely incorrect.</p>
<p>For MCPs, the Type III FWE is the probability that the sign of any tested effect is misclassified.</p>
</div>
<div id="false-discovery-rate-fdr" class="section level4" number="15.1.2.5">
<h4><span class="header-section-number">15.1.2.5</span> False Discovery Rate (FDR)</h4>
<p>Benjamini and Hochberg (1995) referred to the expected proportion of erroneously rejected null
hypotheses among the rejected ones as the False Discovery Rate, or FDR. Formally, for a given
family of k hypotheses and a given MCP, let R= number of hypotheses rejected, and let V = the
(unknown) number of erroneously rejected ones. Define V/R = 0 in case R=0. Then FDR is the
expected value of V/R</p>
<p><span class="math display">\[
\begin{array}{cccc}
\hline &amp; H_{0} \text { accepted } &amp; H_{0} \text { rejected } &amp; \text { Total } \\
\hline H_{0} \text { true } &amp; m-V &amp; V &amp; m \\
H_{0} \text { false } &amp; k-m-R+V &amp; R-V &amp; k-m \\
\text { Total } &amp; k-R &amp; R &amp; k \\
\hline
\end{array}
\]</span></p>
<p><span class="math display">\[
\mathrm{FDR}=E(V / R)
\]</span>
(assuming <span class="math inline">\(0 / 0\)</span> is defined as 0 ), whereas
<span class="math display">\[
\mathrm{FWE}=P(V&gt;0)
\]</span>
Under the overall null hypothesis, FDR and FWE are equal, since in this case <span class="math inline">\(V / R=1\)</span> when there is at least one rejection, and <span class="math inline">\(V / R=0\)</span> when there are no rejections.</p>
</div>
</div>
<div id="the-adjusted-p" class="section level3" number="15.1.3">
<h3><span class="header-section-number">15.1.3</span> The adjusted P</h3>
<p>Marginal p-value is based on the marginal p-values, which do not account for a multiplicity adjustment.</p>
<p>The adjusted P value is the smallest familywise significance level at which a particular comparison will be declared statistically significant as part of the multiple comparison testing. A separate adjusted P value is computed for each comparison in a family of comparisons.</p>
<p>The following show the R code about teh comparsion of adjusted and un-adjusted p-values</p>
<pre><code>library(multcomp)
data(thuesen,package = &quot;ISwR&quot;)
thuesen &lt;- read.sas7bdat(&quot;~/Desktop/SASUniversityEdition/myfolders/Daten/thuesen.sas7bdat&quot;)

thuesen.lm &lt;- lm(short.velocity ~ blood.glucose,data = thuesen)
thuesen.mc &lt;- glht(thuesen.lm, linfct = diag(2))

## With adjustment.
summary(thuesen.mc, 
        test = adjusted(type = &quot;bonferroni&quot;))
## without adjustment.
summary(thuesen.mc, test = univariate())</code></pre>
<p>Furthermore, there are different methods for p value adjust.</p>
<pre><code>Input = (&quot;
Food               Raw.p
 Blue_fish         .34
 Bread             .594
 Butter            .212
 Carbohydrates     .384
 Cereals_and_pasta .074
 Dairy_products    .94
 Eggs              .275
 Fats              .696
 Fruit             .269
 Legumes           .341
 Nuts              .06
&quot;)
Data = read.table(textConnection(Input),header=TRUE)
## Order data by p-value
Data = Data[order(Data$Raw.p),]

## Perform p-value adjustments and add to data frame
Data$Bonferroni = 
      p.adjust(Data$Raw.p, 
               method = &quot;bonferroni&quot;)
Data$BH = 
      p.adjust(Data$Raw.p, 
               method = &quot;BH&quot;)
Data$Holm = 
      p.adjust(Data$ Raw.p, 
               method = &quot;holm&quot;)
Data$Hochberg = 
      p.adjust(Data$ Raw.p, 
               method = &quot;hochberg&quot;)
Data$Hommel = 
      p.adjust(Data$ Raw.p, 
               method = &quot;hommel&quot;)
Data$BY = 
      p.adjust(Data$ Raw.p, 
               method = &quot;BY&quot;)

                Food Raw.p Bonferroni     BH Holm Hochberg    Hommel BY
11              Nuts 0.060      0.660 0.4070 0.66     0.66 0.5485714  1
5  Cereals_and_pasta 0.074      0.814 0.4070 0.74     0.74 0.5920000  1
3             Butter 0.212      1.000 0.5280 1.00     0.94 0.8700000  1
9              Fruit 0.269      1.000 0.5280 1.00     0.94 0.9280000  1
7               Eggs 0.275      1.000 0.5280 1.00     0.94 0.9280000  1
1          Blue_fish 0.340      1.000 0.5280 1.00     0.94 0.9400000  1
10           Legumes 0.341      1.000 0.5280 1.00     0.94 0.9400000  1
4      Carbohydrates 0.384      1.000 0.5280 1.00     0.94 0.9400000  1
2              Bread 0.594      1.000 0.7260 1.00     0.94 0.9400000  1
8               Fats 0.696      1.000 0.7656 1.00     0.94 0.9400000  1
6     Dairy_products 0.940      1.000 0.9400 1.00     0.94 0.9400000  1</code></pre>
</div>
<div id="basic-statistical-concepts" class="section level3" number="15.1.4">
<h3><span class="header-section-number">15.1.4</span> Basic Statistical Concepts</h3>
<p>The hypotheses described here are for the two-sample t-test, a common test for comparing two groups.
The assumptions of the two-sample t-test are important: random, independent samples from the two groups,
common variances, and normally distributed data.</p>
<ul>
<li>The null hypothesis is <span class="math inline">\(H_{0}: \mu_{1}=\mu_{2} ;\)</span> that is, the hypotheses that the population means are equal.</li>
<li>The alternative hypothesis is <span class="math inline">\(H_{A}: \mu_{1} \neq \mu_{2} ;\)</span> that is, the hypotheses that the population means are not equal.</li>
<li>The test statistic is <span class="math inline">\(T=\frac{\bar{X}_{1}-\bar{X}_{2}}{s_{p} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}}\)</span>, where <span class="math inline">\(s_{p}^{2}=\frac{\left(n_{1}-1\right) s_{1}^{2}+\left(n_{2}-1\right) s_{2}^{2}}{n_{1}+n_{2}-2}\)</span>.</li>
<li>The decision rule is to reject <span class="math inline">\(H_{0}\)</span> if <span class="math inline">\(|T| \geq t_{1-\alpha / 2, n-2}\)</span>, where <span class="math inline">\(t_{1-\alpha / 2, n-2}\)</span> is the critical value.</li>
<li>The <span class="math inline">\(p\)</span> -value is the probability of observing a test statistic as large as or larger than the <span class="math inline">\(|T|\)</span> that was observed in the study, assuming the null hypothesis is true.</li>
</ul>
<p>By construction, the <span class="math inline">\(p\)</span> -value is found <span class="math inline">\(\leq \alpha\)</span> wherever <span class="math inline">\(|T| \geq t_{1-\alpha / 2, n-2} .\)</span> Thus, when all of the assumptions are satisfied,
<span class="math display">\[
P\left(p \leq \alpha \mid H_{0} \text { is true }\right)=\alpha
\]</span>
This leads to an important point:</p>
<p><strong>When the null hypothesis is true and when all assumptions are satisfied, the <span class="math inline">\(p\)</span> -value has a uniform distribution.</strong></p>
<p>From the parameter, the adjusted and unadjusted p value can be calculated</p>
<pre><code>## Calculation without adjustment.
## regression coefficients β and their covariance matrix

betahat &lt;- coef(thuesen.lm)
Vbetahat &lt;- vcov(thuesen.lm)
##  compute two individual t test statistics and correlation matrix
C &lt;- diag(2)
Sigma &lt;- diag(1 / sqrt(diag(C %*% Vbetahat %*% t(C))))
t &lt;- Sigma %*% C %*% betahat
Cor &lt;- Sigma %*% (C %*% Vbetahat %*% t(C)) %*% t(Sigma)


## Use the pmvt function of the mvtnorm package to calculate the adjusted p value from the basic bivariate t distribution
library(&quot;mvtnorm&quot;)
thuesen.df &lt;- nrow(thuesen) - length(betahat)
q &lt;- sapply(abs(t), function(x) 1 - pmvt(-rep(x, 2), 
                                         rep(x, 2), 
                                         corr = Cor,
                                         df = thuesen.df))
##  获得了多重调整的p值 q1 &lt;0.001且q2 = 0.064

##  compute the critical value u1−α 计算临界值
delta &lt;- rep(0, 2)
myfct &lt;- function(x, conf) {
  lower &lt;- rep(-x, 2)
  upper &lt;- rep(x, 2)
  pmvt(lower, upper, df = thuesen.df, corr = Cor,
           delta, abseps = 0.0001)[1] - conf
}
u &lt;- uniroot(myfct, lower = 1, upper = 5, conf = 0.95)$root
round(u, 3)</code></pre>
</div>
<div id="functions-in-glht-package-in-r" class="section level3" number="15.1.5">
<h3><span class="header-section-number">15.1.5</span> Functions in glht package in R</h3>
<table>
<thead>
<tr class="header">
<th>Functions</th>
<th>Descriptions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>glht.mc$model</code></td>
<td>The fitted model</td>
</tr>
<tr class="even">
<td><code>glht.mc$linfct</code></td>
<td>linear conflict functions</td>
</tr>
<tr class="odd">
<td><code>glht.mc$vcov</code></td>
<td>Covariance matrix</td>
</tr>
<tr class="even">
<td><code>glht.res &lt;- summary(glht.mc)  glht.res$test$pvalues</code></td>
<td>P-values</td>
</tr>
<tr class="odd">
<td><code>summary(warpbreaks.mc, test = Ftest())</code></td>
<td>Global F-Test</td>
</tr>
<tr class="even">
<td><code>summary(warpbreaks.mc, test = Chisqtest())</code></td>
<td>Wald测试</td>
</tr>
<tr class="odd">
<td><code>summary(warpbreaks.mc, test = univariate())</code></td>
<td>未调整的p值, 不考虑多重性执行了m个单独t检验</td>
</tr>
<tr class="even">
<td><code>summary(warpbreaks.mc, test = adjusted(type = "bonferroni"))</code></td>
<td>Bonferroni校正</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="bonferroni-and-šidák-methods" class="section level2" number="15.2">
<h2><span class="header-section-number">15.2</span> Bonferroni and Šidák Methods</h2>
<div id="lsd-least-significance-difference" class="section level3" number="15.2.1">
<h3><span class="header-section-number">15.2.1</span> LSD (least significance difference)</h3>
<p>least significant difference method. First proposed by Fisher, it is essentially a t-test.</p>
<p>For Two independent sample t test:</p>
<p><span class="math display">\[t=\frac{\bar{X}_{1}-\bar{X}_{2}}{\sqrt{S_{c}^{2}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}}\]</span>
<span class="math display">\[S_{c}^{2}=\frac{\left(n_{1}-1\right) S_{1}^{2}+\left(n_{2}-1\right) S_{2}^{2}}{n_{1}+n_{2}-2}\]</span>
is the variance of the joint estimate of the two samples, under the premise that the sample variance is uniform</p>
<p>The LSD method also performs a t-test of pairwise comparison. The difference is that under the premise of meeting the homogeneity of variance, the LSD method uses the joint variance of <strong>all samples</strong> to estimate the standard error of the mean difference, rather than the <strong>joint variance of the two samples</strong> to be compared. Take the comparison of the mean difference between the three samples as an example, the formula is</p>
<!-- LSD法采用所有样本的联合方差来估计均数差的标准误，而不是要比较的两个样本的联合方差。 -->
<p><span class="math display">\[\begin{aligned}
&amp;S_{c}^{2}=\frac{\left(n_{1}-1\right) S_{1}^{2}+\left(n_{2}-1\right) S_{2}^{2}+\left(n_{3}-1\right) S_{3}^{2}}{n_{1}+n_{2}+n_{3}-3}
\end{aligned}\]</span></p>
<p>The LSD method calculates the smallest significant difference, namely
<span class="math display">\[\begin{aligned}
&amp;L S D=t_{\alpha / 2} \sqrt{S_{c}^{2}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}
\end{aligned}\]</span></p>
<blockquote>
<p>LSD法单次比较的检验水准仍然为α。LSD法检验的灵敏度最高，但是会因为对比的频数增加使得第一类型错误概率增加。为解决该问题，便出现了Sidak法和Bonferroni法。</p>
</blockquote>
</div>
<div id="šidák" class="section level3" number="15.2.2">
<h3><span class="header-section-number">15.2.2</span> Šidák</h3>
<p>Sidak法的也是一种t检验，计算公式和LSD法的相同。但是Sidak法对a进行了调整。如果有k组, 对k组进行两两比较的次数为 <span class="math inline">\(c=\frac{k(k-1)}{2}\)</span> 那么做完c次比较，累积犯一类错误的概率为:
<span class="math inline">\(1-\left(1-\alpha_{a}\right)^{c}\)</span> 令上面的公式值等于0.05, 由此可以反推出调整后的 <span class="math inline">\(\alpha_{a} \quad\)</span> 。例如进行6次事后比 较, 则Sidak法的=0.0085, 以 <span class="math inline">\(\alpha_{a}\)</span> 作为单次比较的显著性水平，显然 <span class="math inline">\(\alpha_{a}\)</span> 变小了。由于 <span class="math inline">\(\alpha_{a}\)</span> 减
你，结论趋于接受无效假设, 因此该方法要比LSD法保守的多。</p>
<p>The rationale for this method is the Boole inequality:
<span class="math display">\[
P\left(A_{1} \text { or } A_{2} \text { or } \ldots \text { or } A_{k}\right) \leq P\left(A_{1}\right)+P\left(A_{2}\right)+\cdots+P\left(A_{k}\right)
\]</span></p>
<p><span class="math display">\[
P\left(\left\{\text { Reject } H_{01}\right\} \text { or }\left\{\text { Reject } H_{02}\right\}\right) \leq P\left(\text { Reject } H_{01}\right)+P\left(\text { Reject } H_{02}\right)
\]</span></p>
<p>For the Šidák method, recall that you can reject an individual hypothesis <span class="math inline">\(H_{0 j}\)</span> if <span class="math inline">\(p_{j} \leq 1-(1-\alpha)^{1 / k}\)</span>;
or equivalently, when <span class="math inline">\(1-\left(1-p_{j}\right)^{k} \leq \alpha\)</span>, where <span class="math inline">\(\alpha\)</span> is the desired FWE level. This gives you the Šidák adjusted <span class="math inline">\(p\)</span> -values.
Šidák Adjusted <span class="math inline">\(p\)</span> -value for Hypothesis <span class="math inline">\(H_{0 j}\)</span>;
<span class="math display">\[
\tilde{p}_{j}=1-\left(1-p_{j}\right)^{k} .
\]</span></p>
</div>
<div id="bonferroni" class="section level3" number="15.2.3">
<h3><span class="header-section-number">15.2.3</span> Bonferroni</h3>
<p>The Bonferroni method is similar to the Sidak method, and α is also adjusted on the basis of the LSD method. The adjustment method is based on Bonferroni’s inequality. If there are k groups, the calculation formula is
<span class="math display">\[\alpha^* = \alpha / k\]</span></p>
<p>The Bonferroni method is generally considered to be the most conservative. When the number of comparisons is small, the effect of this method is better. When the number of comparisons is large (such as k&gt;10), the adjustment of <span class="math inline">\(\alpha\)</span> is somewhat overcorrected and the effect is not as good as Sidak</p>
<pre><code>library(multcomp)
## Create a matrix where each *row* is a contrast
K &lt;- rbind(c(1, -1/2, -1/2), ## ctrl vs. average of trt1 and trt2
           c(1, -1, 0))      ## ctrl vs. trt1
fit.gh &lt;- glht(fit, linfct = mcp(group = K))

## Individual p-values
summary(fit.gh, test = adjusted(&quot;none&quot;))

## Bonferroni corrected p-values
summary(fit.gh, test = adjusted(&quot;bonferroni&quot;))</code></pre>
<p>While the Boole inequality is directly applicable to multiple hypothesis testing, the Bonferroni
inequality is directly applicable to <strong>simultaneous confidence intervals</strong>. As an example, suppose
that you have constructed k=10 simultaneous confidence intervals, all at the CER level
0.05/k=0.05/10=0.005, corresponding to 99.5% confidence intervals. Then the simultaneous
confidence level is</p>
<p><span class="math display">\[
\begin{array}{l}
P(\{\text { Interval } 1 \text { correct }\} \text { and } \ldots \text { and }\{\text { Interval } 10 \text { correct }\}) \\
\geq 1-\{P(\text { Interval } 1 \text { incorrect })+\cdots+P(\text { Interval } 10 \text { incorrect })\} \\
=1-10(0.005) \\
=0.95 .
\end{array}
\]</span></p>
<p>Bonferroni Adjusted <span class="math inline">\(p\)</span> -value for Hypothesis <span class="math inline">\(H_{0 j}\)</span>;
<span class="math display">\[
\tilde{p}_{j}=\left\{\begin{array}{ccc}
k p_{j} &amp; \text { if } &amp; k p_{j} \leq 1 \\
1 &amp; \text { if } &amp; k p_{j}&gt;1
\end{array}\right.
\]</span></p>
<p><strong>Bonferroni and Šidák Adjusted p-Values Using the DATA Step</strong></p>
<pre><code>data pvals1;
 input test pval @@;
 bon_adjp = min(1,10*pval);
 sid_adjp = 1 - (1-pval)**10;
datalines;
1 0.0911 2 0.8912
3 0.0001 4 0.5718
5 0.0132 6 0.9011
7 0.2012 8 0.0289
9 0.0498 10 0.0058
;
proc sort data=pvals1 out=pvals1;
 by pval;
proc print data=pvals1;
run; </code></pre>
<p><strong>Bonferroni and Šidák Adjusted p-Values Using PROC MULTTEST</strong></p>
<pre><code>proc multtest inpvalues(pval)=pvals1 bon sid out=outp;
proc sort data=outp out=outp;
 by pval;
proc print data=outp label;
run;</code></pre>
<p>Bonferroni and Šidák methods are easy to implement, and they correspond naturally to
confidence intervals. Šidák’s method provides slightly more power, but occasionally does not
control the FWE. However, when confidence intervals are not required, adaptive procedures are
more powerful, although they might not control the FWE in some cases. Simulation studies
should be used to understand this issue.</p>
<ul>
<li>For inferences with dependent data: ⇒ Use Bonferroni tests or intervals.</li>
<li>For inferences with independent data: ⇒ Use Šidák tests or intervals.</li>
</ul>
</div>
<div id="schweder-spjøtvoll-p-value-plot" class="section level3" number="15.2.4">
<h3><span class="header-section-number">15.2.4</span> Schweder-Spjøtvoll p-Value Plot</h3>
<p>This plot, which is very useful for assessing multiplicity, depicts the relationship between values <span class="math inline">\(q=1-p\)</span> and their rank order. Specifically, if <span class="math inline">\(q_{(1)} \leq \ldots \leq q_{(k)}\)</span> are the ordered values of the <span class="math inline">\(q\)</span> ’s, then <span class="math inline">\(q_{(1)}=1-p_{(k)}, q_{(2)}=1-p_{(k-1)}\)</span>, etc. The method is to plot the <span class="math inline">\(\left(j, q_{(j)}\right)\)</span> pairs. If the hypotheses all are truly null, then the <span class="math inline">\(p\)</span> -values will behave like a sample from the uniform distribution, and the graph should lie approximately on a straight diagonal line. Deviations from linearity, particularly points in the upper-right corner of the graph that are below the extended trend line from the points in the lower-left corner, suggest hypotheses that are false, since their <span class="math inline">\(p\)</span> -values are too small to be consistent with the uniform distribution.</p>
<pre><code>*** Schweder-Spjøtvoll p-Value Plot Using PROC MULTTEST ;
data pvals1;
 input test pval @@;
 bon_adjp = min(1,10*pval);
 sid_adjp = 1 - (1-pval)**10;
datalines;
1 0.0911 2 0.8912
3 0.0001 4 0.5718
5 0.0132 6 0.9011
7 0.2012 8 0.0289
9 0.0498 10 0.0058
;

ods graphics on;
proc multtest inpvalues(pval)=pvals1 plots= RawUniformPlot;
run;
ods graphics off;</code></pre>
<div class="figure" style="text-align: center">
<img src="./02_Plots/Schweder-Spjøtvoll 1.png" alt="Figure: Schweder-Spjøtvoll (Uniform Probability) Plot" width="100%" />
<p class="caption">
(#fig:Schweder-Spjøtvoll p-Value Plot)Figure: Schweder-Spjøtvoll (Uniform Probability) Plot
</p>
</div>
<p>How does the plot look when there are no true effects</p>
<pre><code>ods graphics on;
proc multtest inpvalues(probt)=ttests plots= RawUniformPlot;
run;
ods graphics off;</code></pre>
<div class="figure" style="text-align: center">
<img src="./02_Plots/Schweder-Spjøtvoll 2.png" alt="Figure: Plot of p-Values for the Cold Study" width="100%" />
<p class="caption">
(#fig:Plot of p-Values for the Cold Study)Figure: Plot of p-Values for the Cold Study
</p>
</div>
<p><strong>Adaptive Methods</strong></p>
<p>FWE of an MCP depends upon the number of true null hypotheses, m. In order to protect the FWE in all possible circumstances, you had to protect it for the complete null hypothesis where all nulls are true (i.e., where m=k). Thus, in the Bonferroni method, you use k as a divisor for the critical value (and as a multiplier for the adjusted p-value). If you know m, the number of true nulls, then you may use m as a divisor (or multiplier for adjusted p-values) instead of k, and still control the FWE. From the examination of the Schweder-Spjøtvoll plot, you can estimate the total number of true null hypotheses <span class="math inline">\(\hat m\)</span>, and modify the critical value of the Bonferroni procedure by rejecting any hypothesis<span class="math inline">\(H_{0 j}\)</span> for which <span class="math inline">\(p_{j} \leq \alpha / \hat{m} .\)</span></p>
<p><strong>Adaptive Holm (AHOLM) method specified in the following program.</strong></p>
<pre><code>*** Estimating the Number of Null Hypotheses;
ods graphics on;
proc multtest inpvalues(pval)=pvals1
 plots= RawUniformPlot aholm;
run;
ods graphics off;</code></pre>
<div class="figure" style="text-align: center">
<img src="./02_Plots/Schweder-Spjøtvoll 3.png" alt="Figure: Estimating the Number of True Nulls Using Hochberg and Benjamini’s Method" width="100%" />
<p class="caption">
(#fig:g Hochberg and Benjamini’s)Figure: Estimating the Number of True Nulls Using Hochberg and Benjamini’s Method
</p>
</div>
</div>
</div>
<div id="mcp-among-treatment-means-in-the-one-way-balanced-anova" class="section level2" number="15.3">
<h2><span class="header-section-number">15.3</span> MCP among Treatment Means in the One-Way Balanced ANOVA</h2>
<div id="ls-means" class="section level3" number="15.3.1">
<h3><span class="header-section-number">15.3.1</span> LS-Means</h3>
<p>Least square means are means for groups that are adjusted for means of other factors in the model.</p>
<p>A least square mean, or LS-mean, is the predicted average within a certain category for a “balanced”
population; for this reason, the LS-means are also called the “estimated population marginal
means” (Searle, Speed, and Milliken, 1980).</p>
<p>LS-means correspond to Type III tests in the same way that arithmetic means correspond to
Type I tests; as with Type III tests, they are intended to be used with complicated, possibly
unbalanced models as simple means can be used with simple models. Also, as Type III tests are
identical to Type I tests for simple models, so LS-means are the same as arithmetic means when
the latter are appropriate.</p>
<blockquote>
<p>最小二乘均值或LS均值是“均衡”人群在特定类别内的预测平均值；因此，LS均值也称为“估计的人口边际均值”（Searle，Speed和Milliken，1980年）。 LS-均值对应于III型测试，其计算方式与I型测试相同。与III型测试一样，它们旨在与复杂的，可能不平衡的模型一起使用，因为简单的方法可以与简单的模型一起使用。此外，由于类型III的测试与简单模型的类型I的测试相同，因此在适当的情况下，LS均值与算术平均值相同。</p>
</blockquote>
<pre><code>*** Selling Prices of Homes;
data House;
 input Location$ Price Sqfeet Age @@;
datalines;
A 213.5 2374 4 A 219.9 2271 8 A 227.9 2088 5
A 192.5 1645 8 A 203.0 1814 6 A 242.1 2553 7
A 220.5 1921 9 A 205.5 1854 2 A 201.2 1536 9
A 194.7 1677 3 A 229.0 2342 5 A 208.7 1862 4
A 199.7 1894 7 A 212.0 1774 9 A 204.8 1476 8
A 186.1 1466 7 A 203.5 1800 8 A 193.0 1491 5
A 199.5 1749 8 A 198.1 1690 7 A 244.8 2741 5
A 196.3 1460 5 A 195.1 1614 6 A 225.8 2244 6
A 226.9 2165 6 A 204.7 1828 4 B 174.2 1503 6
B 169.9 1689 6 B 177.0 1638 2 B 167.0 1276 6
B 198.9 2101 9 B 181.2 1668 5 B 185.7 2123 4
B 199.8 2208 5 B 155.7 1273 8 B 220.1 2519 4
B 209.1 2303 6 B 182.4 1800 3 B 202.7 2336 8
B 192.0 2100 6 B 184.1 1697 4 C 190.8 1674 4
C 198.2 2307 7 C 194.6 2152 5 C 187.9 1948 9
D 202.5 2258 2 D 181.3 1965 6 D 186.1 1772 3
D 194.7 2385 1 D 164.7 1345 4 D 193.5 2220 8
D 180.1 1883 8 D 192.3 2012 6 D 180.6 1898 5
E 205.3 2362 7 E 206.3 2362 7 E 184.3 1963 9
E 176.6 1941 7 E 182.4 1975 5 E 198.8 2529 6
E 186.8 2079 5 E 188.5 2190 4 E 177.5 1897 5
E 186.9 1946 4
; </code></pre>
<p><strong>Calculate the least squares mean in R</strong></p>
<p>Remark:</p>
<ul>
<li>最小二乘均值根据参考表格（reference grid）
<ul>
<li>参考水平的组合构成参考表格</li>
<li>如果是因子，那么因子的每个水平作为参考水平；</li>
<li>如果是协变量，那么用协变量的总体均值作为参考水平；</li>
</ul></li>
<li>一旦建立了参考表格，最小二乘均值就是基于表格的简单预测，或者说是预测值列表的边际均值（marginal means）。</li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="multiple-comparison.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lsmeans)</span>
<span id="cb11-2"><a href="multiple-comparison.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(oranges)</span></code></pre></div>
<pre><code>##   store day price1 price2  sales1  sales2
## 1     1   1     37     61 11.3208  0.0047
## 2     1   2     37     37 12.9151  0.0037
## 3     1   3     45     53 18.8947  7.5429
## 4     1   4     41     41 14.6739  7.0652
## 5     1   5     57     41  8.6493 21.2085
## 6     1   6     49     33  9.5238 16.6667</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="multiple-comparison.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(oranges)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    36 obs. of  6 variables:
##  $ store : Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 2 2 2 2 ...
##  $ day   : Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 1 2 3 4 ...
##  $ price1: int  37 37 45 41 57 49 49 53 53 53 ...
##  $ price2: int  61 37 53 41 41 33 49 53 45 53 ...
##  $ sales1: num  11.32 12.92 18.89 14.67 8.65 ...
##  $ sales2: num  0.0047 0.0037 7.5429 7.0652 21.2085 ...</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="multiple-comparison.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="do">### Build a model</span></span>
<span id="cb15-2"><a href="multiple-comparison.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="do">### store and day are factor variables, so they are used as fixed effects</span></span>
<span id="cb15-3"><a href="multiple-comparison.html#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="do">### price1 and price2 are used as covariates;</span></span>
<span id="cb15-4"><a href="multiple-comparison.html#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="multiple-comparison.html#cb15-5" aria-hidden="true" tabindex="-1"></a>oranges.lm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales1 <span class="sc">~</span> price1 <span class="sc">+</span> price2 <span class="sc">+</span> store <span class="sc">+</span> day , <span class="at">data =</span> oranges)</span>
<span id="cb15-6"><a href="multiple-comparison.html#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(oranges.lm1)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: sales1
##           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## price1     1 516.59  516.59 29.0996 1.763e-05 ***
## price2     1  62.73   62.73  3.5334  0.072873 .  
## store      5 212.95   42.59  2.3991  0.068548 .  
## day        5 433.10   86.62  4.8793  0.003456 ** 
## Residuals 23 408.31   17.75                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="multiple-comparison.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="do">### Create a reference Grid</span></span>
<span id="cb17-2"><a href="multiple-comparison.html#cb17-2" aria-hidden="true" tabindex="-1"></a>oranges.rg1 <span class="ot">&lt;-</span> <span class="fu">ref.grid</span>(oranges.lm1)</span>
<span id="cb17-3"><a href="multiple-comparison.html#cb17-3" aria-hidden="true" tabindex="-1"></a>oranges.rg1</span></code></pre></div>
<pre><code>## &#39;emmGrid&#39; object with variables:
##     price1 = 51.222
##     price2 = 48.556
##     store = 1, 2, 3, 4, 5, 6
##     day = 1, 2, 3, 4, 5, 6</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="multiple-comparison.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="do">### Obtain the predicted value of different reference level combinations</span></span>
<span id="cb19-2"><a href="multiple-comparison.html#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="do">### Using summary() or predict()</span></span>
<span id="cb19-3"><a href="multiple-comparison.html#cb19-3" aria-hidden="true" tabindex="-1"></a>oranges.rg1.prediction <span class="ot">&lt;-</span> <span class="fu">summary</span>(oranges.rg1)</span>
<span id="cb19-4"><a href="multiple-comparison.html#cb19-4" aria-hidden="true" tabindex="-1"></a>oranges.rg1.prediction</span></code></pre></div>
<pre><code>##  price1 price2 store day prediction   SE df
##    51.2   48.6 1     1         2.92 2.72 23
##    51.2   48.6 2     1         4.96 2.38 23
##    51.2   48.6 3     1         3.20 2.38 23
##    51.2   48.6 4     1         6.20 2.36 23
##    51.2   48.6 5     1         5.54 2.36 23
##    51.2   48.6 6     1        10.56 2.37 23
##    51.2   48.6 1     2         3.85 2.70 23
##    51.2   48.6 2     2         5.89 2.34 23
##    51.2   48.6 3     2         4.13 2.34 23
##    51.2   48.6 4     2         7.13 2.35 23
##    51.2   48.6 5     2         6.47 2.33 23
##    51.2   48.6 6     2        11.49 2.34 23
##    51.2   48.6 1     3        11.02 2.53 23
##    51.2   48.6 2     3        13.06 2.42 23
##    51.2   48.6 3     3        11.30 2.42 23
##    51.2   48.6 4     3        14.30 2.43 23
##    51.2   48.6 5     3        13.64 2.36 23
##    51.2   48.6 6     3        18.66 2.35 23
##    51.2   48.6 1     4         6.10 2.65 23
##    51.2   48.6 2     4         8.14 2.35 23
##    51.2   48.6 3     4         6.38 2.35 23
##    51.2   48.6 4     4         9.38 2.39 23
##    51.2   48.6 5     4         8.72 2.34 23
##    51.2   48.6 6     4        13.74 2.34 23
##    51.2   48.6 1     5        12.80 2.44 23
##    51.2   48.6 2     5        14.84 2.47 23
##    51.2   48.6 3     5        13.08 2.47 23
##    51.2   48.6 4     5        16.08 2.52 23
##    51.2   48.6 5     5        15.42 2.40 23
##    51.2   48.6 6     5        20.44 2.37 23
##    51.2   48.6 1     6         8.75 2.79 23
##    51.2   48.6 2     6        10.79 2.34 23
##    51.2   48.6 3     6         9.03 2.34 23
##    51.2   48.6 4     6        12.03 2.36 23
##    51.2   48.6 5     6        11.37 2.35 23
##    51.2   48.6 6     6        16.39 2.37 23</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="multiple-comparison.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="do">### Get LS Mean for day</span></span>
<span id="cb21-2"><a href="multiple-comparison.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lsmeans</span>(oranges.rg1,<span class="st">&quot;day&quot;</span>)</span></code></pre></div>
<pre><code>##  day lsmean   SE df lower.CL upper.CL
##  1     5.56 1.77 23     1.91     9.22
##  2     6.49 1.73 23     2.92    10.07
##  3    13.66 1.75 23    10.04    17.29
##  4     8.74 1.73 23     5.16    12.33
##  5    15.44 1.79 23    11.75    19.14
##  6    11.39 1.77 23     7.74    15.05
## 
## Results are averaged over the levels of: store 
## Confidence level used: 0.95</code></pre>
<p><strong>coefficients</strong></p>
<p>Construct confidence intervals for and perform hypothesis tests on linear combinations using the ESTIMATE statement;
The ESTIMATE statement specifies the coefficients in the vector <span class="math inline">\(\mathbf{c}\)</span> that define the linear combination <span class="math inline">\(\mathbf{c}^{\prime} \boldsymbol{\beta}\)</span> that you want to estimate.</p>
<pre><code>proc glm data=House;
 class Location;
 model Price = Location Sqfeet Age;
 estimate &#39;gamma&#39; Intercept 1 Location 0 0 0 0 0 Sqfeet 0 Age 0 ;
 estimate &#39;m1-m2&#39; Intercept 0 Location 1 -1 0 0 0 Sqfeet 0 Age 0 ;
run; quit; </code></pre>
<p><strong>Inference for Estimable Linear Combinations</strong></p>
<p><span class="math display">\[
\frac{\mathbf{c}^{\prime} \hat{\beta}-\mathbf{c}^{\prime} \boldsymbol{\beta}}{\text { s.e. }\left(\mathbf{c}^{\prime} \boldsymbol{\beta}\right)} \sim t_{d / \varepsilon}
\]</span>
where the standard error of <span class="math inline">\(\mathbf{c}^{\prime} \hat{\boldsymbol{\beta}}\)</span> is
<span class="math display">\[
\text { s.e. }\left(\mathbf{c}^{\prime} \hat{\boldsymbol{\beta}}\right)=\hat{\sigma} \sqrt{\mathbf{c}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{c}} \text { . }
\]</span>
The <span class="math inline">\(t\)</span> -statistic for testing <span class="math inline">\(H_{0}: \mathbf{c}^{\prime} \boldsymbol{\beta}=0\)</span> is then
<span class="math display">\[
t=\frac{\mathbf{c}^{\prime} \hat{\boldsymbol{\beta}}}{\text { s.e. }\left(\mathbf{c}^{\prime} \hat{\boldsymbol{\beta}}\right)}
\]</span>
and the two-sided <span class="math inline">\(p\)</span> -value is
<span class="math display">\[
p=P\left(\left|T_{d j e}\right| \geq|t|\right)=2 P\left(T_{d f e} \geq|t|\right) .
\]</span></p>
<p>In order to compute the confidence interval for the ‘m1-m2’ linear combination, use the
CLPARM option on the MODEL statement, as in the following program.</p>
<pre><code>proc glm data=House;
 class Location;
 model Price = Location Sqfeet Age / clparm;
 estimate &#39;m1-m2&#39; Intercept 0 Location 1 -1 0 0 0 Sqfeet 0 Age 0 ;
run; quit;</code></pre>
</div>
<div id="the-multivariate-t-distribution" class="section level3" number="15.3.2">
<h3><span class="header-section-number">15.3.2</span> The Multivariate t Distribution</h3>
<p>Most of the classical MCPs fall under the general umbrella of “MaxT methods”; that is, they are based on the distribution of the <strong>maximum of multiple t-statistics.</strong></p>
<p>Confidence intervals for the estimable functions <span class="math inline">\(\mathbf{c}_{i}^{\prime} \boldsymbol{\beta}\)</span> have the form
<span class="math display">\[
\mathbf{c}^{\prime} \hat{\boldsymbol{\beta}} \pm c_{\alpha} s . e .\left(\mathbf{c}^{\prime} \hat{\boldsymbol{\beta}}\right)
\]</span>
where <span class="math inline">\(c_{\alpha}\)</span> is a critical value that is selected to make the <span class="math inline">\(\mathrm{FWE}=\alpha\)</span> for the set of confidence intervals for the family <span class="math inline">\(\mathbf{c}_{1}^{\prime} \boldsymbol{\beta}, \mathbf{c}_{2}^{\prime} \boldsymbol{\beta}, \ldots, \mathbf{c}_{k}^{\prime} \boldsymbol{\beta}\)</span>.</p>
<p>To find the right <span class="math inline">\(c_{\alpha}\)</span>, you can use the joint distribution of the statistics
<span class="math display">\[
T_{i}=\frac{\mathbf{c}_{i}^{\prime} \hat{\boldsymbol{\beta}}-\mathbf{c}_{i}^{\prime} \boldsymbol{\beta}}{\operatorname{s.e.}\left(\mathbf{c}_{i}^{\prime} \hat{\boldsymbol{\beta}}\right)}, \quad i=1, \ldots, k
\]</span>
The collection of random variables <span class="math inline">\(\left\{T_{1}, T_{2}, \ldots, T_{k}\right\}\)</span> has the multivariate <span class="math inline">\(t\)</span> distribution when the classical linear model assumptions are valid.</p>
<p>If <span class="math inline">\(\mathbf{Z}=\left(Z_{1}, \ldots, Z_{k}\right)\)</span> is distributed as multivariate normal with zero mean with known covariance matrix <span class="math inline">\(\mathbf{R}\)</span>, and if <span class="math inline">\(V\)</span> is distributed as Chi-Square with <span class="math inline">\(d f\)</span> degrees of freedom, independent of <span class="math inline">\(\mathbf{Z}\)</span>, then
<span class="math display">\[
\mathbf{T}=\frac{\mathbf{Z}}{\sqrt{V / d f}}
\]</span>
has the multivariate <span class="math inline">\(t\)</span> distribution with dispersion matrix <span class="math inline">\(\mathbf{R}\)</span> and degrees of freedom <span class="math inline">\(d f\)</span>.</p>
<p>First, define the contrast matrix
<span class="math display">\[
\mathbf{C}=\left(\mathbf{c}_{1}, \ldots, \mathbf{c}_{k}\right)
\]</span>
Then you can write the estimates of the set of <span class="math inline">\(k\)</span> estimable linear combinations as the <span class="math inline">\(k \times 1\)</span> vector <span class="math inline">\(\mathbf{C}^{\prime} \hat{\boldsymbol{\beta}}\)</span>, which is distributed as multivariate normal when the assumptions are true:
<span class="math display">\[
\mathbf{C}^{\prime} \hat{\boldsymbol{\beta}} \sim \boldsymbol{N}_{k}\left(\mathbf{C}^{\prime} \boldsymbol{\beta}, \sigma^{2} \mathbf{C}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{C}\right)
\]</span>
(If the X matrix contains random variables, then this is the conditional distribution, given the observed <span class="math inline">\(\mathbf{X}\)</span>.)
From this expression you can derive <span class="math inline">\(Z\)</span> -statistics:
<span class="math display">\[
Z_{i}=\frac{\mathbf{c}_{i}^{\prime} \hat{\boldsymbol{\beta}}-\mathbf{c}_{i}^{\prime} \boldsymbol{\beta}}{\sigma \sqrt{\mathbf{c}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{c}}} \sim N(0,1)
\]</span>
You can write the entire set of <span class="math inline">\(Z\)</span> -statistics in matrix/vector notation as
<span class="math display">\[
\mathbf{Z}=\left(\sigma^{2} \mathbf{D}\right)^{-1 / 2}\left(\mathbf{C}^{\prime} \hat{\boldsymbol{\beta}}-\mathbf{C}^{\prime} \boldsymbol{\beta}\right),
\]</span>
where <span class="math inline">\(\mathbf{D}\)</span> is the diagonal matrix having diagonal elements <span class="math inline">\(\mathbf{c}_{i}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{c}_{i}\)</span>, thus
<span class="math display">\[
\mathbf{Z} \sim N_{k}\left(0, \mathbf{D}^{-1 / 2} \mathbf{C}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{C D}^{-1 / 2}\right)
\]</span>
The <span class="math inline">\(\mathbf{R}\)</span> matrix is the <strong>covariance matrix</strong> of the <span class="math inline">\(\mathbf{Z}\)</span> vector:
<span class="math display">\[
\mathbf{R}=\mathbf{D}^{-1 / 2} \mathbf{C}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{C} \mathbf{D}^{-1 / 2}
\]</span></p>
<p>Notice that
- <span class="math inline">\(\mathbf{R}\)</span> is the correlation matrix of the <span class="math inline">\(Z \mathrm{~s}\)</span> as there are <span class="math inline">\(1 \mathrm{~s}\)</span> on the diagonal.
- <span class="math inline">\(\mathbf{R}\)</span> is a known matrix, depending on no unknown parameters.
- The correlations between the <span class="math inline">\(Z\)</span> ’s depend on
+ the set of linear combinations to be estimated (determined by C), and
+ the design of the study and the model used for the analysis (determined by <span class="math inline">\(\mathbf{X}\)</span> ).</p>
<p>Now, to get the vector of <span class="math inline">\(t\)</span> statistics, you can write
<span class="math display">\[
\mathbf{T}=\frac{\mathbf{Z}}{s / \sigma}
\]</span>
To remove the <span class="math inline">\(\sigma\)</span> in the <span class="math inline">\(Z\)</span> statistic and replace it with <span class="math inline">\(s\)</span>.
Under the model assumptions,
<span class="math display">\[
\frac{d f \times s^{2}}{\sigma^{2}} \sim \chi_{d f}^{2}
\]</span>
and is independent of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, thus establishing the representation of <span class="math inline">\(\mathbf{T}\)</span> as
<span class="math display">\[
\mathbf{T}=\frac{\mathbf{Z}}{\sqrt{V / d f}}
\]</span>
and hence that it has the multivariate <span class="math inline">\(t\)</span> distribution with dispersion matrix <span class="math inline">\(\mathbf{R}=\mathbf{D}^{-1 / 2} \mathbf{C}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{C} \mathbf{D}^{-1 / 2}\)</span></p>
<p><strong>Obtaining the R Matrix for Multiple Comparisons</strong></p>
<pre><code>proc orthoreg data=House;
 class Location;
 model Price = Location Sqfeet Age;
 lsmestimate Location
 &#39;m1-m2&#39; 1 -1 0 0 0,
 &#39;m2-m3&#39; 0 1 -1 0 0,
 &#39;m3-m4&#39; 0 0 1 -1 0,
 &#39;m4-m5&#39; 0 0 0 1 -1 / corr;
run;</code></pre>
</div>
<div id="calculating-the-critical-value-c_alpha" class="section level3" number="15.3.3">
<h3><span class="header-section-number">15.3.3</span> Calculating the Critical Value <span class="math inline">\(c_{\alpha}\)</span></h3>
<p>Suppose for simplicity that the critical value <span class="math inline">\(c_{\alpha}\)</span> is for two-sided intervals and tests. There are many ways that you can find it, or at least approximate it. First, you might try to integrate the multivariate <span class="math inline">\(t\)</span> distribution and solve for <span class="math inline">\(c_{\alpha}\)</span> :
<span class="math display">\[
\int_{-c_{\alpha}}^{c_{c}} \ldots \int_{-c_{\alpha}}^{c_{c}} f\left(t_{1}, \ldots, t_{k} ; d f, \mathbf{R}\right) d t_{1} \ldots d t_{k}=1-\alpha
\]</span>
This approach is often impractical because the complicated form of the multivariate <span class="math inline">\(t\)</span> distribution function <span class="math inline">\(f\left(t_{1}, \ldots, t_{k} ; d f, \mathbf{R}\right)\)</span> precludes analytical integration; numerical integration methods also can founder if there are too many dimensions <span class="math inline">\(k\)</span>.</p>
<p>The value <span class="math inline">\(c_{\alpha}\)</span> is found the following ways:</p>
<ul>
<li><strong>Exact Analytic Solution</strong>: When the data are balanced and the comparisons are simple, the multivariate <span class="math inline">\(t\)</span> integral simplifies and is solvable in terms of known and special mathematical distribution functions like “Tukey’s studentized range distribution” and “Dunnett’s range distribution”<br />
</li>
<li><strong>Conservative Analytic Solution</strong>: In some cases with unbalanced data and/or more complex comparisons, the exact analytic solution provides a conservative solution in that the <span class="math inline">\(c_{\alpha}\)</span> is larger than it needs to be.</li>
<li><strong>Approximate Analytic Solution</strong>: In some cases, with unbalanced data and/or more complex comparisons, the exact analytic solution provides an approximate solution in that the <span class="math inline">\(c_{\alpha}\)</span> is perhaps larger than it needs to be, or perhaps smaller.</li>
<li><strong>Simple Monte Carlo Solution</strong>: By simulating many multivariate <span class="math inline">\(t\)</span> vectors, you can estimate the <span class="math inline">\(1-\alpha\)</span> quantile of <span class="math inline">\(\max T\)</span>.</li>
<li><strong>Control Variate Monte Carlo Solution</strong>: This method also proceeds by simulating multivariate <span class="math inline">\(t\)</span> vectors, but then using control variates to reduce the variance of the estimate of the <span class="math inline">\(1-\alpha\)</span> quantile of <span class="math inline">\(\max \mathrm{T}\)</span>.</li>
<li><strong>Quasi-Monte Carlo Solution</strong>: This method proceeds by approximating the multiple integral shown above by using a systematic grid of <span class="math inline">\(t\)</span> vectors. The method can often provide much better accuracy with far fewer <span class="math inline">\(t\)</span> vectors (Genz and Bretz, 2009).</li>
</ul>
</div>
<div id="all-pairwise-comparisons-and-studentized-range-distribution" class="section level3" number="15.3.4">
<h3><span class="header-section-number">15.3.4</span> All Pairwise Comparisons and Studentized Range Distribution</h3>
<p>In general, there are such comparisons.</p>
<p><span class="math display">\[
\left(\begin{array}{l}
g \\
2
\end{array}\right)=\frac{g !}{2 !(g-2) !}=\frac{g(g-1)}{2}
\]</span>
For all simultaneous pairwise comparisons <span class="math inline">\(\mu_{i}-\mu_{i^{\prime}}, 1 \leq i, i^{\prime} \leq g\)</span>, the critical value <span class="math inline">\(c_{\alpha}\)</span> must satisfy
<span class="math display">\[
P\left(\bar{y}_{i}-\bar{y}_{i^{\prime}}-c_{\alpha} \hat{\sigma} \sqrt{2 / n} \leq \mu_{i}-\mu_{i^{\prime}} \leq \bar{y}_{i}-\bar{y}_{i^{\prime}}+c_{\alpha} \hat{\sigma} \sqrt{2 / n}, \text { for all } i, i^{\prime}\right)=1-\alpha
\]</span>
or equivalently
<span class="math display">\[
P\left(\max _{i, i^{\prime}} \frac{\left|\left(\bar{y}_{i}-\mu_{i}\right)-\left(\bar{y}_{i^{\prime}}-\mu_{i^{\prime}}\right)\right|}{\hat{\sigma} \sqrt{2 / n}} \leq c_{\alpha}\right)=1-\alpha
\]</span></p>
<p>This formula shows the “MaxT.” In the balanced ANOVA, the
MaxT statistic has a particularly simple form because the denominator standard error
<span class="math inline">\(\hat{\sigma} \sqrt{2 / n}\)</span> is the same for all <span class="math inline">\(t\)</span> -statistics. This simplification, along with the special structure of the set of all pairwise comparisons, allows for <span class="math inline">\(c_{\alpha}\)</span> to be calculated analytically from the studentized range distribution. When the standard errors differ for the various <span class="math inline">\(t\)</span> -statistics, more complex approximations such as simulation-based methods are needed.</p>
<p><strong>Studentized Range Distribution</strong></p>
<p>If <span class="math inline">\(Z_{1}, \ldots, Z_{g}\)</span> are independent standard normal random variables, and <span class="math inline">\(V\)</span> is a random variable distributed as chi-square with <span class="math inline">\(v\)</span> degrees of freedom, independent of the <span class="math inline">\(Z \mathrm{~s}\)</span>, then
<span class="math display">\[
Q_{g, v}^{R}=\max _{i, i^{\prime}} \frac{\left|Z_{i}-Z_{i^{\prime}}\right|}{\sqrt{V / v}}
\]</span>
has the studentized range distribution with parameters <span class="math inline">\(g\)</span> and <span class="math inline">\(r\)</span>.
With this definition and some algebraic manipulation, along with well-known results concerning distributions involving normally distributed variables, you can show that <span class="math inline">\(c_{\alpha}\)</span> satisfies
<span class="math display">\[
P\left(\frac{Q_{g, g(n-1)}^{R}}{\sqrt{2}} \leq c_{\alpha}\right)=1-\alpha
\]</span>
or equivalently that
<span class="math display">\[
c_{\alpha}=\frac{q_{1-\alpha, g, g(n-1)}^{R}}{\sqrt{2}}
\]</span>
where <span class="math inline">\(q_{1-\alpha_{m}}^{R}\)</span> is the <span class="math inline">\(1-\alpha\)</span> quantile of the studentized range distribution.</p>
<p><strong>“Hand Calculation” of Studentized Range Critical Value</strong></p>
<p>The quantiles <span class="math inline">\(q_{1-\alpha_{\alpha}}^{R}\)</span> of the studentized range distribution can be calculated using the PROBMC
function in SAS, which evaluates the cumulative probability distribution function of the random variable <span class="math inline">\(Q_{g, v}^{R} .\)</span></p>
<pre><code>data;
 qval = probmc(&quot;RANGE&quot;,.,.95,45,5);
 c_alpha = qval/sqrt(2);
run; </code></pre>
<p><strong>Implementation</strong></p>
<p>Obtaining Pairwise Comparisons Using the LSMEANS Statement</p>
<pre><code>proc glm data=House;
 class Location;
 model Price = Location Sqfeet Age;
 lsmeans Location / tdiff;
run; quit;</code></pre>
<p>Pairwise Comparisons with a Control, For example, perhaps region B is considered the premium region, and you wish
to know how other regions compare with it.</p>
<pre><code>proc glm data=House;
 class Location;
 model Price = Location Sqfeet Age;
 lsmeans Location / tdiff=control(&#39;B&#39;);
run; quit;</code></pre>
</div>
<div id="tukeys-method-for-all-pairwise-comparisons" class="section level3" number="15.3.5">
<h3><span class="header-section-number">15.3.5</span> Tukey’s Method for All Pairwise Comparisons</h3>
<p>Confidence intervals for all pairwise comparisons in the balanced ANOVA that use the critical value <span class="math inline">\(c_{\alpha}=q_{1-\alpha, g, g(n-1)}^{R} / \sqrt{2}\)</span> from the studentized range distribution are commonly said to be constructed by “Tukey’s Method,” after Tukey (1953). The intervals may also be called “Tukey intervals” in this case. When testing hypotheses <span class="math inline">\(H_{0}: \mu_{i}-\mu_{i^{\prime}}=0\)</span>, either by checking to see if 0 is inside the Tukey interval or by comparing <span class="math inline">\(\left|t_{i, i^{\prime}}\right|\)</span> to <span class="math inline">\(c_{\alpha}=q_{1-\alpha, g, g(n-1)}^{R} / \sqrt{2}\)</span>, the tests are called “Tukey tests.”</p>
<p><strong>Compare the Tukey intervals with the Bonferroni intervals</strong></p>
<p>Since there are <span class="math inline">\(5 \times 4 / 2=10\)</span> pairwise comparisons among the five groups, the Bonferroni critical value uses <span class="math inline">\(\alpha^{\prime}=0.05 / 10=0.005\)</span>, and the critical value nnis <span class="math inline">\(t_{0.9975,45}=2.9521\)</span>. The reason for the difference between the Bonferroni critical value and the Tukey critical value, <span class="math inline">\(2.9521\)</span> vs. <span class="math inline">\(2.84145\)</span>, is that the Tukey critical value is <strong>based on the precise distribution of the 10 pairwise statistics</strong> <span class="math inline">\(\left\{\left(\bar{y}_{i}-\mu_{i}\right)-\left(\bar{y}_{i^{\prime}}-\mu_{i^{\prime}}\right)\right\} /(\hat{\sigma} \sqrt{2 / n}) .\)</span> There are correlations among these statistics because there are
many common random elements. For example, the statistics <span class="math inline">\(\left\{\left(\bar{y}_{1}-\mu_{1}\right)-\left(\bar{y}_{2}-\mu_{2}\right)\right\} /(\hat{\sigma} \sqrt{2 / n})\)</span> and <span class="math inline">\(\left\{\left(\bar{y}_{1}-\mu_{1}\right)-\left(\bar{y}_{3}-\mu_{3}\right)\right\} /(\hat{\sigma} \sqrt{2 / n})\)</span> are <strong>correlated</strong> because both contain the common random elements <span class="math inline">\(\bar{y}_{1}\)</span> and <span class="math inline">\(\hat{\sigma}\)</span>.</p>
<p>In summary, Tukey’s intervals control the FWE precisely (under the assumptions of the model), while the Bonferroni intervals over-control and the unadjusted intervals under-control.</p>
<p><strong>SAS Implementation</strong></p>
<pre><code>***  PROC GLM Calculation of Tukey Adjusted p-Values;
proc glm data=Wloss;
 class Diet;
 model Wloss=Diet;
 lsmeans Diet/pdiff adjust=tukey;
run; quit;</code></pre>
<p><strong>Simultaneous Intervals for Mean Differences</strong></p>
<ul>
<li>Unadjusted Intervals</li>
<li>Bonferroni Intervals</li>
<li>Tukey Intervals</li>
</ul>
<pre><code>proc glm data=Wloss;
 class Diet;
 model Wloss=Diet;
 means Diet/cldiff t bon tukey;
run; </code></pre>
<p><strong>R Implementation</strong></p>
<pre><code>## contrMat specify other contrast matrices in advance, such as &quot;Dunnett&quot;, &quot;Williams&quot;
## Tukey 1
data(warpbreaks)
warpbreaks.aov &lt;- aov(breaks ~ tension, data = warpbreaks)
warpbreaks.mc &lt;- glht(warpbreaks.aov, 
                      linfct = mcp(tension = &quot;Tukey&quot;))
                      
## alternative 1
glht(warpbreaks.aov,linfct = mcp(tension = c(&quot;M - L = 0&quot;,
                                             &quot;H - L = 0&quot;,
                                             &quot;H - M = 0&quot;)))

## alternative 2                  
contr &lt;- rbind(&quot;M - L&quot; = c(-1,  1, 0),
               &quot;H - L&quot; = c(-1,  0, 1),
               &quot;H - M&quot; = c( 0, -1, 1));
glht(warpbreaks.aov, linfct = mcp(tension = contr))

## alternative 3
glht(warpbreaks.aov,
     linfct = cbind(0, contr %*% contr.treatment(3)))
     
     
## Multiple Comparisons of Means: Tukey Contrasts
## Linear Hypotheses:
##           Estimate
## M - L == 0  -10.000
## H - L == 0  -14.722
## H - M == 0   -4.722


## Calculate and plot simultaneous confidence intervals
warpbreaks.ci &lt;- confint(warpbreaks.mc, level = 0.95)
warpbreaks.ci
 
## Unadjusted (marginal) confidence interval
confint(warpbreaks.mc, calpha = univariate_calpha())</code></pre>
<p><strong>The Tukey Adjusted p-Value</strong></p>
<p><span class="math display">\[
\tilde{p}_{i, i^{\prime}}=P\left(Q_{g, g(n-1)}^{R} \geq \sqrt{2}\left|t_{i, i}\right|\right)
\]</span>
By comparison, the ordinary (unadjusted) <span class="math inline">\(p\)</span> -value is given by <span class="math inline">\(p_{i, i^{\prime}}=2 P\left(T_{g(n-1)} \geq t_{i, i} \mid\right)\)</span>, where <span class="math inline">\(T_{V}\)</span> denotes a Student’s <span class="math inline">\(t\)</span> -distributed random variable with <span class="math inline">\(v\)</span> degrees of freedom (here, <span class="math inline">\(v=g(n-1))\)</span>.</p>
<pre><code>**** “By Hand” Calculation of Raw and Tukey Adjusted p-Values;
data;
 n=10; g=5; df=g*(n-1);
 Mean_A=12.05; Mean_B=11.02; MSE=0.993422;
 tstat_AB = (Mean_A-Mean_B)/(sqrt(MSE)*sqrt(2/n));
 raw_p = 2*(1-probt(abs(tstat_AB),df));
 adj_p = 1-probmc(&#39;RANGE&#39;,sqrt(2)*abs(tstat_AB),.,df,g);
run; 
</code></pre>
</div>
<div id="displaying-pairwise-comparisons-graphically" class="section level3" number="15.3.6">
<h3><span class="header-section-number">15.3.6</span> Displaying Pairwise Comparisons Graphically</h3>
<p><strong>Graphical Presentation for Comparing Means: LINES Option SAS</strong></p>
<p>LINES option, which provides a listing of the means in descending order
and a text graph that displays the results of the tests.</p>
<pre><code>proc glm data=Wloss;
 class Diet;
 model Wloss=Diet;
 means Diet/tukey lines;
run; </code></pre>
<p><strong>Graphical Presentation for Comparing Means: The Diffogram</strong></p>
<p>An alternative presentation of the simultaneous confidence intervals is known as the mean-mean scatterplot (Hsu, 1996); in SAS, it is called a diffogram. First, all non-redundant pairs <span class="math inline">\(\left(\bar{y}_{i}, \bar{y}_{i^{\prime}}\right)\)</span>
are plotted on a two-dimensional plot. Then the confidence intervals are represented as <span class="math inline">\(-45^{\circ}\)</span> lines emanating symmetrically from the centers <span class="math inline">\(\left(\bar{y}_{i}, \bar{y}_{i}\right)\)</span>, scaled in such a way that the line covers the <span class="math inline">\(45^{\circ}\)</span> line when the interval covers 0 ; see Figure <span class="math inline">\(4.2\)</span> below.</p>
<pre><code>ods graphics on;
proc glm data=Wloss;
 class Diet;
 model Wloss=Diet;
 lsmeans Diet/cl adjust=tukey;
run;
quit;
ods graphics off; </code></pre>
<div class="figure" style="text-align: center">
<img src="02_Plots/Turkey_Diffgram.PNG" alt="Figure: Diffogram indicating Comparisons of Diets" width="100%" />
<p class="caption">
(#fig:Turkey Diffogram)Figure: Diffogram indicating Comparisons of Diets
</p>
</div>
<p><strong>R Implementation</strong></p>
<pre><code>## CI Plot
plot(warpbreaks.ci, main = &quot;&quot;, 
     ylim = c(0.5, 3.5),xlab = &quot;Breaks&quot;)</code></pre>
<div class="figure" style="text-align: center">
<img src="02_Plots/Turkey_Plots.png" alt="Figure: Turkey CI Plots" width="100%" />
<p class="caption">
(#fig:Turkey CI Plots)Figure: Turkey CI Plots
</p>
</div>
<pre><code>## CI Boxplots
warpbreaks.cld &lt;- cld(warpbreaks.mc)
plot(warpbreaks.cld)</code></pre>
<div class="figure" style="text-align: center">
<img src="02_Plots/Turkey_Boxplots.png" alt="Figure: CI Boxplots" width="100%" />
<p class="caption">
(#fig:Turkey CI Boxplots)Figure: CI Boxplots
</p>
</div>
</div>
<div id="dunnetts-two-sided-comparisons-with-a-control-and-dunnetts-two-sided-range-distribution" class="section level3" number="15.3.7">
<h3><span class="header-section-number">15.3.7</span> Dunnett’s Two-Sided Comparisons with a Control and Dunnett’s Two-Sided Range Distribution</h3>
<p>If you want to make a claim about whether the treated groups’ means are either larger or
smaller than the control group mean, then you should use two-sided intervals.</p>
<p><span class="math inline">\(\bar{y}_{0}\)</span> denote the mean of the control group, you need a <span class="math inline">\(c_{\alpha}\)</span> for which
<span class="math display">\[
P\left(\bar{y}_{i}-\bar{y}_{0}-c_{\alpha} \hat{\sigma} \sqrt{2 / n} \leq \mu_{i}-\mu_{0} \leq \bar{y}_{i}-\bar{y}_{0}+c_{\alpha} \hat{\sigma} \sqrt{2 / n}, \text { for all } i\right)=1-\alpha
\]</span>
Algebraically rearranging terms, you see that <span class="math inline">\(c_{\alpha}\)</span> must satisfy
<span class="math display">\[
P\left(\max _{i} \frac{\left|\left(\bar{y}_{i}-\mu_{i}\right)-\left(\bar{y}_{0}-\mu_{0}\right)\right|}{\hat{\sigma} \sqrt{2 / n}} \leq c_{\alpha}\right)=1-\alpha
\]</span>
the critical value <span class="math inline">\(c_{\alpha}\)</span> can be calculated analytically from Dunnett’s two-sided range distribution.</p>
<p><strong>Dunnett’s two-sided range distribution</strong></p>
<p>If <span class="math inline">\(Z_{0}, Z_{1}, \ldots, Z_{g}\)</span> are independent standard normal random variables, and <span class="math inline">\(V\)</span> is a random variable
distributed as chi-square with <span class="math inline">\(v\)</span> degrees of freedom, independent of the <span class="math inline">\(Z \mathrm{~s}\)</span>, then
<span class="math display">\[
Q_{g, v}^{D 2}=\frac{\max _{i}\left|Z_{i}-Z_{0}\right|}{\sqrt{2 V / v}}
\]</span>
has Dunnett’s two-sided range distribution with parameters <span class="math inline">\(g\)</span> and <span class="math inline">\(v\)</span>.</p>
<pre><code>*** “By Hand Calculation” of Dunnett&#39;s Two-Sided Critical Value;
data;
 c_alpha = probmc(&quot;DUNNETT2&quot;,.,.95,21,6);
run;
proc print;
run;</code></pre>
<pre><code>### Calculate critical values for Dunnett procedure given alpha, df1 and df2 in R;

qDunnett &lt;- function (p, df, k, rho,
                      type = c(&quot;two-sided&quot;, &quot;one-sided&quot;))
{
  type &lt;- match.arg(type)
  alpha &lt;- 1 - p
  if (type == &quot;two-sided&quot;) {
    alpha &lt;- alpha/2
  }
  S &lt;- matrix(rho, nrow=k, ncol=k) + (1-rho)*diag(k)
  if (type == &quot;two-sided&quot;) {
    f &lt;- function(d, df, k, S, p) {
      mnormt::sadmvt(df=df, lower=rep(-d,k), upper=rep(d,k),
                      mean=rep(0,k), S=S, maxpts=2000*k) - p
    }
  }
  else {
    f &lt;- function(d, df, k, S, p) {
      mnormt::pmt(d, S=S, df=df) - p
    }
  }
  d &lt;- uniroot(f,
               df = df, k = k, S = S, p=p,
               lower=qt(1 - alpha, df),
               upper=qt(1 - alpha/k, df),
               tol=.Machine$double.eps, maxiter=5000)$root
  return(d)
}


p &lt;- 0.95; df &lt;- 24; rho &lt;- 0.5; k &lt;- 3
nCDunnett::qNCDun(p=p, nu=df, rho=rho,
                  delta=rep(0,times=k), two.sided=T)
qDunnett(p, df, k, rho)</code></pre>
<p><strong>SAS Implementation</strong></p>
<pre><code>data Tox;
 input Trt @;
 do j = 1 to 4;
 input Gain @; output;
 end;
datalines;
0 97.76 102.56 96.08 125.12
1 91.28 129.20 90.80 72.32
2 67.28 85.76 95.60 73.28
3 80.24 64.88 64.88 78.56
4 96.08 98.24 77.84 95.36 
5 57.68 89.84 98.48 92.72
6 68.72 85.28 68.72 74.24
;

*** Boxplots;
ods graphics on;
proc glm data=Tox;
 class Trt;
 model Gain=Trt;
 means Trt/dunnett;
run; quit;
ods graphics off;</code></pre>
<p><strong>Displaying Two-Sided Dunnett Comparisons Graphically</strong></p>
<pre><code>means Trt/dunnett;

*** Or;
lsmeans Trt/adjust=dunnett;</code></pre>
<p><strong>R Implementation</strong></p>
<pre><code>data(&quot;recovery&quot;, package = &quot;multcomp&quot;)
recovery.aov &lt;- aov(minutes ~ blanket, data = recovery)
recovery.mc &lt;- glht(recovery.aov,
                    linfct = mcp(blanket = &quot;Dunnett&quot;),
                    alternative = &quot;less&quot;)   ## one-sided test
                    
## Alternative
contr &lt;- rbind(&quot;b1 -b0&quot; = c(-1, 1, 0, 0), 
               &quot;b2 -b0&quot; = c(-1, 0, 1, 0),
               &quot;b3 -b0&quot; = c(-1, 0, 0, 1))
summary(glht(recovery.aov, linfct = mcp(blanket = contr),
             alternative = &quot;less&quot;))
             
## KI und Plot
recovery.ci &lt;- confint(recovery.mc, level = 0.95)</code></pre>
<p><strong>Specify linear combination variante from Dunnett</strong></p>
<pre><code>## Variante from Dunnett
contr2 &lt;- rbind(&quot;b2 -b0&quot; = c(-1,  0, 1, 0),
                &quot;b2 -b1&quot; = c( 0, -1, 1, 0),
                &quot;b3 -b0&quot; = c(-1,  0, 0, 1),
                &quot;b3 -b1&quot; = c( 0, -1, 0, 1))
summary(glht(recovery.aov, linfct = mcp(blanket = contr2),
             alternative = &quot;less&quot;))
             
             
Linear Hypotheses:
            Estimate Std. Error t value Pr(&lt;t)    
b2 -b0 &gt;= 0  -7.4667     1.6038  -4.656 &lt;0.001 ***
b2 -b1 &gt;= 0  -5.3333     2.1150  -2.522 0.0278 *  
b3 -b0 &gt;= 0  -1.6667     0.8848  -1.884 0.1054    
b3 -b1 &gt;= 0   0.4667     1.6383   0.285 0.9150    </code></pre>
</div>
<div id="dunnetts-one-sided-comparisons-with-a-control" class="section level3" number="15.3.8">
<h3><span class="header-section-number">15.3.8</span> Dunnett’s One-Sided Comparisons with a Control</h3>
<p>If you want to reject the null hypothesis only when the treated groups’ means are on one side of
(e.g., lower than) the control group mean, and if you are willing to <em>ignore any difference in the opposite direction</em>, then you can <em>get more power</em> by using one-sided tests or one-sided
confidence intervals.</p>
<p>Thus, to obtain the critical points for lower-tailed inferences, you need a <span class="math inline">\(c_{\alpha}\)</span> for which
<span class="math display">\[
P\left(\mu_{i}-\mu_{0} \leq \bar{y}_{i}-\bar{y}_{0}+c_{\alpha} \hat{\sigma} \sqrt{2 / n}, \text { for all } i\right)=1-\alpha
\]</span>
For upper-tailed inferences, you need a <span class="math inline">\(c_{\alpha}\)</span> for which
<span class="math display">\[
P\left(\mu_{i}-\mu_{0} \geq \bar{y}_{i}-\bar{y}_{0}-c_{\alpha} \hat{\sigma} \sqrt{2 / n}, \text { for all } i\right)=1-\alpha
\]</span>
Rearranging terms algebraically and, in the case of lower-tail inference <span class="math inline">\(c_{\alpha}\)</span> must satisfy
<span class="math display">\[
P\left(\max _{i} \frac{\left(\bar{y}_{0}-\mu_{0}\right)-\left(\bar{y}_{i}-\mu_{i}\right)}{\hat{\sigma} \sqrt{2 / n}} \leq c_{\alpha}\right)=1-\alpha
\]</span>
and in the case of upper-tail inference <span class="math inline">\(c_{\alpha}\)</span> must satisfy
<span class="math display">\[
P\left(\max _{i} \frac{\left(\bar{y}_{i}-\mu_{i}\right)-\left(\bar{y}_{0}-\mu_{0}\right)}{\hat{\sigma} \sqrt{2 / n}} \leq c_{\alpha}\right)=1-\alpha
\]</span></p>
<p><strong>SAS Implementation</strong></p>
<pre><code>ods graphics on;
proc glm data=Tox;
 class Trt;
 model Gain=Trt;
 means Trt/dunnettl;
run; quit;
ods graphics off; </code></pre>
<p><strong>Graphing the One-Sided Dunnett Comparisons</strong></p>
<pre><code>lsmeans Trt/pdiff=controll;</code></pre>
<div class="figure" style="text-align: center">
<img src="02_Plots/Dunnett_OneSide.PNG" alt="Figure: Displaying One-Sided Dunnett Comparisons Graphically" width="100%" />
<p class="caption">
(#fig:One-Sided Dunnett Comparisons)Figure: Displaying One-Sided Dunnett Comparisons Graphically
</p>
</div>
</div>
<div id="maximum-modulus-distribution-multiple-inferences-for-independent-estimates" class="section level3" number="15.3.9">
<h3><span class="header-section-number">15.3.9</span> Maximum Modulus Distribution, Multiple Inferences for Independent Estimates</h3>
<p>Whereas the distributions used in
Tukey’s and Dunnett’s tests concern estimates that are dependent, the studentized maximum
modulus distribution concerns estimates that are independent. The most common applications of
this distribution are to confidence intervals for means and to orthogonal comparisons</p>
<p><strong>Simultaneous Intervals for the Treatment Means</strong></p>
<p>Suppose that you want simultaneous confidence intervals for the group means themselves, rather than for their differences. <span class="math inline">\(c_{\alpha}\)</span> for which
<span class="math display">\[
P\left(\bar{y}_{i}-c_{\alpha} \hat{\sigma} / \sqrt{n} \leq \mu_{i} \leq \bar{y}_{i}+c_{\alpha} \hat{\sigma} / \sqrt{n}, \text { for all } i\right)=1-\alpha
\]</span>
Rearranging terms algebraically,
<span class="math display">\[
P\left(\max _{i} \frac{\left|\left(\bar{y}_{i}-\mu_{i}\right)\right|}{\hat{\sigma} / \sqrt{n}} \leq c_{\alpha}\right)=1-\alpha
\]</span>
Since the <span class="math inline">\(\bar{y}_{i}\)</span> are independent, the <span class="math inline">\(t_{i}=\left(\bar{y}_{i}-\mu_{i}\right) /(\hat{\sigma} / \sqrt{n})\)</span> values are nearly independent, too.
They’re not quite independent because they share a common pooled variance estimate <span class="math inline">\(\hat{\sigma}^{2}\)</span>. Thus, you could use Šidák’s method, to approximate <span class="math inline">\(c_{\alpha}\)</span> as the <span class="math inline">\(1-(1-\alpha)^{1 / g}\)</span> quantile of the <span class="math inline">\(t\)</span> -distribution. However, an exact value for <span class="math inline">\(c_{\alpha}\)</span> can be calculated using Tukey’s (1953) maximum modulus distribution.</p>
<p><strong>The Maximum Modulus Distribution</strong></p>
<p>If <span class="math inline">\(Z_{0}, Z_{1}, \ldots, Z_{g}\)</span> are independent standard normal random variables, and <span class="math inline">\(V\)</span> is a random variable distributed as chi-square with <span class="math inline">\(v\)</span> degrees of freedom, independent of the <span class="math inline">\(Z \mathrm{~s}\)</span>, then
<span class="math display">\[
Q_{g, v}^{M M}=\frac{\max _{i}\left|Z_{i}\right|}{\sqrt{V / v}}
\]</span>
has the maximum modulus distribution with parameters <span class="math inline">\(g\)</span> and <span class="math inline">\(v\)</span>.
You can see that the <span class="math inline">\(c_{\alpha}\)</span> for the simultaneous confidence intervals satisfies
<span class="math display">\[
P\left(Q_{g, g(n-1)}^{M M} \leq c_{\alpha}\right)=1-\alpha
\]</span>
or in other words, <span class="math inline">\(c_{\alpha}=q_{1-\alpha, g, g(n-1)}^{M M}\)</span>, where <span class="math inline">\(q_{1-\alpha, r}^{M M}\)</span> is the <span class="math inline">\(1-\alpha\)</span> quantile of the maximum modulus distribution.</p>
<p><strong>SAS Implementation</strong></p>
<pre><code>*** Simultaneous Confidence Intervals for Means;
proc glm data=Wloss;
 class Diet;
 model Wloss=Diet;
 means Diet / clm smm sidak;
run; </code></pre>
<p>Since the intervals are almost independent, Šidák’s adjustment provides a very close
approximation to the maximum modulus method. Using the maximum modulus
distribution yields very slightly (about 0.2%) tighter intervals.</p>
</div>
</div>
<div id="multiple-comparisons-among-treatment-means-in-the-one-way-unbalanced-anova" class="section level2" number="15.4">
<h2><span class="header-section-number">15.4</span> Multiple Comparisons among Treatment Means in the One-Way Unbalanced ANOVA</h2>
<p>For unbalanced sample sizes, s, the simple distributions such as Tukey’s studentized range and Dunnett’s range distributions no longer are valid. The standard errors differ from contrast to contrast, and the simple range distributions can no longer be used directly.</p>
<div id="the-model-and-estimates" class="section level3" number="15.4.1">
<h3><span class="header-section-number">15.4.1</span> The Model and Estimates</h3>
<p><span class="math display">\[
y_{i j}=\mu_{i}+\varepsilon_{i j}
\]</span>
with independent, homoscedastic, and normally distributed <span class="math inline">\(\varepsilon_{i j}\)</span> having mean zero.</p>
<p>In the balanced case, the within-group samples all have the same size <span class="math inline">\(n\)</span>. In the unbalanced case, we allow them to differ, denoting the size of the <span class="math inline">\(i^{\text {th }}\)</span> sample by <span class="math inline">\(n_{i}\)</span>, for <span class="math inline">\(i=1, \ldots, g\)</span>. The estimated parameters are <span class="math inline">\(\hat{\mu}_{i}=\bar{y}_{i}=\left(1 / n_{i}\right) \sum_{j=1}^{n_{i}} y_{i j}\)</span>, and <span class="math inline">\(\hat{\sigma}^{2}=\Sigma_{i=1}^{g}\left\{\left(n_{i}-1\right) s_{i}^{2}\right\} / \Sigma_{i=1}^{g}\left(n_{i}-1\right)\)</span>, where <span class="math inline">\(s_{i}^{2}\)</span> is the
ordinary sample variance estimate for group <span class="math inline">\(i, s_{i}^{2}=\sum_{j=1}^{n_{i}}\left(y_{i j}-\bar{y}_{i}\right)^{2} /\left(n_{i}-1\right)\)</span>. The degrees of freedom for the estimate of <span class="math inline">\(\sigma^{2}\)</span> is the error degrees of freedom <span class="math inline">\(d f e=\sum_{i=1}^{g}\left(n_{i}-1\right)=N-g\)</span>, where
<span class="math inline">\(N=\Sigma_{i=1}^{g} n_{i}\)</span> is the total of all within-group sample sizes.</p>
<p><strong>All Pairwise Comparisons</strong></p>
<p>The confidence intervals for the difference of means <span class="math inline">\(\mu_{i}-\mu_{i^{\prime}}\)</span> have the form
<span class="math display">\[
\bar{y}_{i}-\bar{y}_{i^{\prime}} \pm c_{\alpha} \hat{\sigma} \sqrt{1 / n_{i}+1 / n_{i^{\prime}}}
\]</span>
where <span class="math inline">\(\bar{y}_{i}-\bar{y}_{i^{\prime}}\)</span> is the usual least-squares estimate of <span class="math inline">\(\mu_{i}-\mu_{i^{\prime}}\)</span> and <span class="math inline">\(\hat{\sigma} \sqrt{1 / n_{i}+1 / n_{i}}\)</span> is its standard error. In the case of non-multiplicity-adjusted confidence intervals, you set <span class="math inline">\(c_{\alpha}\)</span> to be the <span class="math inline">\(1-\alpha / 2\)</span> quantile of the <span class="math inline">\(t_{d f e}\)</span> distribution, <span class="math inline">\(t_{1-\alpha / 2, d f e}\)</span>, with <span class="math inline">\(d f e=N-g\)</span>. As always, you can construct Bonferroni-adjusted confidence intervals by setting <span class="math inline">\(c_{\alpha}=t_{1-\alpha^{\prime} / 2, d f e}\)</span>, where <span class="math inline">\(\alpha^{\prime}=\alpha / k, k\)</span> being the number of inferences (e.g., pairwise comparisons) in the family. And, as always, you can improve upon the Bonferroni value by taking into account the distribution of the differences.</p>
<p>Mathematically, <span class="math inline">\(c_{\alpha}\)</span> must satisfy
<span class="math inline">\(P\left(\bar{y}_{i}-\bar{y}_{i}-c_{\alpha} \hat{\sigma} \sqrt{1 / n_{i}+1 / n_{i}} \leq \mu_{i}-\mu_{i^{\prime}} \leq \bar{y}_{i}-\bar{y}_{i^{\prime}}+c_{\alpha} \hat{\sigma} \sqrt{1 / n_{i}+1 / n_{i}}\right.\)</span>, for all <span class="math inline">\(\left.i, i^{\prime}\right)=1-\alpha\)</span>
or equivalently,
<span class="math display">\[
P\left(\max _{i, i^{\prime}} \frac{\left|\left(\bar{y}_{i}-\mu_{i}\right)-\left(\bar{y}_{i^{\prime}}-\mu_{i^{\prime}}\right)\right|}{\hat{\sigma} \sqrt{1 / n_{i}+1 / n_{i}}} \leq c_{\alpha}\right)=1-\alpha
\]</span></p>
<p>Unlike the balanced case, the denominator of this expression, <span class="math inline">\(\hat{\sigma} \sqrt{1 / n_{i}+1 / n_{i^{\prime}}}\)</span>, is <strong>not constant</strong> for all <span class="math inline">\(\left(i, i^{\prime}\right)\)</span> pairs. Thus, the MaxT statistic does <strong>not have the studentized range distribution</strong>; its distribution is actually quite complicated.</p>
</div>
<div id="tukey-kramer-method" class="section level3" number="15.4.2">
<h3><span class="header-section-number">15.4.2</span> Tukey-Kramer Method</h3>
<p>Tukey (1953) and Kramer (1956) independently proposed a method to approximate the critical value <span class="math inline">\(c_{\alpha}\)</span> in unbalanced designs. Recall that when the sample sizes are all equal (i.e., when <span class="math inline">\(\left.n_{1}=\ldots=n_{g}=n\right)\)</span>, the statistic
<span class="math display">\[
\max _{i, i^{\prime}} \sqrt{2}\left|T_{i, i^{\prime}}\right|=\max _{i, i} \sqrt{2}\left|\frac{\left(\bar{y}_{i}-\mu_{i}\right)-\left(\bar{y}_{i^{\prime}}-\mu_{i^{\prime}}\right)}{\hat{\sigma} \sqrt{1 / n+1 / n}}\right|
\]</span>
is distributed as <span class="math inline">\(Q_{g, g(n-1)}^{R}\)</span>, which has the studentized range distribution</p>
<p>which gives the Tukey-Kramer simultaneous confidence intervals:
<span class="math display">\[
\bar{y}_{i}-\bar{y}_{i^{\prime}} \pm\left(q_{1-\alpha, g, d f e}^{R} / \sqrt{2}\right) \hat{\sigma} \sqrt{1 / n_{i}+1 / n_{i}}
\]</span>
Note that, as in the balanced case, the critical value <span class="math inline">\(c_{\alpha}\)</span> is the <span class="math inline">\(1-\alpha\)</span> quantile of the range distribution divided by <span class="math inline">\(\sqrt{2}\)</span>. Thus, there are no real differences in the form of the Tukey-Kramer intervals and the Tukey intervals.</p>
<p>If you apply the above confidence interval formula in the case where all <span class="math inline">\(n_{i}\)</span> are equal (to <span class="math inline">\(n\)</span> ), you get exactly the Tukey intervals for all pairwise comparisons.
However, the Tukey-Kramer intervals are not exact in the sense of providing an exact simultaneous <span class="math inline">\(1-\alpha\)</span> coverage rate and exact <span class="math inline">\(\mathrm{FWE}=\alpha\)</span> when the sample sizes are unequal. Hayter
(1984) proved that the method is in fact <strong>conservative</strong>: the true FWE is <strong>less than or equal</strong> to <span class="math inline">\(\alpha\)</span> for all possible sample size configurations.</p>
<p><strong>SAS Implementation</strong></p>
<p>When you specify TUKEY as an option for the MEANS statement, the Tukey-Kramer method
is used automatically when the sample sizes are unequal. This code produces the following
output.</p>
<pre><code>data Recover;
 input Blanket$ Minutes @@;
 datalines;
b0 15 b0 13 b0 12 b0 16 b0 16 b0 17 b0 13 b0 13 b0 16 b0 17
b0 17 b0 19 b0 17 b0 15 b0 13 b0 12 b0 16 b0 10 b0 17 b0 12
b1 13 b1 16 b1 9
b2 5 b2 8 b2 9
b3 14 b3 16 b3 16 b3 12 b3 7 b3 12 b3 13 b3 13 b3 9 b3 16
b3 13 b3 18 b3 13 b3 12 b3 13
;
proc sgplot data=Recover;
 vbox Minutes / category=Blanket;
run;
proc glm data=Recover;
 class Blanket;
 model Minutes=Blanket;
 means Blanket/tukey;
run; </code></pre>
</div>
<div id="alternative-simulation-based-method" class="section level3" number="15.4.3">
<h3><span class="header-section-number">15.4.3</span> Alternative Simulation-Based Method</h3>
<p>The Tukey-Kramer method is conservative because the critical value <span class="math inline">\(q_{1-\alpha, g, N-g}^{R}\)</span> is larger than the true <span class="math inline">\(c_{\alpha}\)</span>, which is the <span class="math inline">\(1-\alpha\)</span> quantile of the distribution of <span class="math inline">\(\max _{i, i}\left|T_{i, i}\right| .\)</span> To calculate the correct critical value analytically requires multidimensional integration using the <strong>multivariate <span class="math inline">\(t\)</span> distribution</strong>, you can approximate this critical value very easily <strong>by simulating from the multivariate <span class="math inline">\(t\)</span> distribution</strong> with <span class="math inline">\(d f e=N-g\)</span> and dispersion matrix <span class="math inline">\(\mathbf{R}=\mathbf{D}^{-1 / 2} \mathbf{C}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{C} \mathbf{D}^{-1 / 2}\)</span>. The following simulation algorithm avoids the problem of having to specify the <span class="math inline">\(\mathbf{R}\)</span> matrix, and illustrates the concept of <strong>parametric resampling</strong>.</p>
<ol style="list-style-type: decimal">
<li>Generate a random sample <span class="math inline">\(y_{i j}^{*}\)</span> from the standard normal distribution.</li>
<li>Analyze the data exactly as you would if it were an actual data set, getting sample means <span class="math inline">\(\bar{y}_{i}^{*}\)</span> and a pooled variance estimate <span class="math inline">\(\left(\hat{\sigma}^{*}\right)^{2}\)</span>. Compute the test statistics for all pairwise comparisons, <span class="math inline">\(T_{i, i^{\prime}}^{*}=\left(\bar{y}_{i}^{*}-\bar{y}_{i^{\prime}}^{*}\right) /\left(\hat{\sigma}^{*} \sqrt{1 / n_{i}+1 / n_{i^{\prime}}}\right)\)</span>.</li>
<li>Calculate the value <span class="math inline">\(\operatorname{Max} \mathrm{T}=\max _{i, i^{\prime}}\left|T_{i, i^{\prime}}^{*}\right|\)</span> and store it.</li>
<li>Repeat steps 1-3 NSAMP times, and estimate <span class="math inline">\(c_{\alpha}\)</span> as the <span class="math inline">\(1-\alpha\)</span> quantile of the resulting
MaxT values. Call the resulting value <span class="math inline">\(\hat{c}_{\alpha}\)</span>.</li>
</ol>
<p><strong>Simulating the Critical Value for Recovery Data Using Parametric Resampling</strong></p>
<pre><code>data sim;
 array nsize{4} (20,3,3,15);
 do rep = 1 to 20000;
 do i=1 to dim(nsize);
 do j=1 to nsize{i};
 y = rannor(121211);
 output;
 end;
 end;
 end;
run;
ods listing close;
proc glm data=sim;
 by rep;
 class i;
 model y=i;
 lsmeans i/ tdiff;
 ods output Diff=GDiffs;
quit;
ods listing;
proc transpose data=GDiffs out=t(where=(_label_ &gt; RowName));
 by rep RowName;
 var _1 _2 _3 _4;
data t;
 set t;
 abst = abs(COL1);
 keep rep abst;
proc means noprint data=t;
 var abst;
 by rep;
 output out=maxt max=maxt;
run;
proc univariate;
 var maxt;
 ods select Quantiles;
run; </code></pre>
<p>Thus, the correct 95th percentile is estimated to be 2.646847, based on NSAMP=20000
simulations. The Tukey-Kramer approximation resulted in a slightly higher number, 2.68976,
which suggests a slight level of conservatism of the Tukey-Kramer method.</p>
<pre><code>100% Max 4.844511
99% 3.288458
95% 2.646847
90% 2.332767
75% Q3 1.851581
50% Median 1.381995
25% Q1 0.981263
10% 0.681271
5% 0.528405
1% 0.293808
0% Min 0.036276</code></pre>
<p>However, remember that the percentile estimated by simulation is subject to <strong>sampling error</strong>, so the precise degree of conservatism is unclear. Edwards and Berry (1987) suggest generating sufficient samples NSAMP so that <span class="math inline">\(P\left(\operatorname{Max} T \geq \hat{c}_{\alpha}\right)\)</span> (where <span class="math inline">\(\hat{c}_{\alpha}\)</span> is fixed and MaxT is random) is within an accuracy radius <span class="math inline">\(\gamma\)</span> of <span class="math inline">\(\alpha\)</span> with confidence <span class="math inline">\(100(1-\delta) \%\)</span>. You can adjust <span class="math inline">\(\alpha\)</span> using the ALPHA= option, and <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\delta\)</span> with the ACC <span class="math inline">\(=\)</span> and EPS <span class="math inline">\(=\)</span> suboptions of the ADJUST=SIMULATE option, respectively. By default, <span class="math inline">\(\alpha=0.05, \gamma=0.005\)</span>, and <span class="math inline">\(\delta=0.01 ;\)</span> the method yields <span class="math inline">\(\mathrm{NSAMP}=12,604\)</span> in this case. That is, using quantiles from a simulation of this size, a nominal <span class="math inline">\(95 \%\)</span> confidence interval for a mean difference will actually have between <span class="math inline">\(94.5 \%\)</span> and <span class="math inline">\(95.5 \%\)</span> confidence with probability <span class="math inline">\(0.99\)</span></p>
<p>If you specify the ADJUST=SIMULATE option then PROC GLM uses the simulationestimated quantile in forming multiplicity-adjusted confidence intervals for the differences. Although PROC GLM doesn’t display the actual value of the quantile by default, you can use the <strong>REPORT option</strong> for the simulation to print the quantile and other information,</p>
<pre><code>proc glm data=Recover;
 class Blanket;
 model Minutes=Blanket;
 lsmeans Blanket/cl adjust=simulate(seed=121211 report);
 ods select SimResults LSMeanDiffCL;
run;</code></pre>
<p><span class="math display">\[
\begin{array}{lcccc}
\hline &amp; \text { Simulation Results } &amp; &amp; \\
\text { Method } &amp; \text { 95% Quantile } &amp; \text { Estimated } &amp; \text {99% CI } \\
\text { Simulated } &amp; 2.634412 &amp; 0.0500 &amp;  0.0450 &amp; 0.0550\\
\text { Tukey-Kramer } &amp; 2.689757 &amp; 0.0432 &amp; 0.0385 &amp; 0.0478 \\
\text { Bonferroni } &amp; 2.787602 &amp; 0.0338 &amp; 0.0297 &amp; 0.0379 \\
\text { Sidak } &amp; 2.779230 &amp; 0.0346 &amp; 0.0304 &amp; 0.0388 \\
\text { GT-2 } &amp; 2.770830 &amp; 0.0350 &amp; 0.0308 &amp; 0.0392 \\
\text { Scheffe } &amp; 2.928547 &amp; 0.0237 &amp; 0.0202 &amp; 0.0272 \\
T &amp; 2.026192 &amp; 0.1870 &amp; 0.1780 &amp; 0.1959 \\
\hline
\end{array}
\]</span></p>
</div>
<div id="pairwise-comparisons-with-control" class="section level3" number="15.4.4">
<h3><span class="header-section-number">15.4.4</span> Pairwise Comparisons with Control</h3>
<p>Unlike the case of all pairwise comparisons, the critical value cα and the adjusted p-values can
be calculated analytically for Dunnett’s method in the case of all pairwise comparisons with a
control, even though the design is unbalanced. There is no need to use approximations, such as
the Tukey-Kramer or simulation-based.</p>
<p>Suppose the means are <span class="math inline">\(\bar{y}_{0}, \bar{y}_{1}, \ldots, \bar{y}_{g}\)</span>, where <span class="math inline">\(\bar{y}_{0}\)</span> denotes the sample mean for the control group.</p>
<p>To get the critical values and adjusted <span class="math inline">\(p\)</span> -values for two-sided intervals and tests, you need the distribution of
<span class="math display">\[
M_{2}=\max _{i} \frac{\left|\bar{y}_{i}-\bar{y}_{0}\right|}{\hat{\sigma} \sqrt{1 / n_{i}+1 / n_{0}}}
\]</span>
The critical value <span class="math inline">\(c_{\alpha}\)</span> for the two-sided confidence intervals for <span class="math inline">\(\mu_{i}-\mu_{0}\)</span> is the <span class="math inline">\(1-\alpha\)</span> quantile of the distribution of <span class="math inline">\(M_{2}\)</span>, and adjusted <span class="math inline">\(p\)</span> -values for two-sided tests are given as
<span class="math inline">\(\tilde{p}_{i}=P\left(M_{2} \geq\left|t_{i}\right|\right)\)</span>, where <span class="math inline">\(t_{i}\)</span> is the test statistic for <span class="math inline">\(H_{0 i}: \mu_{i}-\mu_{0}=0\)</span>, i.e.,
<span class="math display">\[
t_{i}=\left(\bar{y}_{i}-\bar{y}_{0}\right) /\left(\hat{\sigma} \sqrt{1 / n_{i}+1 / n_{0}}\right)
\]</span>
To get the critical values and adjusted <span class="math inline">\(p\)</span> -values for one-sided intervals and tests, you need the distribution of
<span class="math display">\[
M_{1}=\max _{i} \frac{\bar{y}_{i}-\bar{y}_{0}}{\hat{\sigma} \sqrt{1 / n_{i}+1 / n_{0}}}
\]</span>
The critical value <span class="math inline">\(c_{\alpha}\)</span> for the one-sided confidence bounds is the <span class="math inline">\(1-\alpha\)</span> quantile of the
distribution of <span class="math inline">\(M_{1}\)</span>. Adjusted <span class="math inline">\(p\)</span> -values for one-sided, upper-tail tests are given as <span class="math inline">\(\tilde{p}_{i}=P\left(M_{1} \geq t_{i}\right)\)</span>, and adjusted <span class="math inline">\(p\)</span> -values for one-sided, lower-tail tests are given as <span class="math inline">\(\tilde{p}_{i}=P\left(M_{1} \geq-t_{i}\right)\)</span></p>
<p><strong>Dunnett’s Two-Sided Comparisons with Unbalanced Data</strong></p>
<pre><code>ods graphics on;
proc glm data=Recover;
 class Blanket;
 model Minutes = Blanket;
 lsmeans Blanket/pdiff cl adjust=dunnett;
run;
ods graphics off;</code></pre>
<p><strong>Dunnett’s One-Sided Comparisons with Unbalanced Data</strong></p>
<pre><code>** “By Hand” Calculation of Dunnett&#39;s Exact One-Sided Critical Value 
    and Adjusted p-Value for Unbalanced ANOVA.
data;
 n0=20; n1=3; n2=3; n3=15;
 lambda1 = sqrt(n1/(n0+n1));
 lambda2 = sqrt(n2/(n0+n2));
 lambda3 = sqrt(n3/(n0+n3));
 c_alpha = probmc(&#39;DUNNETT1&#39;,.,.90,37,3,lambda1,lambda2,lambda3);
 t3 = -1.66666667/0.88477275;
 adjp_3 = 1-probmc(&#39;DUNNETT1&#39;,-t3,.,37,3,lambda1,lambda2,lambda3);
run; 


ods graphics on;
proc glm data=Recover;
 class Blanket;
 model Minutes = Blanket;
 lsmeans Blanket / pdiff=controll cl alpha=0.10;
run;
ods graphics off;</code></pre>
</div>
<div id="comparisons-with-the-average-meananalysis-of-means-anom" class="section level3" number="15.4.5">
<h3><span class="header-section-number">15.4.5</span> Comparisons with the Average Mean–Analysis of Means (ANOM)</h3>
<p>whether one group’s mean is confidently different from the average of the means for the set of groups as a
whole. Using this method, a quality control engineer can confidently identify troubled or
unusually good spots (e.g., a shift that is under- or over-performing) or a product formulation or
business process that should be abandoned or emulated.</p>
<pre><code>ods graphics on;
proc glm data=Recover;
 class Blanket;
 model Minutes = Blanket;
 lsmeans Blanket / pdiff=anom(weighted) cl alpha=0.10;
run;
ods graphics off; </code></pre>
</div>
</div>
<div id="generalizations-for-the-analysis-of-covariance-ancova-model" class="section level2" number="15.5">
<h2><span class="header-section-number">15.5</span> Generalizations for the Analysis of Covariance (ANCOVA) model</h2>
<p>A major difference is that the comparisons of interest in ANCOVA are differences of <strong>LS-means rather than ordinary means</strong>. This leads to some interesting graphical comparisons using regression functions. As in the unbalanced one-way case, the <strong>standard errors of estimated LS-mean differences are not constant</strong>, implying that simple range distributions (Tukey-type or Dunnett-type) cannot be used. Also, while for Dunnett comparisons there is an exact representation of the MaxT distribution in the case of unbalanced sample sizes without covariates, this is not the case when there are covariates. Hence, either simulation-based or analytic approximations must be used.</p>
<blockquote>
<p>一个主要的区别是在ANCOVA中感兴趣的比较是LS-均值的差异，而不是普通方法。这导致使用回归函数进行一些有趣的图形比较。与不平衡单向情况一样，估计的LS均值差的标准误差也不是恒定的，这意味着不能使用简单的范围分布（Tukey型或Dunnett型）。同样，对于Dunnett比较而言，在样本量不平衡且没有协变量的情况下，可以精确地表示MaxT分布，而在有协变量的情况下，情况并非如此。因此，必须使用基于仿真的近似值或解析近似值。</p>
</blockquote>
<p>ANCOVA model with interaction may be written as
<span class="math display">\[
y_{i j}=\gamma+\mu_{i}+\beta x_{i j}+\varepsilon_{i j}
\]</span>
where parallelism is indicated by the common slope <span class="math inline">\(\beta\)</span> for all groups.</p>
<p>A recommendation is generally to use the SAS defaults (<strong>Tukey-Kramer, Dunnett-Hsu</strong>) for ANCOVA applications: although they are not exact, their accuracy is usually completely adequate from a practical perspective, and they avoid the troublesome issue of simulation nondeterminacy. Nevertheless, it is prudent to validate the default analysis using the simulation adjustments.</p>
<div id="dunnett-hsu-factor-analytic-approximation" class="section level3" number="15.5.1">
<h3><span class="header-section-number">15.5.1</span> Dunnett-Hsu Factor Analytic Approximation</h3>
<blockquote>
<p>Pairwise Comparisons in ANCOVA</p>
</blockquote>
<ul>
<li>Tukey’s range, Dunnett’s range, and the maximum modulus distributions to account for dependencies among the estimates.</li>
<li>The Range distribution becomes inexact in the case of unbalanced data, while the Dunnett one- and twosided distributions remain exact (with suitable modifications).</li>
<li>When you include covariates, none of these distributions is exact in general. The general alternative of simulation is still available, though, and quantiles can be simulated with relative ease and adequate accuracy using the ADJUST=SIMULATE option.</li>
</ul>
<p>There is an analytical <strong>approximation</strong> that works very well, providing critical values that, while not analytically exact, are exceptionally accurate. In fact, the deterministic error in this analytical approximation is usually much smaller than the Monte Carlo error of the simulation-based methods at reasonable sample sizes.</p>
<!-- 除非在MaxT统计量的构成差异之间的相关矩阵之间具有相关性具有一定的对称性，否则评估MaxT分布的临界值和调整的p值是很难的。 -->
<p>As has been discussed, evaluating the critical values and adjusted <span class="math inline">\(p\)</span> -values for the MaxT distribution is intractable unless the correlation matrix <span class="math inline">\(\mathbf{R}=\mathbf{D}^{-1 / 2} \mathbf{C}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{C} \mathbf{D}^{-1 / 2}\)</span> between the constituent differences in the MaxT statistic has a certain symmetry, in which case the problem reduces to a feasible 2 -fold integral.</p>
<p>The required symmetry is provided by complete balance in the case of Tukey’s test, and by a factor analytic structure (cf. Hsu, 1992 ) in the case of Dunnett’s test. To be precise, the <span class="math inline">\(\mathbf{R}\)</span> matrix has the required symmetry for exact computation of Tukey’s test if the test statistics <span class="math inline">\(t_{i}\)</span> are studentized differences between</p>
<ul>
<li><span class="math inline">\(k(k-1) / 2\)</span> pairs of <span class="math inline">\(k\)</span> uncorrelated means with equal variances, that is, equal sample sizes</li>
<li><span class="math inline">\(k(k-1) / 2\)</span> pairs of <span class="math inline">\(k\)</span> LS-means from a variance-balanced design (for example, a balanced incomplete block design)</li>
</ul>
<p>In the case of comparisons with a control, the <span class="math inline">\(\mathbf{R}\)</span> matrix has the factor analytic structure for exact computation of Dunnett’s test if the <span class="math inline">\(t_{i}\)</span> ’s are studentized differences between</p>
<!-- 在与控件进行比较的情况下，如果$t_ {i}$是学生之间的差异，则$\mathbf {R}$矩阵具有用于精确计算Dunnett检验的因子分析结构。 -->
<ul>
<li><span class="math inline">\(k-1\)</span> means and a control mean, all uncorrelated. Note that it is not required that the variances of the estimated means (that is, the sample sizes) be equal.</li>
<li><span class="math inline">\(k-1\)</span> LS-means and a control LS-mean from either a variance-balanced design, or a design in which the other factors are orthogonal to the treatment factor (for example, a randomized block design with proportional cell frequencies)</li>
</ul>
<p>However, other important situations that do not result in a correlation matrix <span class="math inline">\(\mathbf{R}\)</span> that has the symmetry required for exact computation include</p>
<!-- 但是，其他不会导致具有精确计算所需对称性的相关矩阵$ \ mathbf {R} $的其他重要情况包括：
所有成对的差异，样本大小均不相等 
当存在协变量时，LS均值和对照LS均值之间的差异。-->
<ul>
<li>all pairwise differences with unequal sample sizes</li>
<li>differences between LS-means and a control LS-mean when there are covariates.</li>
</ul>
<p>In these situations, exact calculation of critical values and adjusted <span class="math inline">\(p\)</span> -values is intractable in general. For comparisons with a control when the correlation <span class="math inline">\(\mathbf{R}\)</span> does not have a factor analytic structure, Hsu (1992) suggests approximating <span class="math inline">\(\mathbf{R}\)</span> with a matrix <span class="math inline">\(\mathbf{R}^{\mathrm{F}}\)</span> that does have such a structure and, correspondingly, approximating the MaxT critical values and <span class="math inline">\(p\)</span> -values by assuming that the true correlation matrix is <span class="math inline">\(\mathbf{R}^{\mathrm{F}}\)</span>. The resulting critical values and adjusted <span class="math inline">\(p\)</span> values are calculated exactly for the correlation <span class="math inline">\(\mathbf{R}^{\mathrm{F}}\)</span>, but are approximate for the true correlation
R. (Approximating <span class="math inline">\(\mathbf{R}\)</span> in this way can also be viewed as computing “effective sample sizes” <span class="math inline">\(\tilde{n}_{i}\)</span> for the means and treating them as uncorrelated.)</p>
<!-- 在这些情况下，通常很难计算出临界值和调整后的$ p $值。为了在相关$ \ mathbf {R} $不具有因子分析结构的情况下与控件进行比较，Hsu（1992）建议使用矩阵$ \ mathbf {R} ^ {\ mathrm { F}} $具有这样的结构，并通过假设真实的相关矩阵为$ \ mathbf {R} ^ {\ mathrm {F}} $来近似地近似MaxT临界值和$ p $ -values。精确计算出相关系数$ \ mathbf {R} ^ {\ mathrm {F}} $的结果临界值和调整后的$ p $值，但对于真实相关性则近似R。 -->
<p>When you request Dunnett’s test for LS-means (the PDIFF=CONTROL and
ADJUST=DUNNETT options), the GLM procedure automatically uses Hsu’s approximation
when appropriate</p>
<pre><code>proc glm data=House;
 class Location;
 model Price = Location Sqfeet;
 lsmeans Location / tdiff=control(&#39;B&#39;) pdiff cl;
run;
quit;</code></pre>
</div>
<div id="hsu-nelson-simulation-based-approximation-cvadjust-method" class="section level3" number="15.5.2">
<h3><span class="header-section-number">15.5.2</span> Hsu-Nelson Simulation-Based Approximation: CVADJUST Method</h3>
<blockquote>
<p>Pairwise Comparisons in ANCOVA</p>
</blockquote>
<ul>
<li>simple Monte Carlo method</li>
<li>simulate MaxT values for the correct correlation <span class="math inline">\(\mathbf{R}=\mathbf{D}^{-1 / 2} \mathbf{C}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-} \mathbf{C} \mathbf{D}^{-1 / 2}\)</span></li>
<li>simulate covariate of MaxT values with known distribution for the factor-analytic approximation correlation <span class="math inline">\(\mathbf{R}^{\mathrm{F}}\)</span></li>
</ul>
<p>A useful way to strengthen the simple Monte Carlo method of estimating confidence limits and p-values is to use the method of control variates. This method proceeds by simulating not only MaxT values for the correct correlation but also a covariate of MaxT values with known distribution for the factor-analytic approximation correlation discussed above. The conditional estimates for the correct distribution, conditional on the approximate but known one, can be much tighter than what you get with simple simulation (see Hsu and Nelson, 1998). To use this method, you can request it using the CVADJUST suboption of the ADJUST=SIMULATE option of the LSMEANS statement.</p>
<pre><code>lsmeans Location/tdiff=control(&#39;B&#39;) pdiff
 adjust=simulate(acc=0.0001 seed=121211 cvadjust report) cl;</code></pre>
</div>
<div id="comparisons-in-ancova-models-with-interaction" class="section level3" number="15.5.3">
<h3><span class="header-section-number">15.5.3</span> Comparisons in ANCOVA Models with Interaction</h3>
<p>The ANCOVA model with interaction may be written as
<span class="math display">\[
y_{i j}=\gamma+\mu_{i}+\beta x_{i j}+\beta_{i} x_{i j}+\varepsilon_{i j},
\]</span>
where <span class="math inline">\(i\)</span> denotes CLASS level and <span class="math inline">\(j\)</span> an observation within the CLASS level; where <span class="math inline">\(\gamma\)</span> is the overall intercept term and the <span class="math inline">\(\mu_{i}\)</span> parameterize deviations from the overall intercept for CLASS level <span class="math inline">\(i\)</span>, and where similarly <span class="math inline">\(\beta\)</span> is the overall slope term, and the <span class="math inline">\(\beta_{i}\)</span> correspond to deviations from the overall slope for CLASS level <span class="math inline">\(i\)</span>. The model is overparameterized, meaning that not all parameters can be estimated simultaneously without imposing additional constraints</p>
<pre><code>*** One-Way ANCOVA Analysis with Interaction;
ods graphics on;
proc glm data=House;
 class Location;
 model Price = Location Sqfeet location*sqfeet;
run;
quit;
ods graphics off;</code></pre>
<div class="figure" style="text-align: center">
<img src="02_Plots/ACNOVA_Effect.PNG" alt="Figure: Fitted ANCOVA Model with Interaction" width="100%" />
<p class="caption">
(#fig:ACNOVA Effect)Figure: Fitted ANCOVA Model with Interaction
</p>
</div>
<p>There is some suggestion of interaction in that the effect of SQFEET on PRICE seems less in LOCATION C, but this may be an artifact due to small sample size, because the interaction is not statistically significant (for testing <span class="math inline">\(H_{0}: \beta_{1}=\ldots=\beta_{5}=0, F(4,54)=1.54, p=0.2045\)</span> ). Nevertheless, the graph illustrates clearly that comparisons of housing prices between levels of the CLASS variable differ, depending on the chosen value of the covariate.</p>
<p>In PROC GLM, the default for <strong>LSMEANS is to compare the CLASS levels at the average value(s) of the covariate(s)</strong>. In the housing example, the average value of SQFEET is <span class="math inline">\(1947.28125\)</span>, thus the statements</p>
<p><code>lsmeans Location / adjust=tukey cl;</code> is equel to <code>lsmeans Location / adjust=tukey cl at Sqfeet=1947.28125;</code></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="anova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="non-parametric-test.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/16-Multiple-Comparison.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
