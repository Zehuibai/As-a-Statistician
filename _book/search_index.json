[["survival-analysis.html", "Chapter 18 Survival Analysis 18.1 Preliminary 18.2 Kaplan-Meier estimator 18.3 Compare the survival function 18.4 Cox Proportional Hazards Model", " Chapter 18 Survival Analysis 18.1 Preliminary Survival analysis mainly focuses on processing a special kind of time data, and the time data may be partially censored. The shortcomings of the ordinary least squares regression method are that the event occurrence time is usually not normally distributed, and the model cannot handle Censor. The non-parametric method can simply and quickly check the survival experience, while the Cox proportional hazard regression model is still the main Analytical method. 18.1.1 Probability density function The function that describes likelihood of observing Time at time t relative to all other survival times is known as the probability density function the probability of observing a survival time within the interval \\([a, b]\\) is \\[ \\operatorname{Pr}(a \\leq \\text { Time } \\leq b)=\\int_{a}^{b} f(t) d t=\\int_{a}^{b} \\lambda e^{-\\lambda t} d t \\] ### Cumulative distribution function Describes the probability of observing Time less than or equal to some timet \\(t\\) \\(\\operatorname{Pr}(\\) Time \\(\\leq t)\\) \\[ \\begin{array}{c} F(t)=\\int_{0}^{t} f(t) d t \\\\ f(t)=\\frac{d F(t)}{d t} \\end{array} \\] In SAS, we can graph an estimate of the cdf using proc univariate. proc univariate data = whas500(where=(fstat=1)); var lenfol; cdfplot lenfol; run; 18.1.2 Survival function \\[ S(t)=P(T&gt;t)=1-F(t) \\] The survival function gives the probability that a person survives longer than some specified time \\(t\\) : that is, \\(S(t)\\) gives the probability that the random variable \\(T\\) exceeds the specified time \\(t .\\) And here, some important characteristics: It is nonincreasing; that is, it heads downward as \\(t\\) increases. At time \\(t=0, S(t)=S(0)=1\\); that is, at the start of the study, since no one has gotten the event yet, the probability of surviving past time zero is one. At time \\(t=\\inf , S(t)=S(\\mathrm{inf})=0 ;\\) that is, theoretically, if the study period increased without limit. eventually nobody would survive, so the survival curve must eventually fall to zero. Here we can use proc lifetest to graph \\(S_(t)\\). proc lifetest data=whas500(where=(fstat=1)) plots=survival(atrisk); time lenfol*fstat(0); run; 18.1.3 Hazard function The hazard function \\(h(t)\\), is given by the formula: \\[ h(t)=\\lim _{\\Delta_{t} \\rightarrow 0} \\frac{P(t \\leq T&lt;t+\\Delta t \\mid T \\geq t)}{\\Delta t} \\] We could say that the hazard function is the probability that if you survive to time \\(t\\), you will experience the event in the next instant, or in other words, the hazard function gives the instantaneous potential per unit time for the event to occur, given that the individual has survived up to time \\(t\\). Because of the given sign here, the hazard function is sometimes called a conditional failure rate. We can estimate the hazard function is SAS as well using proc lifetest proc lifetest data=whas500(where=(fstat=1)) plots=hazard(bw=200); time lenfol*fstat(0); run; 18.1.4 Cumulative hazard function Calculated by integrating the hazard function over an interval of time: \\[H(t) = \\int_0^th(u)du\\] The cumulative hazard function H(t) and the survival function S(t) have a simple monotonic relationship. Therefore, when the survival function reaches the maximum at the beginning of the analysis time, the cumulative hazard function is the smallest. As time goes by, the survival function advances toward its minimum, and the cumulative hazard function advances toward its maximum. You can use proc lifetest to estimate the cumulative hazard function, and then send the results to proc sgplot for plotting. ods output ProductLimitEstimates = ple; proc lifetest data=whas500(where=(fstat=1)) nelson outs=outwhas500; time lenfol*fstat(0); run; proc sgplot data = ple; series x = lenfol y = CumHaz; run; 18.1.5 Mean Residual Life \\[r(t)=E(T-t \\mid T \\geq t)=\\frac{\\int_{t}^{\\infty} S(u) d u}{S(t)},\\] Intuitively, this is as simple as when I know that I have lived to the time point t and how many years I have to live. 18.1.6 Relation between functions The survival function can be ascertained from the probability density function by integrating over the probability density function from time \\(t\\) to infinity, or by calculating the difference between one and the cumulative distribution function \\(F(t)\\). The hazard can then be found by dividing the negative derivative of the survival function by the survival function. Note that the functions \\(f(t), F(t), h(t)\\), and \\(H(t)\\) are all related. Assume that \\(T\\) is non-negative and continuos: Probability density function: \\[ f(t)=F^{\\prime}(t)=\\frac{d F(t)}{d t} \\] Cumulative distribution function: \\[ F(t)=P(T \\leq t)=\\int_{0}^{t} f(u) d u \\] Survival function \\[ \\begin{array}{l} S(t)=1-F(t)\\\\ S(t)=P(T&gt;t)=\\int_{t}^{+\\infty} f(u) d u \\\\ S(t)=\\exp \\left(-\\int_{0}^{t} h(u) d u\\right) \\\\ S(t)=\\exp (-H(t)) \\end{array} \\] Hazard function \\[h(t) = \\frac{ f(t)}{S(t)}= \\frac{ -d[S(t)]/dt}{S(t)}\\] Cumulative hazard function o Cumulative hazard function \\[ H(t)=\\int_{0}^{t} h(u) d u \\] Assume that \\(T\\) is non-negative and discrete, Probability mass function: \\[ \\begin{aligned} p\\left(t_{i}\\right) &amp;=P\\left(T=t_{i}\\right) \\\\ p\\left(t_{i}\\right) &amp;=S\\left(t_{i-1}\\right)-S\\left(t_{i}\\right) \\\\ p\\left(t_{i}\\right) &amp;=F\\left(t_{i}\\right)-F\\left(t_{i-1}\\right) \\end{aligned} \\] Cumulative distribution function: \\[ F(t)=P(T \\leq t)=\\sum_{t_{i} \\leq t} p\\left(t_{i}\\right) \\] Survival function \\[ S(t)=\\prod_{t_{i} \\leq t}\\left(1-h\\left(t_{i}\\right)\\right) \\] Hazard function \\[ \\begin{aligned} h(t) &amp;=\\frac{p\\left(t_{i}\\right)}{S\\left(t_{i-1}\\right)}=\\frac{-d[S(t)] / d t}{S(t)} \\\\ h(t) &amp;=1-\\frac{S\\left(t_{i}\\right)}{S\\left(t_{i-1}\\right)} \\end{aligned} \\] Cumulative hazard function \\[ H(t)=\\sum_{t_{i} \\leq t} h\\left(t_{i}\\right) \\] 18.2 Kaplan-Meier estimator 18.2.1 KM Introduction \\[\\hat S(t)=\\prod_{t_i\\leq t}\\frac{n_i  d_i}{n_i},\\] \\(n_{i}\\) is the number of subjects at risk \\(d_{i}\\) is the number of subjects who fail, both at time \\(t_{i}\\), the number who failed out of \\(n_{i}\\) 18.2.2 Nelson-Aalen estimator of the cumulative hazard function Since it and the survival function \\(S(t)=e^{-H(t)}\\) Nelson-Aalen estimator is a non-parametric estimator of the cumulative hazard function \\[ \\hat{H}(t)=\\sum_{t_{i} l e q t} \\frac{d_{i}}{n_{i}} \\] The Nelson-Aalen estimator is requested in SAS through the nelson option on the proc lifetest statement. SAS will output both Kaplan Meier estimates of the survival function and Nelson-Aalen estimates of the cumulative hazard function in one table. Quartile Estimates: Calculating median, mean, and other survival times proc lifetest data=whas500 atrisk nelson; time lenfol*fstat(0); run; 18.2.3 Survival curve in SAS proc lifetest data=whas500 atrisk plots=survival(cb) outs=outwhas500; time lenfol*fstat(0); run; Graphing the Kaplan-Meier estimate By default, proc lifetest graphs the Kaplan Meier estimate, even without the plot= option on the proc lifetest statement, so we could have used the same code from above that produced the table of Kaplan-Meier estimates to generate the graph. However, we would like to add confidence bands and the number at risk to the graph, so we add plots=survival(atrisk cb) 95%Hall-Wellner   () 95% survivor conftype Life Table method KM TIMELISTPROC LIFEREGKM  life-table method  actuarial method PROC LIFETESTWilcoxon  ODS GRAPHICS ON; PROC LIFETEST DATA=recid METHOD=LIFE PLOTS=(S,H); TIME week*arrest(0); RUN; ODS GRAPHICS OFF; Interval Number Number Sample Probability Standard [Lower, Upper) Failed Censored Size of Failure Error 0 10 14 0 432.0 0.0324 0.00852 10 20 21 0 418.0 0.0502 0.0107 20 30 23 0 397.0 0.0579 0.0117 30 40 23 0 374.0 0.0615 0.0124 40 50 26 0 351.0 0.0741 0.0140 50 60 7 318 166.0 0.0422 0.0156 18.2.4 KM Survival Plots in SAS  ()  , NOCENSOR. ATRISK ()  95%, CL pointwise limits, 95% .  confidence bands that can be interpreted by saying that we are \\(95 \\%\\) confident that the entire survivor function falls within the upper curve and the lower curve. More complex methods are needed to produce such bands, and PROC LIFETEST offers two: the HallWellner method and the equal precision (EP) method. 95% ,  PROC LIFETESTHallWellner (EP)  EP , PLOTS \\(=\\mathrm{S}(\\mathrm{CB}=\\) EP)  EP, PLOTS \\(=\\mathrm{S}(\\mathrm{CL} \\mathrm{CB}=\\mathrm{EP})\\)   Other transformations are available, the most attractive being the logit \\(\\log [\\hat{S}(t) /(1-\\hat{S}(t))]\\) To switch to this transform, include the CONFTYPE=LOGIT option in the PROC statement. proc lifetest data=whas500 atrisk PLOTS=S(NOCENSOR ATRISK CL) OUTSURV=outwhas500; time lenfol*fstat(0); run; Test in plot ODS GRAPHICS ON; PROC LIFETEST DATA=myel PLOTS=S(TEST); TIME dur*status(0); STRATA treat; RUN; ODS GRAPHICS OFF; log-log survival plots LS produces a plot of \\(-\\log \\hat{S}(t)\\) versus \\(t\\). \\[ -\\log S(t)=\\int_{0}^{t} h(u) d u \\] The LLS keyword produces a plot of \\(\\log [-\\log \\hat{S}(t)]\\) versus \\(\\log t\\). If survival times follow a Weibull distribution, which has a hazard given by \\(\\log h(t)=\\alpha+\\beta \\log t\\), then the log-log survival plot (log cumulative hazard plot) should be a straight line with a slope of \\(\\beta\\). If the hazards are proportional, the log-log survivor functions should be strictly parallel. PROC LIFETEST DATA=COMBINE PLOTS=LLS; TIME years*event(0); STRATA type; RUN; Hazard plots Examine smoothed hazard plots using the kernel smoothing option ODS GRAPHICS ON; PROC LIFETEST DATA=combine PLOTS=H(BW=10); TIME years*event(0); STRATA type; RUN; ODS GRAPHICS OFF; ## confidence limits around the hazard function; PROC LIFETEST DATA=recid PLOTS=H(CL); 18.2.5 Convert Personal-level to Personal-period in R In studies of survival or modeling discrete-time events, one compact way to store data is in what may be called, person-level or generally observation-level. For example, you could have three variables, one indicating the observation, one indicating the time period the event occurred or the last follow-up period and one indicating whether the observation was censored. The PLPP function takes five arguments. The first, data is the data set to be converted. The second, id is the name of the variable containing the identifier for each observation. The third, period is the name of the variable that indicates how many periods the person or observation was in. The fourth, event is the name of the variable that indicates whether the event occurred or not or whether the observation was censored (depending on which direction you are converting). The fifth, direction indicates whether the function should go from person-level to person-period or from person-period to person-level. There are two options, period to go to person-period or level to go to person-level. Now lets try it out. For the examples that follow to work, you need to source the function into R. ## Person-Level Person-Period Converter Function PLPP &lt;- function(data, id, period, event, direction = c(&quot;period&quot;, &quot;level&quot;)){ ## Data Checking and Verification Steps stopifnot(is.matrix(data) || is.data.frame(data)) stopifnot(c(id, period, event) %in% c(colnames(data), 1:ncol(data))) if (any(is.na(data[, c(id, period, event)]))) { stop(&quot;PLPP cannot currently handle missing data in the id, period, or event variables&quot;) } ## Do the conversion switch(match.arg(direction), period = { index &lt;- rep(1:nrow(data), data[, period]) idmax &lt;- cumsum(data[, period]) reve &lt;- !data[, event] dat &lt;- data[index, ] dat[, period] &lt;- ave(dat[, period], dat[, id], FUN = seq_along) dat[, event] &lt;- 0 dat[idmax, event] &lt;- reve}, level = { tmp &lt;- cbind(data[, c(period, id)], i = 1:nrow(data)) index &lt;- as.vector(by(tmp, tmp[, id], FUN = function(x) x[which.max(x[, period]), &quot;i&quot;])) dat &lt;- data[index, ] dat[, event] &lt;- as.integer(!dat[, event]) }) rownames(dat) &lt;- NULL return(dat) } ## Read in the person-level dataset teachers &lt;- read.csv(&quot;https://stats.idre.ucla.edu/stat/examples/alda/teachers.csv&quot;) ## Look at a subset of the cases subset(teachers, id %in% c(20, 126, 129)) ## id t censor ## 19 20 3 0 ## 125 126 12 0 ## 128 129 12 1 ## Uses PLPP to convert to person-period and store in object, &#39;tpp&#39; tpp &lt;- PLPP(data = teachers, id = &quot;id&quot;, period = &quot;t&quot;, event = &quot;censor&quot;, direction = &quot;period&quot;) ## Look at a subset of the cases subset(tpp, id %in% c(20, 126, 129)) ## id t censor ## 95 20 1 0 ## 96 20 2 0 ## 97 20 3 1 ## 740 126 1 0 ## 741 126 2 0 ## 742 126 3 0 ## 743 126 4 0 ## 744 126 5 0 ## 745 126 6 0 ## 746 126 7 0 ## 747 126 8 0 ## 748 126 9 0 ## 749 126 10 0 ## 750 126 11 0 ## 751 126 12 1 ## 760 129 1 0 ## 761 129 2 0 ## 762 129 3 0 ## 763 129 4 0 ## 764 129 5 0 ## 765 129 6 0 ## 766 129 7 0 ## 767 129 8 0 ## 768 129 9 0 ## 769 129 10 0 ## 770 129 11 0 ## 771 129 12 0 ### Convert person-period to person-level ## Read in person-period dataset teachers.pp &lt;- read.csv(&quot;https://stats.idre.ucla.edu/stat/examples/alda/teachers_pp.csv&quot;) ## Look at a subset of the cases subset(teachers.pp, id %in% c(20, 126, 129)) ## id period event ## 95 20 1 0 ## 96 20 2 0 ## 97 20 3 1 ## 740 126 1 0 ## 741 126 2 0 ## 742 126 3 0 ## 743 126 4 0 ## 744 126 5 0 ## 745 126 6 0 ## 746 126 7 0 ## 747 126 8 0 ## 748 126 9 0 ## 749 126 10 0 ## 750 126 11 0 ## 751 126 12 1 ## 760 129 1 0 ## 761 129 2 0 ## 762 129 3 0 ## 763 129 4 0 ## 764 129 5 0 ## 765 129 6 0 ## 766 129 7 0 ## 767 129 8 0 ## 768 129 9 0 ## 769 129 10 0 ## 770 129 11 0 ## 771 129 12 0 18.2.6 Package survfit in R ## Compute survival curves library(&quot;survival&quot;) library(&quot;survminer&quot;) data(&quot;lung&quot;) fit &lt;- survfit(Surv(time, status) ~ 1, data = lung) ## Stratification fit &lt;- survfit(Surv(time, status) ~ sex, data = lung) summary(fit) ## Call: survfit(formula = Surv(time, status) ~ sex, data = lung) ## ## sex=1 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 11 138 3 0.9783 0.0124 0.9542 1.000 ## 12 135 1 0.9710 0.0143 0.9434 0.999 ## 13 134 2 0.9565 0.0174 0.9231 0.991 ## 15 132 1 0.9493 0.0187 0.9134 0.987 ## 26 131 1 0.9420 0.0199 0.9038 0.982 ## 30 130 1 0.9348 0.0210 0.8945 0.977 ## 31 129 1 0.9275 0.0221 0.8853 0.972 ## 53 128 2 0.9130 0.0240 0.8672 0.961 ## 54 126 1 0.9058 0.0249 0.8583 0.956 ## 59 125 1 0.8986 0.0257 0.8496 0.950 ## 60 124 1 0.8913 0.0265 0.8409 0.945 ## 65 123 2 0.8768 0.0280 0.8237 0.933 ## 71 121 1 0.8696 0.0287 0.8152 0.928 ## 81 120 1 0.8623 0.0293 0.8067 0.922 ## 88 119 2 0.8478 0.0306 0.7900 0.910 ## 92 117 1 0.8406 0.0312 0.7817 0.904 ## 93 116 1 0.8333 0.0317 0.7734 0.898 ## 95 115 1 0.8261 0.0323 0.7652 0.892 ## 105 114 1 0.8188 0.0328 0.7570 0.886 ## 107 113 1 0.8116 0.0333 0.7489 0.880 ## 110 112 1 0.8043 0.0338 0.7408 0.873 ## 116 111 1 0.7971 0.0342 0.7328 0.867 ## 118 110 1 0.7899 0.0347 0.7247 0.861 ## 131 109 1 0.7826 0.0351 0.7167 0.855 ## 132 108 2 0.7681 0.0359 0.7008 0.842 ## 135 106 1 0.7609 0.0363 0.6929 0.835 ## 142 105 1 0.7536 0.0367 0.6851 0.829 ## 144 104 1 0.7464 0.0370 0.6772 0.823 ## 147 103 1 0.7391 0.0374 0.6694 0.816 ## 156 102 2 0.7246 0.0380 0.6538 0.803 ## 163 100 3 0.7029 0.0389 0.6306 0.783 ## 166 97 1 0.6957 0.0392 0.6230 0.777 ## 170 96 1 0.6884 0.0394 0.6153 0.770 ## 175 94 1 0.6811 0.0397 0.6076 0.763 ## 176 93 1 0.6738 0.0399 0.5999 0.757 ## 177 92 1 0.6664 0.0402 0.5922 0.750 ## 179 91 2 0.6518 0.0406 0.5769 0.736 ## 180 89 1 0.6445 0.0408 0.5693 0.730 ## 181 88 2 0.6298 0.0412 0.5541 0.716 ## 183 86 1 0.6225 0.0413 0.5466 0.709 ## 189 83 1 0.6150 0.0415 0.5388 0.702 ## 197 80 1 0.6073 0.0417 0.5309 0.695 ## 202 78 1 0.5995 0.0419 0.5228 0.687 ## 207 77 1 0.5917 0.0420 0.5148 0.680 ## 210 76 1 0.5839 0.0422 0.5068 0.673 ## 212 75 1 0.5762 0.0424 0.4988 0.665 ## 218 74 1 0.5684 0.0425 0.4909 0.658 ## 222 72 1 0.5605 0.0426 0.4829 0.651 ## 223 70 1 0.5525 0.0428 0.4747 0.643 ## 229 67 1 0.5442 0.0429 0.4663 0.635 ## 230 66 1 0.5360 0.0431 0.4579 0.627 ## 239 64 1 0.5276 0.0432 0.4494 0.619 ## 246 63 1 0.5192 0.0433 0.4409 0.611 ## 267 61 1 0.5107 0.0434 0.4323 0.603 ## 269 60 1 0.5022 0.0435 0.4238 0.595 ## 270 59 1 0.4937 0.0436 0.4152 0.587 ## 283 57 1 0.4850 0.0437 0.4065 0.579 ## 284 56 1 0.4764 0.0438 0.3979 0.570 ## 285 54 1 0.4676 0.0438 0.3891 0.562 ## 286 53 1 0.4587 0.0439 0.3803 0.553 ## 288 52 1 0.4499 0.0439 0.3716 0.545 ## 291 51 1 0.4411 0.0439 0.3629 0.536 ## 301 48 1 0.4319 0.0440 0.3538 0.527 ## 303 46 1 0.4225 0.0440 0.3445 0.518 ## 306 44 1 0.4129 0.0440 0.3350 0.509 ## 310 43 1 0.4033 0.0441 0.3256 0.500 ## 320 42 1 0.3937 0.0440 0.3162 0.490 ## 329 41 1 0.3841 0.0440 0.3069 0.481 ## 337 40 1 0.3745 0.0439 0.2976 0.471 ## 353 39 2 0.3553 0.0437 0.2791 0.452 ## 363 37 1 0.3457 0.0436 0.2700 0.443 ## 364 36 1 0.3361 0.0434 0.2609 0.433 ## 371 35 1 0.3265 0.0432 0.2519 0.423 ## 387 34 1 0.3169 0.0430 0.2429 0.413 ## 390 33 1 0.3073 0.0428 0.2339 0.404 ## 394 32 1 0.2977 0.0425 0.2250 0.394 ## 428 29 1 0.2874 0.0423 0.2155 0.383 ## 429 28 1 0.2771 0.0420 0.2060 0.373 ## 442 27 1 0.2669 0.0417 0.1965 0.362 ## 455 25 1 0.2562 0.0413 0.1868 0.351 ## 457 24 1 0.2455 0.0410 0.1770 0.341 ## 460 22 1 0.2344 0.0406 0.1669 0.329 ## 477 21 1 0.2232 0.0402 0.1569 0.318 ## 519 20 1 0.2121 0.0397 0.1469 0.306 ## 524 19 1 0.2009 0.0391 0.1371 0.294 ## 533 18 1 0.1897 0.0385 0.1275 0.282 ## 558 17 1 0.1786 0.0378 0.1179 0.270 ## 567 16 1 0.1674 0.0371 0.1085 0.258 ## 574 15 1 0.1562 0.0362 0.0992 0.246 ## 583 14 1 0.1451 0.0353 0.0900 0.234 ## 613 13 1 0.1339 0.0343 0.0810 0.221 ## 624 12 1 0.1228 0.0332 0.0722 0.209 ## 643 11 1 0.1116 0.0320 0.0636 0.196 ## 655 10 1 0.1004 0.0307 0.0552 0.183 ## 689 9 1 0.0893 0.0293 0.0470 0.170 ## 707 8 1 0.0781 0.0276 0.0390 0.156 ## 791 7 1 0.0670 0.0259 0.0314 0.143 ## 814 5 1 0.0536 0.0239 0.0223 0.128 ## 883 3 1 0.0357 0.0216 0.0109 0.117 ## ## sex=2 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 5 90 1 0.9889 0.0110 0.9675 1.000 ## 60 89 1 0.9778 0.0155 0.9478 1.000 ## 61 88 1 0.9667 0.0189 0.9303 1.000 ## 62 87 1 0.9556 0.0217 0.9139 0.999 ## 79 86 1 0.9444 0.0241 0.8983 0.993 ## 81 85 1 0.9333 0.0263 0.8832 0.986 ## 95 83 1 0.9221 0.0283 0.8683 0.979 ## 107 81 1 0.9107 0.0301 0.8535 0.972 ## 122 80 1 0.8993 0.0318 0.8390 0.964 ## 145 79 2 0.8766 0.0349 0.8108 0.948 ## 153 77 1 0.8652 0.0362 0.7970 0.939 ## 166 76 1 0.8538 0.0375 0.7834 0.931 ## 167 75 1 0.8424 0.0387 0.7699 0.922 ## 182 71 1 0.8305 0.0399 0.7559 0.913 ## 186 70 1 0.8187 0.0411 0.7420 0.903 ## 194 68 1 0.8066 0.0422 0.7280 0.894 ## 199 67 1 0.7946 0.0432 0.7142 0.884 ## 201 66 2 0.7705 0.0452 0.6869 0.864 ## 208 62 1 0.7581 0.0461 0.6729 0.854 ## 226 59 1 0.7452 0.0471 0.6584 0.843 ## 239 57 1 0.7322 0.0480 0.6438 0.833 ## 245 54 1 0.7186 0.0490 0.6287 0.821 ## 268 51 1 0.7045 0.0501 0.6129 0.810 ## 285 47 1 0.6895 0.0512 0.5962 0.798 ## 293 45 1 0.6742 0.0523 0.5791 0.785 ## 305 43 1 0.6585 0.0534 0.5618 0.772 ## 310 42 1 0.6428 0.0544 0.5447 0.759 ## 340 39 1 0.6264 0.0554 0.5267 0.745 ## 345 38 1 0.6099 0.0563 0.5089 0.731 ## 348 37 1 0.5934 0.0572 0.4913 0.717 ## 350 36 1 0.5769 0.0579 0.4739 0.702 ## 351 35 1 0.5604 0.0586 0.4566 0.688 ## 361 33 1 0.5434 0.0592 0.4390 0.673 ## 363 32 1 0.5265 0.0597 0.4215 0.658 ## 371 30 1 0.5089 0.0603 0.4035 0.642 ## 426 26 1 0.4893 0.0610 0.3832 0.625 ## 433 25 1 0.4698 0.0617 0.3632 0.608 ## 444 24 1 0.4502 0.0621 0.3435 0.590 ## 450 23 1 0.4306 0.0624 0.3241 0.572 ## 473 22 1 0.4110 0.0626 0.3050 0.554 ## 520 19 1 0.3894 0.0629 0.2837 0.534 ## 524 18 1 0.3678 0.0630 0.2628 0.515 ## 550 15 1 0.3433 0.0634 0.2390 0.493 ## 641 11 1 0.3121 0.0649 0.2076 0.469 ## 654 10 1 0.2808 0.0655 0.1778 0.443 ## 687 9 1 0.2496 0.0652 0.1496 0.417 ## 705 8 1 0.2184 0.0641 0.1229 0.388 ## 728 7 1 0.1872 0.0621 0.0978 0.359 ## 731 6 1 0.1560 0.0590 0.0743 0.328 ## 735 5 1 0.1248 0.0549 0.0527 0.295 ## 765 3 1 0.0832 0.0499 0.0257 0.270 ## Access to the sort summary table summary(fit)$table ## records n.max n.start events *rmean *se(rmean) median 0.95LCL 0.95UCL ## sex=1 138 138 138 112 326.0841 22.91156 270 212 310 ## sex=2 90 90 90 53 460.6473 34.68985 426 348 550 ## function survfit() returns a list of variables,components can be accessed as follow: d &lt;- data.frame(time = fit$time, n.risk = fit$n.risk, n.event = fit$n.event, n.censor = fit$n.censor, surv = fit$surv, upper = fit$upper, lower = fit$lower ) head(d) ## time n.risk n.event n.censor surv upper lower ## 1 11 138 3 0 0.9782609 1.0000000 0.9542301 ## 2 12 135 1 0 0.9710145 0.9994124 0.9434235 ## 3 13 134 2 0 0.9565217 0.9911586 0.9230952 ## 4 15 132 1 0 0.9492754 0.9866017 0.9133612 ## 5 26 131 1 0 0.9420290 0.9818365 0.9038355 ## 6 30 130 1 0 0.9347826 0.9768989 0.8944820 ## Kaplan-Meier life table: summary of survival curves summary(fit) res.sum &lt;- surv_summary(fit) head(res.sum) ## time n.risk n.event n.censor surv std.err upper lower strata ## 1 11 138 3 0 0.9782609 0.01268978 1.0000000 0.9542301 sex=1 ## 2 12 135 1 0 0.9710145 0.01470747 0.9994124 0.9434235 sex=1 ## 3 13 134 2 0 0.9565217 0.01814885 0.9911586 0.9230952 sex=1 ## 4 15 132 1 0 0.9492754 0.01967768 0.9866017 0.9133612 sex=1 ## 5 26 131 1 0 0.9420290 0.02111708 0.9818365 0.9038355 sex=1 ## 6 30 130 1 0 0.9347826 0.02248469 0.9768989 0.8944820 sex=1 ## sex ## 1 1 ## 2 1 ## 3 1 ## 4 1 ## 5 1 ## 6 1 ## surv_summary attr(res.sum, &quot;table&quot;) ## records n.max n.start events *rmean *se(rmean) median 0.95LCL 0.95UCL ## sex=1 138 138 138 112 326.0841 22.91156 270 212 310 ## sex=2 90 90 90 53 460.6473 34.68985 426 348 550 ## Confidence Interval type ## One of &quot;none&quot;, &quot;plain&quot;, &quot;log&quot; (the default), &quot;log-log&quot;, &quot;logit&quot; or &quot;arcsin&quot;. # The none option  # The plain option  +-k *se(curve), kconf.int. # The log option calculates intervals based on the cumulative hazard or log(survival). # The log-log option bases the intervals on the log hazard or log(-log(survival)), loglog-logsurvival # The logit option on log(survival/(1- survival)) and arcsin on arcsin(survival). ## 1) Normalverteilungsannahme ## S(t) +- SE(S(t)) * u_(1-alpha/2) a &lt;- survfit(Surv(time, status) ~ sex, data = lung, conf.type=&quot;plain&quot;) ## 2) log-Methode,nicht in Vorlesung b &lt;- survfit(Surv(time, status) ~ sex, data = lung, conf.type=&quot;log&quot;) ## 3) log-log-Methode ## S(t)^(exp(+-SE(log(-logS(t)))*u_(1-alpha/2))), dabei muss SE(log(-logS(t))) mit Formel (2.16) berechnet werden # ( nicht: S(t)^(exp(+-SE *(log(-logS(t)))*u_(1-alpha/2))) ) b &lt;- survfit(Surv(time, status) ~ sex, data = lung, conf.type=&quot;log-log&quot;) #  quantile(a ,probs =1- c(0.75,0.5,0.25)) ## $quantile ## 25 50 75 ## sex=1 144 270 457 ## sex=2 226 426 687 ## ## $lower ## 25 50 75 ## sex=1 107 212 371 ## sex=2 186 345 524 ## ## $upper ## 25 50 75 ## sex=1 177 306 567 ## sex=2 310 524 735 ## Survival curves comparing ## When rho=0, log-rank test or Mantel-Haenszel test is performed. ## When rho=1, the Peto correction test of Gehan-Wilcoxon is performed, which gives a greater weight to the early outcome events. But this is not the Wilcoxon test surv_diff &lt;- survdiff(Surv(time, status) ~ sex, data = lung) surv_diff ## Call: ## survdiff(formula = Surv(time, status) ~ sex, data = lung) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## sex=1 138 112 91.6 4.55 10.3 ## sex=2 90 53 73.4 5.68 10.3 ## ## Chisq= 10.3 on 1 degrees of freedom, p= 0.001 ## Extracting information from a survdiff object surv_diff &lt;- survdiff(Surv(time, status) ~ sex, data = lung) 1 - pchisq(surv_diff$chisq, length(surv_diff$n) - 1) ## [1] 0.001311165 18.2.7 KM Survival Plots in R ## Change color, linetype by strata, risk.table color by strata ggsurvplot(fit, pval = TRUE, conf.int = TRUE, risk.table = TRUE, # Add risk table risk.table.col = &quot;strata&quot;, # Change risk table color by groups linetype = &quot;strata&quot;, # Change line type by groups surv.median.line = &quot;hv&quot;, # Specify median survival ggtheme = theme_bw(), # Change ggplot2 theme palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;)) ## The plot can be further customized ggsurvplot( fit, # survfit object with calculated statistics. pval = TRUE, # show p-value of log-rank test. conf.int = TRUE, # show confidence intervals for # point estimaes of survival curves. conf.int.style = &quot;step&quot;, # customize style of confidence intervals xlab = &quot;Time in days&quot;, # customize X axis label. break.time.by = 200, # break X axis in time intervals by 200. ggtheme = theme_light(), # customize plot and risk table with a theme. risk.table = &quot;abs_pct&quot;, # absolute number and percentage at risk. risk.table.y.text.col = T,# colour risk table text annotations. risk.table.y.text = FALSE,# show bars instead of names in text annotations # in legend of risk table. ncensor.plot = TRUE, # plot the number of censored subjects at time t surv.median.line = &quot;hv&quot;, # add the median survival pointer. legend.labs = c(&quot;Male&quot;, &quot;Female&quot;), # change legend labels. palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;) # custom color palettes. ) ### Plot cumulative events ## fun = &quot;event&quot; cumulative events ## fun = &quot;log&quot;: log transformation ## fun = &quot;cumhaz&quot;: cumulative hazard ## plot cumulative events ggsurvplot(fit, conf.int = TRUE, risk.table.col = &quot;strata&quot;, # Change risk table color by groups ggtheme = theme_bw(), # Change ggplot2 theme palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;), fun = &quot;event&quot;) ## Plot log survival curve ggsurvplot(fit, conf.int = TRUE, risk.table.col = &quot;strata&quot;, # Change risk table color by groups ggtheme = theme_bw(), # Change ggplot2 theme palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;), fun = &quot;log&quot;) ## Plot cummulative hazard ## The cummulative hazard is commonly used to estimate the hazard probability ggsurvplot(fit, conf.int = TRUE, risk.table.col = &quot;strata&quot;, # Change risk table color by groups ggtheme = theme_bw(), # Change ggplot2 theme palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;), fun = &quot;cumhaz&quot;) ## Complex survival curves in grouping fit2 &lt;- survfit( Surv(time, status) ~ sex + rx + adhere, data = colon ) # Plot survival curves by sex and facet by rx and adhere ggsurv &lt;- ggsurvplot(fit2, fun = &quot;event&quot;, conf.int = TRUE, ggtheme = theme_bw()) ggsurv$plot +theme_bw() + theme (legend.position = &quot;right&quot;)+ facet_grid(rx ~ adhere) 18.3 Compare the survival function 18.3.1 Tests of equality of the survival function The log-rank test, is a hypothesis test to compare the survival distributions of two samples. It is a nonparametric test and appropriate to use when the data are right-skewed and censored (technically, the censoring must be non-informative). The logrank test is based on the same assumptions as the Kaplan-Meier survival curvenamely, that censoring is unrelated to prognosis, the survival probabilities are the same for subjects recruited early and late in the study, and the events happened at the times specified. Deviations from these assumptions matter most if they are satisfied differently in the groups being compared, for example if censoring is more likely in one group than another The calculation of the statistic for the nonparametric Log-Rank and Wilcoxon tests is given by : \\[Q = \\frac{\\bigg[\\sum\\limits_{i=1}^m w_j(d_{ij}-\\hat e_{ij})\\bigg]^2}{\\sum\\limits_{i=1}^m w_j^2\\hat v_{ij}},\\] \\(d_{i j}\\) is the observed number of failures in stratum \\(i\\) at time \\(t_{i j}\\) \\(\\hat{e}_{i j}\\) is the expected number of failures in stratum \\(i\\) at time \\(t_{i j}\\) \\(\\hat{v}_{i j}\\) is the estimator of the variance of \\(d_{i j}\\) \\(w_{j}\\) is the weight of the difference at time \\(t_{j}\\) The log-rank or Mantel-Haenzel test uses \\(w_{j}=1\\) The Wilcoxon test uses \\(w_{j}=n_{j}\\), so that differences are weighted by the number at risk at time \\(t_{j}\\) Specified, for group 1, the log-rank statistic can be written as \\[ \\sum_{j=1}^{r}\\left(d_{1 j}-e_{1 j}\\right) \\] where the summation is over all unique event times (in both groups), and there are a total of \\(r\\) such times. \\(d_{1 j}\\) is the number of deaths that occur in group 1 at time \\(j\\), and \\(e_{1 j}\\) is the expected number of events in group 1 at time \\(j\\). The expected number is given by \\(n_{1 j} d_{j} / n_{j}\\), where \\(n_{j}\\) is the total number of cases that are at risk just prior to time \\(j, n_{1 j}\\) is the number at risk just prior to time \\(j\\) in group 1, and \\(d_{j}\\) is the total number of deaths at time \\(j\\) in both groups. The Wilcoxon statistic, given by \\[\\sum_{j=1}^{r} n_{j}\\left(d_{1 j}-e_{1 j}\\right)\\] strata statement in proc lifetest in SAS proc lifetest data=whas500 atrisk plots=survival(atrisk cb) outs=outwhas500; strata gender; time lenfol*fstat(0); run; Figure 18.1: Figure: Strata statement in proc lifetest Test of Equality over Strata Title Title Title Test Chi-Square DF Pr &gt; Chi-Square Log-Rank 7.7911 1 0.0053 Wilcoxon 5.5370 1 0.0186 -2Log(LR) 10.5120 1 0.0012 log-rank test v.s. Wilcoxon test If the log-rank test is meaningful but the Wilcoxon test is meaningless, it indicates that the long-term difference may be large, but not necessarily in the early stage, and the difference may not be large. If the log-rank test is meaningless and the Wilcoxon test is meaningful, it indicates that there is a large difference in early survival and little difference in long-term survival. Because the Wilcoxon test is more important for early weights than for late weights (nj does not increase with time), it is not as sensitive as log-rank test to differences between groups that occur at later time points. In other words, although the two statistics test the same null hypothesis, they have different sensitivities to various deviations from the hypothesis. Wilcoxon Wilcoxon Wilcoxonnj In particular, the log-rank test is more powerful for detecting differences of the form \\[S_{1}(t)=\\left[S_{2}(t)\\right]^{\\gamma}, \\gamma &gt; 1\\] This equation defines a proportional hazards model, the log-rank test is closely related to tests for differences between two groups that are performed within the framework of Coxs proportional hazards model In contrast, the Wilcoxon test is more powerful than the log-rank test in situations where event times have log-normal distributions (discussed in the next chapter) with a common variance but with different means in the two groups. 18.3.2 Other nonparametric tests for STRATA statement In addition to the log-rank and Wilcoxon tests, which are produced by default, the STRATA statement also has options for four other nonparametric tests: Tarone-Ware, Peto-Peto, modified Peto-Peto, Fleming-Harrington. Like the Wilcoxon and log-rank tests, all of these can be represented as a weighted sum of observed and expected numbers of events: STRATA treat / TESTS=ALL; For the Tarone-Ware test, \\(w j\\) is the square root of \\(n i\\), so this test behaves much like the Wilcoxon test in being more sensitive to differences at earlier rather than later times. \\[ \\sum_{j=1}^{r} w_{j}\\left(d_{1 j}-e_{1 j}\\right) \\] Thats also true of the two Peto tests, for which \\(w_{j}\\) is a function of the survivor function itself. \\[ \\sum_{j=1}^{r} \\hat{S}\\left(t_{j}\\right)\\left(d_{1 j}-e_{1 j}\\right) \\] Fleming-Harrington is actually a family of tests in which the weights depend on two parameters, \\(p\\) and \\(q\\) which can be chosen by the user: \\[w_{j}=\\hat{S}\\left(t_{j}\\right)^{p}\\left[1-\\hat{S}\\left(t_{j}\\right)\\right]^{q}\\] When both \\(\\mathrm{p}\\) and \\(\\mathrm{q}\\) are 0 , you get the log-rank test. When \\(\\mathrm{p}\\) is 1 and \\(\\mathrm{q}\\) is 0 , you get something very close to the Peto-Peto test. When \\(\\mathrm{q}\\) is 1 and \\(\\mathrm{p}\\) is 0 , wi increases with time, unlike any of the other tests. 18.3.3 Multiple comparisons When more than one strata, the ADJUST option tells PROC LIFETEST to produce p-values for all six pairwise comparisons of the four strata and then to report p-values that have been adjusted for multiple comparisons using Tukeys method (other methods are also available): PROC LIFETEST DAta=my.recid; TIME week*arrest(0); STRATA wexp paro / ADJUST=TUKEY; RUN; For numeric variables, you can use the STRATA statement to define groups by intervals rather than by unique values. PROC LIFETEST DAta=my.recid; TIME week*arrest(0); STRATA age(21 24 28) / ADJUST=BON; RUN; age &lt; 21 21  age &lt; 24 24  age &lt; 28 28  age 18.3.4 Comparing survival functions using Log-Rank HR One way to compare two survival curves is to calculate the hazard ratio (HR) proc lifetest data=whas500 atrisk plots=hazard(bw=200) outs=outwhas500; strata bmi(15,18.5,25,30,40); time lenfol*fstat(0); run; Kaplan-Meier () ,  : - Treatment A and Treatment B. the observed and expected deaths are summed, to give - \\(O_{A}=\\Sigma O_{A t}, O_{B}=\\Sigma O_{B t}, E_{A}=\\Sigma E_{A t}\\), and \\(E_{B}=\\Sigma E_{B t}\\) - Finally the Logrank statistic is calculated as \\[ \\chi_{\\text {Logrank }}^{2}=\\frac{\\left(O_{A}-E_{A}\\right)^{2}}{E_{A}}+\\frac{\\left(O_{B}-E_{B}\\right)^{2}}{E_{B}} \\] - It can be written as \\[ \\chi_{\\text {Logrank }}^{2}=\\left(O_{A}-E_{A}\\right)^{2}\\left(\\frac{1}{E_{A}}+\\frac{1}{E_{B}}\\right) \\] - since \\(\\left(O_{A}-E_{A}\\right)^{2}=\\left(O_{B}-E_{B}\\right)^{2}\\). - We can obtain the ratio \\(O_{A} / E_{A}\\) and \\(\\left.O_{B} / E_{B}\\right)\\), we can calculate the hazard ratio HR, defined as the ratio of these two relative death rates; that is, \\[H R=\\frac{O_{A} / E_{A}}{O_{B} / E_{B}}\\] Confidence Interval of HR In calculating \\(C I s\\), it is convenient if the statistic under consideration can be assumed to follow an approximately Normal distribution. However, the estimate of the \\(H R\\) is not normally distributed. In particular it has a possible range of values from 0 to \\(\\infty\\), with the null hypothesis value of unity not located at the centre of this interval. To make the scale symmetric and to enable us to calculate \\(C I s\\), we transform the estimate to make it approximately normally distributed. We do this by using log \\(H R\\), rather than \\(H R\\) itself, as the basis for our calculation. It is possible to show that a general \\(100(1-\\alpha) \\% C I\\) for the \\(\\log H R\\) is Convert the estimated value to an approximately normal distribution. For this, we use logarithmic HR \\[ \\log H R-\\left[z_{1-\\alpha / 2} \\times S E(\\log H R)\\right] \\text { to } \\log H R+\\left[z_{1-\\alpha / 2} \\times S E(\\log H R)\\right] \\] HR itself is then \\[ \\exp \\left[\\operatorname { l o g } H R - [ z _ { 1 - \\alpha / 2 } \\times S E ( \\operatorname { l o g } H R ) ] \\text { to } \\operatorname { e x p } \\left[\\log H R+\\left[z_{1-\\alpha / 2} \\times S E(\\log H R)\\right]\\right.\\right. \\] In both these expressions \\[S E(\\log H R)=\\sqrt{\\left(\\frac{1}{E_{A}}+\\frac{1}{E_{B}}\\right)}\\] 18.3.5 Mantel-Haenszel HR , \\(S E(\\log H R)\\)  \\(S E\\)   hypergeometric variance Logrank Mantel-Haenszel \\(t\\) : \\[ V_{t}=\\frac{m_{t} n_{t} r_{t} s_{t}}{N_{t}^{2}\\left(N_{t}-1\\right)} \\] If all deaths occur at different times, that is, there is no fixed observation value, then \\[r{t}=1, s{t}=N_{t}-1\\] \\[V_{t}=m_{t} n_{t} / N_{t}^{2} .\\] As for both \\(E_{A t}\\) and \\(E_{B t}\\), we finally sum the individual \\(V_{t}\\) calculated at each distinct event time to obtain \\(V=\\Sigma V_{t}\\). The Mantel-Haenszel test statistic is then defined as \\[ \\chi_{M H}^{2}=\\frac{\\left(O_{A}-E_{A}\\right)^{2}}{V} \\] This has an approximate \\(\\chi^{2}\\) distribution with \\(d f=1\\) in the same way as the Logrank test. It should be noted that \\(\\chi_{M H}^{2}\\) only differs from \\(\\chi_{\\text {Logrank }}^{2}\\) by the expression for \\(V\\). The use of the Mantel-Haenszel statistic to test for the difference between two groups also gives an alternative estimate of the HR: \\[ H R_{M H}=\\exp \\left(\\frac{O_{A}-E_{A}}{V}\\right) \\] The corresponding \\(S E\\) of the Mantel-Haenszel \\(\\log H R_{M H}\\) is \\[S E\\left(\\log H R_{M H}\\right)=1 / V^{1 / 2}\\] This \\(S E\\left(\\log H R_{M H}\\right)\\) can be used to give \\(C I\\) s derived from the Mantel-Haenszel estimate of the \\(H R\\) as \\[ \\exp \\left[\\log H R_{M H}-\\left(z_{1-\\alpha / 2} / V^{1 / 2}\\right)\\right] \\text { to } \\exp \\left[\\log H R_{M H}+\\left(z_{1-\\alpha / 2} / V^{1 / 2}\\right)\\right] \\] See more under Survival Analysis-Survival Analysis Using SAS - A Practical Guide (2nd Edition) 18.4 Cox Proportional Hazards Model 18.4.1 Introduction The hazard rate can also be interpreted as the rate at which failures occur at that point in time, or the rate at which risk is accumulated \\[h(t|x)=exp(\\beta_0 + \\beta_1x)\\] \\[h(t)=h_0(t)exp(x\\beta_x)\\] \\[HR = \\frac{h(t|x_2)}{h(t|x_1)} = \\frac{h_0(t)exp(x_2\\beta_x)}{h_0(t)exp(x_1\\beta_x)} = exp(\\beta_x(x_2-x_1))\\] Partial Likelihood , \\(\\quad j\\)  \\(T_{1} \\sim \\lambda_{1}(t), T_{2} \\sim \\lambda_{2}(t), \\ldots T_{j} \\sim \\lambda_{j}(t)\\) Cox Regression  \\[ P\\left(T_{1}&lt;T_{2}&lt;\\ldots&lt;T_{J}\\right)=\\prod_{i=1}^{J} \\frac{\\lambda_{j}(t)}{\\sum_{k=j}^{J} \\lambda_{k}(t)}=\\prod_{i=1}^{J} \\frac{\\exp \\left(x_{j}^{T} \\beta\\right)}{\\sum_{k=j}^{J} \\exp \\left(x_{k}^{T} \\beta\\right)} \\] , ,  \\(t_{j}\\),  : \\[ \\frac{\\exp \\left(x_{j}^{T} \\beta\\right)}{\\sum_{k \\in R\\left(t_{j}\\right)} \\exp \\left(x_{k} \\beta\\right)} \\]  \\(\\beta \\)  , given the subjects covariates values \\(x_{j}\\)  \\[ L(\\beta)=\\prod_{j} \\frac{\\exp \\left(x_{j}^{T} \\beta\\right)}{\\sum_{k \\in R\\left(t_{j}\\right)} \\exp \\left(x_{k} \\beta\\right)} \\] \\(k \\in R\\left(t_{j}\\right)\\)  \\(t_{j}-\\)  \\(k\\)  \\(t_{j}\\)    , , Cox1975  (Partial Likelihood) NR iteration for parameter estimate  \\[ \\hat{\\beta}^{(r+1)}=\\hat{\\beta}^{(r)}+\\left(X^{T} W X\\right)^{-1} X^{T}(d-P d) \\] - \\(w_{i}=\\exp \\left(x_{i}^{T} \\beta\\right)\\) - \\(Y_{i}\\left(t_{j}\\right)\\)  \\(t_{j}\\) 1, \\(Y_{i}\\left(t_{j}\\right)\\)  \\(t_{j}\\) index - \\(\\pi_{i j}=Y_{i}\\left(t_{j}\\right) \\frac{w_{i}}{\\sum_{k \\in R\\left(t_{j}\\right)} w_{k}}=Y_{i}\\left(t_{j}\\right) \\frac{w_{i}}{\\sum_{k=1}^{n} Y_{k}\\left(t_{j}\\right) w_{k}}\\) - \\(P=\\left\\{\\pi_{i j}\\right\\}_{i, j}\\), \\(i\\)  \\(t_{j}\\)  - \\(W_{k k}=-\\sum_{i} \\delta_{i} \\pi_{k i}\\left(1-\\pi_{k i}\\right), W_{k k}\\)  \\(i\\) ,  \\(k\\)   \\(\\pi(1-\\pi)\\) - \\(W_{k j}=\\sum_{i} \\delta_{i} \\pi_{k i} \\pi_{j i}, k j\\)  \\(k\\)  \\(k, j\\)  \\((\\) Y ),  \\(\\pi_{k} \\pi_{j}\\) Wald \\[\\quad \\hat{\\beta} \\sim N\\left(\\beta,\\left(X^{T} W X\\right)^{-1}\\right)\\] Likelihood Ratio Likelihood Ratio Test(LRT) LRT ()  \\[-2\\left(l\\left(\\hat{\\beta}_{1}\\right)-l\\left(\\hat{\\beta}_{2}\\right)\\right) \\sim \\chi_{p}^{2}, \\quad\\] Score Test Score Method \\[u(\\beta)=\\sum_{i=1}^{n} \\delta_{i} x_{i}-\\sum_{i=1}^{n} \\sum_{k=1}^{n} Y_{k}\\left(t_{i}\\right) \\frac{w_{k}}{\\sum_{j=1}^{n} Y_{j}\\left(t_{i}\\right) w_{j}} x_{i} \\delta_{i}=X^{T}(d-P d)\\] \\[I(\\beta)=\\sum_{i=1}^{n} \\sum_{k=1}^{n} \\delta_{i} x_{i}\\left[\\frac{Y_{k}\\left(t_{i}\\right) w_{k}}{\\sum_{j=1}^{n} Y_{j}\\left(t_{i}\\right) w_{j}} \\frac{x_{k}^{T}\\left(\\sum_{j=1}^{n} Y_{j}\\left(t_{i}\\right) w_{j}\\right)+\\sum_{j=1}^{n} Y_{j}\\left(t_{i}\\right) w_{j} x_{j}^{T}}{\\sum_{j=1}^{n} Y_{j}\\left(t_{i}\\right) w_{j}}\\right]\\] \\(u\\left(\\beta_{0}\\right) / \\sqrt{I\\left(\\beta_{0}\\right)}\\), \\(\\hat{\\beta}\\) "]]
