[["mixed-model.html", "Chapter 22 Mixed Model 22.1 Introduction 22.2 Random Slope and Intercept Model 22.3 Random Slope and Intercept Model with Covariance Structure 22.4 Generalized Estimating Equation 22.5 Hierarchical Regression Model for Normal Response", " Chapter 22 Mixed Model 22.1 Introduction 22.1.1 Correlated response data        Repeated measurements:    Clustered data    Such as Paired data Studies on twins where each pair serves as a natural cluster. Ophthalmology studies where a pair of eyes serves as a cluster. Spatially correlated data  22.1.2 Hierarchical and Marginal model The general linear mixed model can be written as: \\[ \\begin{array}{c} \\mathbf{Y}_{i}=X_{i} \\beta+Z_{i} \\mathbf{b}_{i}+\\epsilon_{i} \\\\ \\mathbf{b}_{i} \\sim N(\\mathbf{0}, D) \\quad \\text { und } \\quad \\epsilon_{i} \\sim N\\left(0, \\Sigma_{i}\\right) \\end{array} \\] und \\(\\mathbf{b}_{1}, \\ldots, \\mathbf{b}_{N}, \\epsilon_{1}, \\ldots, \\epsilon_{N}\\) stoch. unabhängig sind Dies kann umgeschrieben werden als Hierarchical model \\[ \\mathbf{Y}_{i} \\mid \\mathbf{b}_{i}=N\\left(X_{i} \\beta+Z_{i} \\mathbf{b}_{i}, \\Sigma_{i}\\right), \\quad \\mathbf{b}_{i} \\sim N(\\mathbf{0}, D) \\] One often assumes that \\(\\Sigma_{i}\\) only depends on the dimension of \\(i\\) i.e. that the unknown parameters in \\(\\Sigma_{i}\\) are independent on \\(i\\) ( \\(i\\) ,  \\(\\Sigma_{i}\\)  \\(i\\) ) Marginal model The following marginal model follows from the hierarchical model: \\[\\mathbf{Y}_{i} \\sim N\\left(X_{i} \\beta, Z_{i} D Z_{i}^{T}+\\Sigma_{i}\\right)\\] The hierarchical model implies the marginal model, the reverse is not generally true: \\(\\quad V_{i}=Z_{i} D Z_{i}^{T}+\\Sigma_{i}\\) 22.1.3 MLE in Marginal model \\[ \\mathbf{Y}=\\left(\\begin{array}{c} \\mathbf{Y}_{1} \\\\ \\vdots \\\\ \\mathbf{Y}_{N} \\end{array}\\right), \\quad \\mathbf{V}=\\operatorname{diag}\\left(V_{1}, \\ldots, V_{N}\\right), \\quad \\mathbf{X}=\\left(\\begin{array}{c} X_{1} \\\\ \\vdots \\\\ X_{N} \\end{array}\\right) \\]  \\(\\alpha\\)  \\(q(q+1) / 2\\) ,  \\(D\\)  \\(\\Sigma_{i}\\)  \\(\\mathbf{V}\\)  \\(\\alpha\\),   \\(\\mathbf{V}(\\alpha)\\) - \\[ \\mathbf{Y} \\sim N(\\mathbf{X} \\beta, \\mathbf{V}(\\alpha)) \\] The Log-Likelihood-Kern is \\[I(\\beta, \\alpha)=-\\frac{1}{2}\\left\\{|\\mathbf{V}(\\alpha)|+(\\mathbf{Y}-\\mathbf{X} \\beta)^{T} \\mathbf{V}(\\alpha)^{-1}(\\mathbf{Y}-\\mathbf{X} \\beta)\\right\\}\\]  \\(\\alpha\\) () \\(\\beta\\) MLAitken: \\[ \\hat{\\beta}(\\alpha)=\\left(\\mathbf{X}^{T} \\mathbf{V}(\\alpha)^{-1} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{T} \\mathbf{V}(\\alpha)^{-1} \\mathbf{Y} \\]  \\(\\hat{\\alpha}\\)  \\(\\hat{\\beta}\\)  \\(\\alpha\\) (Nullstelle des Scores ) Der MLE für \\(\\beta\\) ergibt sich dann durch einsetzen von \\(\\hat{\\alpha}\\) in obige Formel \\((\\hat{\\beta}(\\hat{\\alpha}))\\) Problem of MLE  MLEEMIGLS     XNXN-1MLNN-1 22.1.4 REML in Marginal model REMLN-1MLEREML  REML ordinary least squares (OLS) . OLSXOLS0 REML GLSmatrix multiplication REMLGeneralized least squares () PROC MIXEDPROC GLIMMIXMLEREML REMLPattersonThompson 1971  REMLPROC MIXEDMETHOD = MLPROC GLIMMIXMETHOD = MSPLMLMLREML ML For example, in the one-sample case, when \\(y_{1}, y_{2}, \\ldots, y_{n}\\) is a random sample from \\(\\mathrm{N}\\left(\\mu, \\sigma^{2}\\right)\\), the ML estimate of the variance is as follows: \\[ \\sum_{i}\\left(y_{i}-\\bar{y}\\right)^{2} / n \\] whereas the sample variance - which is the simplest REML variance estimate-is as follows: \\[ \\sum_{i}\\left(y_{i}-\\bar{y}\\right)^{2} /(n-1) \\] ML1 ( \\(a=0.05\\), 25%)  proc mixed data=bond method=reml; class ingot metal; model pres=metal; random ingot; run; proc glimmix data=bond method=rspl; class ingot metal; model pres=metal; random ingot; run; 22.1.5 Variance correction 22.1.5.1 Kackar-Harville correction  () MLEREML p-valuefisher information (asympotics)  , , Bayesian  (asympotics)Kackar-HarvilleKendward-roger  Kackar-Harville, REML  \\[ \\operatorname{Var}(\\gamma)=\\operatorname{Var}^{R E M L}(\\hat{\\gamma})+\\text { Small Sample Bias } \\] KHKRsmall sample biasKH, Small sample bias (SSB)   () ; Taylor series expansion SSB; Prasad-Rao, Harville-JeskeKH Prasad-Rao-Jeske-kackar-Harville correction. 22.1.5.2 Kendward-Roger correction KRREML, REML GLS,  KRREML,  , t Z (F) Kt=/  KR (method of moments matching procedure) ,  22.1.6 Hypothesis tests Tests of fixed effects are typically done with either Wald or likelihood ratio (LRT) tests. With the assumptions of asymptotic distributions and independent predictors, Wald and LRT tests are equivalent. When a data set size is not large enough to be a good approximation of the asymptotic distribution or there is some correlation amongst the predictors, the Wald and LRT test results can vary considerably. Wald test. Tests of the effect size which is scaled using the estimated standard error. LRT (Likelihood Ratio Test.) Tests the difference in two nested models using the Chi square distribution. The Wald test is based only on estimates from the model being evaluated. This results in an implied assumption that a model which holds the parameter being tested to zero will be the same with the exception of the parameter which is being tested. Correlation between the tested predictor and the other model predictors, can cause the estimate made from the model including the parameter to be different from a model which holds the parameter to zero. The LRT requires the formal estimation of a model which restricts the parameter to zero and therefore accounts for correlation in its test. Wald LRT 22.1.6.1 Wald Test j  ,  \\(\\beta\\) j ( \\()\\) dj,  aH0 \\[ \\begin{array}{c} H_{0}: \\beta_{j}=d_{j} \\text { gegen } H_{1}: \\beta_{j} \\neq d_{j} \\\\ t_{j}=\\frac{\\hat{\\beta}_{j}-d_{j}}{\\hat{\\sigma}_{j}} \\end{array} \\]  \\(\\hat{\\sigma}_{j}\\)  \\(\\hat{\\boldsymbol{\\beta}}\\)  \\(j\\) -th \\(\\gamma\\)  \\(\\varepsilon\\)  \\(t_{j}\\)  ,  \\(\\left|t_{j}\\right|&gt;z_{1-\\alpha / 2}\\)  \\(\\$ H_{-}\\{0\\} \\$\\) $  \\[ H_{0}: \\boldsymbol{C} \\boldsymbol{\\beta}=\\boldsymbol{d} \\text { gegen } H_{1}: \\boldsymbol{C} \\boldsymbol{\\beta} \\neq \\boldsymbol{d} \\] \\[ W=(\\boldsymbol{C} \\hat{\\boldsymbol{\\beta}}-\\boldsymbol{d})^{\\prime}\\left(\\boldsymbol{C}^{\\prime} \\boldsymbol{A}_{11} \\boldsymbol{C}\\right)^{-1}(\\boldsymbol{C} \\hat{\\boldsymbol{\\beta}}-\\boldsymbol{d}) \\] The approximate covariance matrices are given by \\[\\widehat{\\operatorname{Cov}}(\\hat{\\boldsymbol{\\beta}})=\\boldsymbol{A}_{11}=\\left\\{\\sum_{i=1}^{m} \\boldsymbol{X}_{i}^{\\prime} \\hat{\\boldsymbol{V}}_{i}^{-1} \\boldsymbol{X}_{i}\\right\\}^{-1}\\] 22.1.6.2 Likelihood Ratio Test The Likelihood Ratio Test (LRT) of fixed effects requires the models be fit with by MLE (use REML=FALSE for linear mixed models.) The LRT of mixed models is only approximately 2 distributed. For tests of fixed effects the p-values will be smaller. Thus if a p-value is greater than the cutoff value, you can be confident that a more accurate test would also retain the null hypothesis. For p-values that are only a little below the cutoff value, a more accurate approach would need to be used. 22.1.7 Residual Structure  \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{array}\\right] \\] Figure 22.1: Figure: Residual Structure in the mixed model  0.12 group effect variance sd var_prop student Intercept 0.064 0.252 0.523 Residual Content 0.058 0.241 0.477 (relax the assumption of equal variances, and estimate each separately.) \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma_1^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3^2 \\\\ \\end{array}\\right] \\] (covariance structure of compound symmetry ) \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2\\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} \\\\ \\end{array}\\right] \\] /    \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{ccc} 1 &amp; \\rho_1 &amp; \\rho_2 \\\\ \\rho_1 &amp; 1 &amp; \\rho_3 \\\\ \\rho_2 &amp; \\rho_3 &amp; 1 \\\\ \\end{array}\\right] \\] 1    2 \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 \\\\ \\end{array}\\right] \\] 22.1.8 Converge problems 1.2.(. 1-1) 0.91Corr0.9 (Barr D. J., 2013) Corr0.9Corr 3- CorrCorrCorr0.9Corr  22.1.9 Preface of linear mixed model shrimp &lt;- read.csv(&quot;./01_Datasets/shrimp.csv&quot;,sep=&quot;,&quot;, header=TRUE) ## AnimalID- ## SireID- ## DamID- ## FamilyID- ## SexID- ## TankID- ## M1BW- ## M2BW-M2Age ##  library(ggplot2) ggplot(data=shrimp,aes(x=SexID,y=M2BW,color=SexID))+ geom_boxplot()+ geom_dotplot(binaxis = &quot;y&quot;,stackdir = &quot;center&quot;,position = &quot;dodge&quot;,binwidth = 0.25) ##  ## Howevwe  ## 2105 TankID ##  ## Choose the model  ## &quot;SAS for Mixed models (Second edition)&quot;: An effect is called fixed if the levels in the study represent all possible levels of the factor, or at least all levels about which inference is to be made ## Factor effects are random if they are used in the study to represent only a sample (ideally, a random sample) of a larger set of potential levels ##  ## ,  ## a) b)  shrimp.lm &lt;- lm(M2BW~SexID,shrimp) summary(shrimp.lm) ## ## Call: ## lm(formula = M2BW ~ SexID, data = shrimp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.9765 -3.1765 -0.0765 3.0235 18.9235 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.3765 0.1082 317.65 &lt;2e-16 *** ## SexIDMale -6.2137 0.1560 -39.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.101 on 4280 degrees of freedom ## Multiple R-squared: 0.2704, Adjusted R-squared: 0.2702 ## F-statistic: 1586 on 1 and 4280 DF, p-value: &lt; 2.2e-16 library(dplyr) shrimp %&gt;% group_by(SexID) %&gt;% transmute(sex.mean=mean(M2BW)) %&gt;% unique() ## # A tibble: 2 x 2 ## # Groups: SexID [2] ## SexID sex.mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Male 28.2 ## 2 Female 34.4 ggplot(data=shrimp,aes(x=SexID,y=M2BW,color=SexID))+ geom_dotplot(binaxis = &quot;y&quot;,stackdir = &quot;center&quot;,position = &quot;dodge&quot;,binwidth = 0.25)+ geom_vline(xintercept = 1)+ geom_vline(xintercept = 2)+ geom_hline(yintercept = 34.37646)+ geom_hline(yintercept = 28.16273)+ geom_abline(intercept = 34.3765+6.2137, slope=-6.2137)+ geom_text(x=2.4,y=31.5,label=&quot;-6.6155&quot;)+ annotate(&quot;segment&quot;,x=2.5,xend=2.5,y=28.16273,yend = 34.37646) ## Pop:FamilyPopSexTankSex:M1BW library(lme4) shrimp.lm.9 &lt;- lmer(M2BW ~ 1 + PopID + SexID + TankID + SexID:M1BW + (1|PopID:FamilyID),shrimp) summary(shrimp.lm.9) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: M2BW ~ 1 + PopID + SexID + TankID + SexID:M1BW + (1 | PopID:FamilyID) ## Data: shrimp ## ## REML criterion at convergence: 23070.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.5304 -0.5854 0.0240 0.6388 3.8977 ## ## Random effects: ## Groups Name Variance Std.Dev. ## PopID:FamilyID (Intercept) 5.933 2.436 ## Residual 12.527 3.539 ## Number of obs: 4241, groups: PopID:FamilyID, 105 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 35.7188 1.0605 33.683 ## PopIDPop2 -1.6596 0.6884 -2.411 ## PopIDPop3 -3.7742 0.6733 -5.606 ## PopIDPop4 -5.8638 0.7359 -7.968 ## SexIDMale -5.6599 0.5901 -9.591 ## TankIDT2 -2.9491 0.1097 -26.884 ## SexIDFemale:M1BW 0.3757 0.1204 3.122 ## SexIDMale:M1BW 0.2904 0.1231 2.360 ## ## Correlation of Fixed Effects: ## (Intr) PpIDP2 PpIDP3 PpIDP4 SxIDMl TnIDT2 SIDF:M ## PopIDPop2 -0.384 ## PopIDPop3 -0.419 0.506 ## PopIDPop4 -0.519 0.476 0.494 ## SexIDMale -0.248 0.002 0.007 -0.004 ## TankIDT2 -0.042 -0.006 -0.004 -0.001 -0.030 ## SxIDFm:M1BW -0.889 0.077 0.108 0.250 0.291 -0.010 ## SxIDMl:M1BW -0.711 0.074 0.100 0.246 -0.352 0.010 0.786 anova(shrimp.lm.9) ## Analysis of Variance Table ## npar Sum Sq Mean Sq F value ## PopID 3 1233 411 32.8225 ## SexID 1 40283 40283 3215.7223 ## TankID 1 9038 9038 721.5100 ## SexID:M1BW 2 122 61 4.8839 ## Compare with linear model shrimp.lm.8 &lt;- lm(M2BW ~ 1 + PopID + SexID + TankID + SexID:M1BW,shrimp) ## Mixed model Residual Variance 12.527 ##  ## , mean(resid(shrimp.lm.8)^2) ## [1] 18.42117 22.2 Random Slope and Intercept Model This structure assumes that the errors are independent, and thus is termed an independent structure. \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma_1^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3^2 \\\\ \\end{array}\\right] \\] 22.2.1 Model Introduction Suppose data are collected longitudinally at times \\(t_{1}, \\ldots, t_{p} .\\) For each individual \\(i, i=1, \\ldots, n\\), at time \\(t_{j}, j=1, \\ldots, p\\), the observations are \\(x_{1 i j}, \\ldots, x_{k i j}, y_{i j}\\) The random slope and intercept model is defined as \\[ y_{i j}=\\beta_{0}+\\beta_{1} x_{1 i j}+\\cdots+\\beta_{k} x_{k i j}+\\beta_{k+1} t_{j}+u_{1 i}+u_{2 i} t_{j}+\\varepsilon_{i j} \\] where \\(u_{1 i}\\) s are independent \\(\\mathcal{N}\\left(0, \\sigma_{u_{1}}^{2}\\right)\\) random intercepts, \\(u_{2 i}\\) s are independent \\(\\mathcal{N}\\left(0, \\sigma_{u_{2}}^{2}\\right)\\) random slopes, and \\(\\varepsilon_{i j}\\) s are independent \\(\\mathcal{N}\\left(0, \\sigma^{2}\\right)\\) errors that are also independent of \\(u_{1 i}\\) s and \\(u_{2 i}\\) s It is assumed that \\(\\operatorname{Cov}\\left(u_{1 i}, u_{2 i}\\right)=\\sigma_{u_{1} u_{2}}\\), and \\(\\operatorname{Cov}\\left(u_{1 i}, u_{2 i^{\\prime}}\\right)=0\\) for \\(i \\neq i^{\\prime}\\)  \\(x_{1}, \\ldots, x_{k}\\) ,  \\(u_{1}\\)  \\(u_{2}\\) ,   \\(x 1, \\ldots, x k\\)  \\(\\operatorname{Cov}\\left(y_{i j}, y_{i^{\\prime} j^{\\prime}}\\right)=0, i \\neq i^{\\prime}\\), , , ,  \\(\\operatorname{Cov}\\left(y_{i j}, y_{i j^{\\prime}}\\right)=\\sigma_{u_{1}}^{2}+\\sigma_{u_{1} u_{2}}\\left(t_{j}+t_{j^{\\prime}}\\right)+\\sigma_{u_{2}}^{2} t_{j} t_{j^{\\prime}}\\) for \\(j \\neq j^{\\prime}\\). ,  \\(y_{i j}\\)  \\(y_{i j}\\) \\[\\mathbb{E}(y)=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+\\beta_{k+1} t\\] \\[\\operatorname{Var}\\left(y_{i j}\\right)=\\sigma_{u_{1}}^{2}+2 \\sigma_{u_{1} u_{2}} t_{j}+\\sigma_{u_{2}}^{2} t_{j}^{2}+\\sigma^{2}\\] 22.2.2 SAS Implementation ## short-and-wide form to the long-form data set data cholesterol; input id gender$ age LDL0 LDL6 LDL9 LDL24 @@; cards; 1 M 50 73 71 80 85 2 F 72 174 164 139 112 3 M 46 85 86 82 90 4 F 71 172 150 139 127 5 F 75 186 177 153 145 6 F 68 184 169 153 138 7 F 63 196 188 163 155 8 M 73 137 137 132 104 9 M 59 135 120 106 106 10 M 60 111 110 100 76 11 F 59 127 126 106 99 12 M 46 88 87 84 80 13 F 67 176 150 156 153 14 F 52 155 135 128 120 15 M 65 142 117 114 97 16 F 75 158 143 145 135 17 F 57 148 131 138 102 18 M 58 125 111 118 124 19 M 48 76 65 94 98 20 M 47 116 108 94 107 21 F 53 191 185 162 113 22 F 73 167 165 162 140 23 M 62 109 104 93 94 24 F 77 167 164 155 155 25 M 55 103 94 75 78 26 F 74 122 126 105 111 27 F 79 203 204 178 145 ; data longform; set cholesterol; array m[4] (0 6 9 24); array c[4] LDL0 LDL6 LDL9 LDL24; do i=1 to 4; month=m[i]; LDL=c[i]; output; end; keep id gender age month LDL; run; ## construct a histogram with fitted bell-shaped curve and conduct normality testing proc univariate data=longform; var LDL; histogram LDL/normal; run; ## fit a random slope and intercept model proc mixed data=covtest: class gender; model LDL=gender age month/solution; random intercept month/subject=id type=un; run; Covariance Parameter Estimates Cov Parm Estimate Pr Z UN(1,1) 520.17 0.0013 UN(2,1) -16.3953 0.0087 UN(2,2) 0.7846 0.0028 Residual 69.8556 &lt;.0001 The outpm= produces prediction of the response \\(y\\) for each row in the data set. If a new prediction is desired, the case must be added to the data set. Specification of type=un, an unstructured type of variance-covariance matrix, requests an estimate of the covariance \\(\\sigma_{u_{1} u_{2}}\\). In the output, the estimators of \\(\\sigma_{u_{1}}^{2}, \\sigma_{u_{1} u_{2}}, \\sigma_{u_{2}}^{2}\\), and \\(\\sigma^{2}\\) are termed UN \\((1,1)\\), UN \\((2,1)\\), UN \\((2,2)\\) and Residual, respectively. 22.2.3 R Implementation 22.2.3.1 Model with one random intercept effekct library(&quot;lme4&quot;) library(&quot;lmerTest&quot;) library(&quot;dplyr&quot;) load(&quot;./01_Datasets/gpa.RData&quot;) ## Standard regression gpa_lm = lm(gpa ~ occasion, data = gpa) summary(gpa_lm) ## ## Call: ## lm(formula = gpa ~ occasion, data = gpa) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.90553 -0.22447 -0.01184 0.26921 1.19447 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.599214 0.017846 145.65 &lt;2e-16 *** ## occasion 0.106314 0.005894 18.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3487 on 1198 degrees of freedom ## Multiple R-squared: 0.2136, Adjusted R-squared: 0.2129 ## F-statistic: 325.3 on 1 and 1198 DF, p-value: &lt; 2.2e-16 ## Run a mixed model, taking into account the specific effects of the students ## (1 | student) means that the intercept allowed to be expressed by 1 varies from student to student gpa_mixed = lmer(gpa ~ occasion + (1 | student), data = gpa) summary(gpa_mixed) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: gpa ~ occasion + (1 | student) ## Data: gpa ## ## REML criterion at convergence: 408.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6169 -0.6373 -0.0004 0.6361 2.8310 ## ## Random effects: ## Groups Name Variance Std.Dev. ## student (Intercept) 0.06372 0.2524 ## Residual 0.05809 0.2410 ## Number of obs: 1200, groups: student, 200 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.599e+00 2.170e-02 3.223e+02 119.8 &lt;2e-16 *** ## occasion 1.063e-01 4.074e-03 9.990e+02 26.1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## occasion -0.469 ## Another way to interpret the variance output is to record the student variance as a percentage of the total variance, which is 0.064 / 0.122 = 52%. ## This is also called intra-class correlation because it is also an estimate of intra-cluster correlation ## Profile confidence intervals confint(gpa_mixed) ## 2.5 % 97.5 % ## .sig01 0.22517423 0.2824604 ## .sigma 0.23071113 0.2518510 ## (Intercept) 2.55665145 2.6417771 ## occasion 0.09832589 0.1143027 ## + ranef(gpa_mixed)$student %&gt;% head(5) ## (Intercept) ## 1 -0.07089492 ## 2 -0.21557844 ## 3 0.08825694 ## 4 -0.18664174 ## 5 0.03038354 ##  coef(gpa_mixed)$student %&gt;% head(5) ## (Intercept) occasion ## 1 2.528319 0.1063143 ## 2 2.383636 0.1063143 ## 3 2.687471 0.1063143 ## 4 2.412573 0.1063143 ## 5 2.629598 0.1063143 ##  library(merTools) predictInterval(gpa_mixed) #  ## fit upr lwr ## 1 2.527754 2.857876 2.198193 ## 2 2.639213 2.970260 2.278418 ## 3 2.739715 3.071613 2.407754 ## 4 2.852531 3.180465 2.507187 ## 5 2.953583 3.298971 2.618523 ## 6 3.069081 3.430316 2.749822 ## 7 2.375838 2.727681 2.042268 ## 8 2.476961 2.815444 2.149444 ## 9 2.619053 2.942406 2.260052 ## 10 2.687695 3.037342 2.357359 ## 11 2.800480 3.161569 2.473903 ## 12 2.916139 3.248582 2.601057 ## 13 2.685334 3.000648 2.356426 ## 14 2.796096 3.143392 2.457484 ## 15 2.903134 3.248235 2.583522 ## 16 3.021372 3.336691 2.671722 ## 17 3.111031 3.418202 2.786394 ## 18 3.217161 3.571694 2.912119 ## 19 2.421293 2.752005 2.072561 ## 20 2.488513 2.839386 2.176470 ## 21 2.629753 2.958613 2.278276 ## 22 2.724190 3.062037 2.400877 ## 23 2.841768 3.165476 2.517686 ## 24 2.935064 3.289298 2.616462 ## 25 2.614031 2.979034 2.281764 ## 26 2.755615 3.074416 2.388426 ## 27 2.851109 3.193582 2.521720 ## 28 2.951506 3.297792 2.617014 ## 29 3.053256 3.400116 2.710043 ## 30 3.163925 3.510425 2.841364 ## 31 2.306957 2.630902 1.966930 ## 32 2.403430 2.740608 2.083655 ## 33 2.498250 2.842373 2.164602 ## 34 2.628994 2.963392 2.301451 ## 35 2.736146 3.046439 2.424816 ## 36 2.824988 3.149365 2.495610 ## 37 2.459204 2.799245 2.129238 ## 38 2.567197 2.921492 2.241064 ## 39 2.666284 3.006860 2.325520 ## 40 2.766381 3.088303 2.462100 ## 41 2.870449 3.217839 2.555785 ## 42 2.988499 3.295001 2.660369 ## 43 2.806811 3.131501 2.487684 ## 44 2.932388 3.267625 2.611971 ## 45 3.047054 3.362839 2.694298 ## 46 3.123750 3.462890 2.797203 ## 47 3.246379 3.596588 2.884200 ## 48 3.340816 3.684805 3.050055 ## 49 2.710598 3.025625 2.366478 ## 50 2.808172 3.143298 2.486872 ## 51 2.910734 3.248965 2.603786 ## 52 3.021426 3.347259 2.700383 ## 53 3.147374 3.467916 2.800576 ## 54 3.224299 3.577147 2.901021 ## 55 2.618933 2.954464 2.277282 ## 56 2.731808 3.055518 2.371236 ## 57 2.849239 3.153823 2.506136 ## 58 2.946823 3.259939 2.611349 ## 59 3.042263 3.384160 2.693157 ## 60 3.129716 3.482543 2.806298 ## 61 2.964111 3.276722 2.635005 ## 62 3.063885 3.387093 2.718508 ## 63 3.159885 3.497304 2.825346 ## 64 3.281133 3.616836 2.942039 ## 65 3.370893 3.716029 3.047183 ## 66 3.488723 3.797634 3.148634 ## 67 2.604323 2.912901 2.282750 ## 68 2.706964 3.045670 2.383009 ## 69 2.814558 3.147877 2.497370 ## 70 2.937702 3.278628 2.603385 ## 71 3.030507 3.377021 2.711071 ## 72 3.128392 3.471986 2.809468 ## 73 3.127671 3.461424 2.796016 ## 74 3.211317 3.526268 2.897016 ## 75 3.332426 3.652186 2.990628 ## 76 3.437823 3.780492 3.104711 ## 77 3.531084 3.883233 3.233059 ## 78 3.660842 3.986129 3.331213 ## 79 2.722509 3.056435 2.401746 ## 80 2.835539 3.155086 2.508210 ## 81 2.907634 3.248317 2.580881 ## 82 3.032837 3.381802 2.685290 ## 83 3.151225 3.485357 2.800926 ## 84 3.246817 3.582219 2.927396 ## 85 2.371039 2.710285 2.040222 ## 86 2.477903 2.803464 2.144704 ## 87 2.588084 2.897886 2.241135 ## 88 2.703960 3.026639 2.381989 ## 89 2.815363 3.145868 2.500979 ## 90 2.919537 3.254633 2.584166 ## 91 2.475125 2.807941 2.120305 ## 92 2.576949 2.893296 2.255483 ## 93 2.677168 3.009114 2.322561 ## 94 2.790845 3.132655 2.490557 ## 95 2.894681 3.241178 2.558215 ## 96 2.995941 3.334691 2.665125 ## 97 2.325008 2.647862 1.992829 ## 98 2.444085 2.777507 2.127254 ## 99 2.541765 2.850755 2.212386 ## 100 2.647904 2.968916 2.305968 ## 101 2.741778 3.100108 2.417935 ## 102 2.865995 3.186061 2.539617 ## 103 2.313432 2.660923 1.946511 ## 104 2.420028 2.731888 2.102959 ## 105 2.503902 2.854953 2.189744 ## 106 2.616295 2.942220 2.278291 ## 107 2.728872 3.057156 2.385875 ## 108 2.829453 3.162840 2.506365 ## 109 2.497552 2.847743 2.178332 ## 110 2.599803 2.905236 2.265323 ## 111 2.721704 3.051463 2.375983 ## 112 2.824162 3.141427 2.479649 ## 113 2.922806 3.253388 2.597794 ## 114 3.040784 3.376165 2.713527 ## 115 2.826389 3.158939 2.511518 ## 116 2.935087 3.251664 2.621068 ## 117 3.024820 3.363589 2.709477 ## 118 3.157350 3.488915 2.826059 ## 119 3.260415 3.607824 2.940139 ## 120 3.358924 3.688452 3.016092 ## 121 2.291987 2.618688 1.967056 ## 122 2.418983 2.753728 2.076272 ## 123 2.498566 2.841135 2.155368 ## 124 2.617599 2.963929 2.301008 ## 125 2.717024 3.076792 2.405720 ## 126 2.824691 3.162107 2.485915 ## 127 2.720057 3.038864 2.380235 ## 128 2.812394 3.156385 2.490469 ## 129 2.929563 3.231966 2.598719 ## 130 3.043837 3.352908 2.715330 ## 131 3.136580 3.485774 2.808653 ## 132 3.260663 3.612915 2.905471 ## 133 2.670514 3.015516 2.321441 ## 134 2.780833 3.106906 2.446933 ## 135 2.874815 3.197116 2.568897 ## 136 2.995840 3.320831 2.656662 ## 137 3.103371 3.445568 2.764480 ## 138 3.192833 3.530884 2.858778 ## 139 2.607640 2.924289 2.278086 ## 140 2.701393 3.048514 2.350056 ## 141 2.823442 3.162809 2.497310 ## 142 2.927037 3.261214 2.573876 ## 143 3.027536 3.357844 2.695059 ## 144 3.149043 3.483435 2.825077 ## 145 3.000812 3.311536 2.660695 ## 146 3.095913 3.426936 2.747282 ## 147 3.199432 3.530873 2.874616 ## 148 3.321934 3.648699 2.976397 ## 149 3.407546 3.723191 3.062163 ## 150 3.526905 3.867551 3.198976 ## 151 2.347605 2.700935 1.993270 ## 152 2.422978 2.755387 2.095129 ## 153 2.541567 2.855422 2.224377 ## 154 2.645050 2.979210 2.316645 ## 155 2.756645 3.072551 2.440053 ## 156 2.850748 3.203253 2.516628 ## 157 2.240574 2.556223 1.900619 ## 158 2.321775 2.645700 2.001927 ## 159 2.429369 2.747130 2.098504 ## 160 2.530068 2.859890 2.190739 ## 161 2.642991 2.980363 2.332513 ## 162 2.744201 3.091139 2.405728 ## 163 2.717168 3.049754 2.378721 ## 164 2.804242 3.150717 2.491932 ## 165 2.931753 3.247756 2.591334 ## 166 3.020219 3.348986 2.695604 ## 167 3.155538 3.463832 2.786947 ## 168 3.246838 3.590704 2.903621 ## 169 2.705783 3.048290 2.353002 ## 170 2.791548 3.130265 2.466190 ## 171 2.899587 3.235101 2.573351 ## 172 3.025374 3.359707 2.667701 ## 173 3.116999 3.454241 2.778771 ## 174 3.214860 3.585504 2.885262 ## 175 2.824638 3.144008 2.509761 ## 176 2.920849 3.264388 2.575099 ## 177 3.037373 3.368942 2.701008 ## 178 3.151665 3.482366 2.833719 ## 179 3.256618 3.578681 2.940395 ## 180 3.355831 3.667381 3.023447 ## 181 2.243842 2.566208 1.919928 ## 182 2.349933 2.671640 2.026574 ## 183 2.444513 2.798922 2.126407 ## 184 2.570736 2.901423 2.239560 ## 185 2.659811 2.968923 2.305186 ## 186 2.767988 3.103739 2.452949 ## 187 2.848599 3.189624 2.527676 ## 188 2.968759 3.294476 2.640245 ## 189 3.080508 3.409171 2.758361 ## 190 3.167299 3.495963 2.841982 ## 191 3.265837 3.622171 2.954363 ## 192 3.383509 3.703553 3.053970 ## 193 2.460682 2.768474 2.116857 ## 194 2.563080 2.904772 2.247757 ## 195 2.675722 3.022757 2.351247 ## 196 2.786301 3.121466 2.461750 ## 197 2.895408 3.219686 2.588620 ## 198 2.994164 3.311039 2.668327 ## 199 2.400502 2.732687 2.063299 ## 200 2.492701 2.825486 2.157920 ## 201 2.604198 2.937542 2.284380 ## 202 2.718941 3.021765 2.380161 ## 203 2.811177 3.149006 2.495133 ## 204 2.926263 3.247263 2.574004 ## 205 2.662640 3.006076 2.348450 ## 206 2.791620 3.138090 2.479453 ## 207 2.890200 3.231116 2.553599 ## 208 3.015876 3.359964 2.695950 ## 209 3.127742 3.444337 2.771312 ## 210 3.209289 3.539797 2.877171 ## 211 2.719371 3.071074 2.388066 ## 212 2.832952 3.169921 2.502336 ## 213 2.941171 3.283024 2.602586 ## 214 3.048286 3.384048 2.711826 ## 215 3.150412 3.473069 2.791002 ## 216 3.247612 3.580381 2.943338 ## 217 2.682412 3.012792 2.346793 ## 218 2.792252 3.130588 2.475064 ## 219 2.883447 3.226891 2.565784 ## 220 3.005163 3.301569 2.678503 ## 221 3.101421 3.429590 2.768139 ## 222 3.206822 3.564984 2.886195 ## 223 3.002600 3.370386 2.689400 ## 224 3.130119 3.460003 2.793364 ## 225 3.234254 3.575833 2.900199 ## 226 3.323754 3.669841 2.994135 ## 227 3.438061 3.767908 3.107905 ## 228 3.571642 3.884939 3.225865 ## 229 2.561336 2.889106 2.201364 ## 230 2.636903 2.983328 2.285544 ## 231 2.745467 3.097439 2.423850 ## 232 2.872199 3.178237 2.549645 ## 233 2.970429 3.308944 2.649886 ## 234 3.077891 3.409042 2.745464 ## 235 2.877630 3.192111 2.548231 ## 236 2.968856 3.303665 2.660994 ## 237 3.089751 3.420766 2.738540 ## 238 3.200924 3.532169 2.858714 ## 239 3.285417 3.645769 2.946483 ## 240 3.414098 3.734781 3.074055 ## 241 3.118130 3.441156 2.785430 ## 242 3.224749 3.570267 2.888980 ## 243 3.338950 3.670107 3.001351 ## 244 3.437717 3.777688 3.100848 ## 245 3.555335 3.878204 3.228912 ## 246 3.657703 4.012317 3.327649 ## 247 2.575573 2.893852 2.226099 ## 248 2.638942 2.975322 2.335357 ## 249 2.776254 3.102098 2.446096 ## 250 2.874365 3.204983 2.540380 ## 251 2.977153 3.306645 2.620465 ## 252 3.088236 3.376919 2.739161 ## 253 2.656301 2.975594 2.336348 ## 254 2.780583 3.115600 2.445526 ## 255 2.897198 3.193192 2.551669 ## 256 2.991432 3.327303 2.663732 ## 257 3.109861 3.442105 2.772581 ## 258 3.214599 3.552295 2.870394 ## 259 2.779638 3.081626 2.444151 ## 260 2.891180 3.228832 2.527748 ## 261 2.977547 3.334129 2.651231 ## 262 3.095332 3.420522 2.749392 ## 263 3.204531 3.548265 2.889470 ## 264 3.301437 3.629639 2.971058 ## 265 2.767826 3.083317 2.467184 ## 266 2.894709 3.215113 2.536835 ## 267 2.990286 3.336113 2.648098 ## 268 3.092810 3.410755 2.765108 ## 269 3.191870 3.525811 2.841357 ## 270 3.292705 3.619645 2.956050 ## 271 2.328151 2.652502 1.971460 ## 272 2.436469 2.751244 2.091271 ## 273 2.536911 2.869715 2.203259 ## 274 2.647028 2.972572 2.302558 ## 275 2.740145 3.072379 2.395088 ## 276 2.847795 3.184647 2.521542 ## 277 2.369750 2.704003 2.034563 ## 278 2.464571 2.784460 2.134333 ## 279 2.576214 2.902181 2.263430 ## 280 2.694300 3.011558 2.364294 ## 281 2.791081 3.132793 2.452624 ## 282 2.915414 3.233771 2.604450 ## 283 2.576477 2.924697 2.234959 ## 284 2.707285 3.035684 2.391884 ## 285 2.818759 3.149259 2.505341 ## 286 2.904400 3.267307 2.580202 ## 287 3.023960 3.341658 2.703333 ## 288 3.119259 3.459556 2.812664 ## 289 2.704179 3.020021 2.394657 ## 290 2.792738 3.112681 2.470245 ## 291 2.922901 3.261304 2.583663 ## 292 3.014730 3.350775 2.693240 ## 293 3.128125 3.448146 2.790173 ## 294 3.233049 3.557534 2.900222 ## 295 2.520732 2.867617 2.179085 ## 296 2.613274 2.957015 2.301726 ## 297 2.756659 3.079242 2.401639 ## 298 2.865023 3.192839 2.532003 ## 299 2.954702 3.277034 2.616133 ## 300 3.074762 3.383416 2.732542 ## 301 2.584374 2.895553 2.225599 ## 302 2.688822 3.009840 2.341841 ## 303 2.778227 3.107035 2.448080 ## 304 2.895692 3.204493 2.567690 ## 305 2.995756 3.324978 2.640225 ## 306 3.115138 3.449471 2.797482 ## 307 2.742778 3.108991 2.399601 ## 308 2.872675 3.197672 2.555249 ## 309 2.961481 3.295943 2.618468 ## 310 3.089827 3.409010 2.746869 ## 311 3.167712 3.525675 2.855717 ## 312 3.274280 3.621101 2.956955 ## 313 2.670502 2.999250 2.353874 ## 314 2.753025 3.101952 2.440805 ## 315 2.865395 3.182393 2.537016 ## 316 2.995367 3.313926 2.659789 ## 317 3.099627 3.434838 2.738139 ## 318 3.203444 3.537616 2.865925 ## 319 2.777411 3.119378 2.454432 ## 320 2.899592 3.229224 2.562974 ## 321 2.991739 3.335352 2.659639 ## 322 3.114248 3.440150 2.807173 ## 323 3.220901 3.529477 2.907855 ## 324 3.332648 3.649840 2.995391 ## 325 2.734486 3.063385 2.401242 ## 326 2.844221 3.172860 2.504266 ## 327 2.949559 3.306445 2.617633 ## 328 3.066344 3.387752 2.744585 ## 329 3.163389 3.505269 2.839436 ## 330 3.248263 3.595948 2.933968 ## 331 2.352067 2.667606 2.009743 ## 332 2.415847 2.760057 2.101469 ## 333 2.545861 2.888430 2.223251 ## 334 2.662137 2.984836 2.328427 ## 335 2.764408 3.097996 2.438190 ## 336 2.878292 3.201837 2.551211 ## 337 2.678801 3.017736 2.327947 ## 338 2.786451 3.102979 2.461298 ## 339 2.884047 3.203311 2.534315 ## 340 2.989949 3.319622 2.670714 ## 341 3.097223 3.433131 2.744119 ## 342 3.209531 3.505245 2.894111 ## 343 2.416600 2.752014 2.070658 ## 344 2.508489 2.843547 2.172496 ## 345 2.617971 2.935313 2.284461 ## 346 2.709132 3.038763 2.362133 ## 347 2.843578 3.178032 2.518252 ## 348 2.957380 3.261109 2.605261 ## 349 2.649634 2.990358 2.317841 ## 350 2.771921 3.126802 2.429524 ## 351 2.871465 3.210552 2.553279 ## 352 2.984462 3.329833 2.679800 ## 353 3.069199 3.435641 2.742578 ## 354 3.182544 3.523159 2.870216 ## 355 2.451547 2.764413 2.109378 ## 356 2.556288 2.907216 2.246215 ## 357 2.663082 2.991810 2.315470 ## 358 2.764745 3.094356 2.439846 ## 359 2.868315 3.185371 2.543079 ## 360 2.957057 3.280340 2.635880 ## 361 2.866512 3.201257 2.532794 ## 362 2.947325 3.297118 2.610874 ## 363 3.071701 3.404781 2.757353 ## 364 3.188130 3.509208 2.853775 ## 365 3.288113 3.634513 2.958147 ## 366 3.367083 3.705036 3.048711 ## 367 2.487597 2.831525 2.149255 ## 368 2.589124 2.904528 2.273488 ## 369 2.688403 3.036382 2.349504 ## 370 2.800673 3.141962 2.457914 ## 371 2.912683 3.245108 2.570044 ## 372 2.999087 3.342850 2.674618 ## 373 2.864925 3.183518 2.540715 ## 374 2.988006 3.286052 2.632406 ## 375 3.062869 3.423200 2.724223 ## 376 3.192526 3.516575 2.842904 ## 377 3.298402 3.631475 2.981449 ## 378 3.395935 3.735560 3.069216 ## 379 2.697839 3.029769 2.383301 ## 380 2.785814 3.124968 2.464610 ## 381 2.908135 3.227234 2.548072 ## 382 3.027371 3.375514 2.694050 ## 383 3.128908 3.443826 2.790042 ## 384 3.237541 3.572203 2.896529 ## 385 2.352459 2.666698 2.024111 ## 386 2.467240 2.789553 2.135302 ## 387 2.564633 2.880742 2.233130 ## 388 2.641843 2.998155 2.339916 ## 389 2.765093 3.118585 2.457455 ## 390 2.893050 3.227030 2.567195 ## 391 2.805898 3.158873 2.477551 ## 392 2.906984 3.255328 2.597385 ## 393 3.007373 3.342012 2.698609 ## 394 3.123212 3.438241 2.773928 ## 395 3.238465 3.524844 2.904241 ## 396 3.330038 3.689264 3.001790 ## 397 2.378421 2.723480 2.049871 ## 398 2.519213 2.804583 2.149362 ## 399 2.596734 2.938262 2.258553 ## 400 2.698026 3.053603 2.374360 ## 401 2.816872 3.144443 2.484216 ## 402 2.924919 3.234589 2.583710 ## 403 2.313045 2.665182 1.979074 ## 404 2.432413 2.751021 2.080772 ## 405 2.520970 2.848897 2.198899 ## 406 2.639842 2.985082 2.303728 ## 407 2.728491 3.073630 2.397615 ## 408 2.850486 3.182686 2.526808 ## 409 2.310586 2.642637 1.990040 ## 410 2.434118 2.760132 2.085409 ## 411 2.518739 2.864027 2.201204 ## 412 2.658263 2.971317 2.333442 ## 413 2.745308 3.088105 2.433891 ## 414 2.839695 3.176758 2.506160 ## 415 2.627939 2.963281 2.265684 ## 416 2.730902 3.064907 2.384336 ## 417 2.835667 3.173682 2.506307 ## 418 2.929859 3.269990 2.609301 ## 419 3.053648 3.379108 2.725438 ## 420 3.163087 3.472773 2.847374 ## 421 2.598123 2.920858 2.271866 ## 422 2.718415 3.043342 2.378408 ## 423 2.801019 3.128009 2.469554 ## 424 2.921101 3.260388 2.584465 ## 425 3.005598 3.363306 2.658824 ## 426 3.114051 3.435310 2.770410 ## 427 3.020217 3.345422 2.683684 ## 428 3.145616 3.445459 2.812105 ## 429 3.261995 3.589423 2.921085 ## 430 3.344003 3.684879 2.998228 ## 431 3.434630 3.767102 3.094869 ## 432 3.566441 3.914176 3.238335 ## 433 2.380182 2.693604 2.053084 ## 434 2.491389 2.810779 2.151154 ## 435 2.592975 2.926297 2.251582 ## 436 2.705194 3.049485 2.380399 ## 437 2.826567 3.144489 2.463694 ## 438 2.932363 3.257240 2.594630 ## 439 2.822803 3.144361 2.484501 ## 440 2.933459 3.259056 2.579918 ## 441 3.010982 3.364720 2.678484 ## 442 3.130856 3.482539 2.797111 ## 443 3.239669 3.580262 2.919171 ## 444 3.351782 3.678569 3.005905 ## 445 2.627611 2.981408 2.305094 ## 446 2.724927 3.050981 2.398422 ## 447 2.843666 3.162341 2.514299 ## 448 2.944242 3.290592 2.605162 ## 449 3.061137 3.400572 2.720231 ## 450 3.166432 3.477619 2.838091 ## 451 2.749173 3.081415 2.440215 ## 452 2.879463 3.233602 2.559793 ## 453 2.966524 3.294725 2.650635 ## 454 3.081444 3.392054 2.757495 ## 455 3.179997 3.504832 2.860046 ## 456 3.296556 3.635248 2.956054 ## 457 3.075996 3.402890 2.749461 ## 458 3.186143 3.539360 2.855632 ## 459 3.307209 3.624891 2.956972 ## 460 3.403493 3.748951 3.070890 ## 461 3.513824 3.860404 3.168381 ## 462 3.627294 3.940950 3.294604 ## 463 2.225768 2.553905 1.882312 ## 464 2.338652 2.673675 2.015840 ## 465 2.444499 2.778776 2.128319 ## 466 2.549025 2.868587 2.216308 ## 467 2.657152 2.983418 2.334925 ## 468 2.741243 3.084653 2.430854 ## 469 2.113575 2.436343 1.776314 ## 470 2.247753 2.565075 1.889704 ## 471 2.333982 2.647755 2.007510 ## 472 2.413004 2.751965 2.108943 ## 473 2.544650 2.864695 2.215298 ## 474 2.662223 3.018096 2.311800 ## 475 2.825235 3.135765 2.473514 ## 476 2.915418 3.271730 2.573349 ## 477 3.018670 3.337547 2.691603 ## 478 3.115171 3.466190 2.792583 ## 479 3.247289 3.568640 2.904557 ## 480 3.334327 3.665053 3.017181 ## 481 2.628571 2.969827 2.313985 ## 482 2.746437 3.097078 2.414476 ## 483 2.852983 3.166895 2.540326 ## 484 2.966616 3.272008 2.627453 ## 485 3.093919 3.412738 2.763549 ## 486 3.177911 3.495884 2.833215 ## 487 2.894408 3.215186 2.572349 ## 488 3.008023 3.358074 2.690326 ## 489 3.120927 3.475717 2.806967 ## 490 3.225505 3.566519 2.912290 ## 491 3.317163 3.633148 3.005123 ## 492 3.425127 3.762758 3.102260 ## 493 2.839331 3.173402 2.515761 ## 494 2.950445 3.301825 2.637482 ## 495 3.050818 3.380041 2.747600 ## 496 3.147641 3.491071 2.835704 ## 497 3.269013 3.605240 2.947235 ## 498 3.388111 3.700326 3.059772 ## 499 2.087903 2.396981 1.738388 ## 500 2.162460 2.494660 1.837784 ## 501 2.278977 2.593720 1.949184 ## 502 2.368749 2.714843 2.048026 ## 503 2.483595 2.829176 2.144139 ## 504 2.589506 2.930494 2.261926 ## 505 2.695731 3.032136 2.366774 ## 506 2.818018 3.150658 2.499768 ## 507 2.931497 3.256116 2.590792 ## 508 3.029541 3.363269 2.694901 ## 509 3.147577 3.444344 2.812728 ## 510 3.234087 3.584929 2.900299 ## 511 2.406550 2.734260 2.092624 ## 512 2.528348 2.858306 2.181425 ## 513 2.627728 2.945598 2.312278 ## 514 2.734132 3.036714 2.402914 ## 515 2.833841 3.145507 2.505320 ## 516 2.925789 3.286504 2.586835 ## 517 2.605581 2.922894 2.269618 ## 518 2.708907 3.014684 2.358610 ## 519 2.809262 3.147816 2.442155 ## 520 2.909127 3.236210 2.594390 ## 521 3.040086 3.356152 2.676177 ## 522 3.129957 3.466131 2.809969 ## 523 2.541006 2.880349 2.233509 ## 524 2.641562 2.981334 2.296090 ## 525 2.761497 3.082490 2.432357 ## 526 2.865675 3.184517 2.542131 ## 527 2.963465 3.303213 2.654962 ## 528 3.078733 3.413422 2.736084 ## 529 2.747252 3.095284 2.424612 ## 530 2.858554 3.169944 2.535108 ## 531 2.959633 3.295675 2.624439 ## 532 3.054857 3.392894 2.702467 ## 533 3.176279 3.505042 2.866945 ## 534 3.273172 3.607648 2.944439 ## 535 2.544927 2.879892 2.211738 ## 536 2.663821 2.998376 2.304805 ## 537 2.758088 3.109703 2.441795 ## 538 2.859670 3.176079 2.536217 ## 539 2.964672 3.312524 2.641196 ## 540 3.100469 3.450809 2.744959 ## 541 2.284021 2.628511 1.945723 ## 542 2.375717 2.707283 2.094686 ## 543 2.512347 2.832141 2.180254 ## 544 2.615426 2.927511 2.277732 ## 545 2.704666 3.052759 2.376825 ## 546 2.819277 3.178275 2.507102 ## 547 3.252919 3.579794 2.922099 ## 548 3.369170 3.695469 3.035530 ## 549 3.469758 3.796323 3.164170 ## 550 3.585924 3.915086 3.234853 ## 551 3.655497 3.982309 3.338905 ## 552 3.783274 4.114812 3.433133 ## 553 2.956820 3.293355 2.611108 ## 554 3.064727 3.403478 2.749846 ## 555 3.167952 3.480319 2.830854 ## 556 3.272331 3.585647 2.946612 ## 557 3.389995 3.720360 3.068138 ## 558 3.498270 3.849978 3.169246 ## 559 2.538603 2.860294 2.193391 ## 560 2.671496 2.957320 2.326153 ## 561 2.770396 3.092172 2.428176 ## 562 2.869601 3.202642 2.539983 ## 563 2.986354 3.310065 2.641220 ## 564 3.063848 3.399646 2.746165 ## 565 2.491474 2.781171 2.158940 ## 566 2.568886 2.914006 2.232448 ## 567 2.685428 3.019454 2.356336 ## 568 2.804710 3.115085 2.488706 ## 569 2.908657 3.229376 2.572195 ## 570 2.989905 3.336016 2.674809 ## 571 2.744022 3.065526 2.382362 ## 572 2.831003 3.185334 2.507327 ## 573 2.950871 3.292005 2.615092 ## 574 3.051229 3.392465 2.725843 ## 575 3.170681 3.506235 2.861201 ## 576 3.295286 3.632103 2.938583 ## 577 2.544720 2.873989 2.221997 ## 578 2.652264 2.976093 2.318826 ## 579 2.765661 3.053566 2.453990 ## 580 2.860159 3.186193 2.515921 ## 581 2.975017 3.284325 2.610158 ## 582 3.057187 3.401650 2.752117 ## 583 2.790032 3.130810 2.469420 ## 584 2.882939 3.215438 2.568131 ## 585 3.007325 3.326740 2.694327 ## 586 3.094777 3.414022 2.792668 ## 587 3.217684 3.539212 2.881264 ## 588 3.315604 3.640814 2.984733 ## 589 2.426042 2.771661 2.095672 ## 590 2.526818 2.874588 2.202518 ## 591 2.641491 2.968166 2.297417 ## 592 2.742530 3.101486 2.444880 ## 593 2.856637 3.193454 2.551433 ## 594 2.959395 3.290135 2.631528 ## 595 2.163002 2.474281 1.826467 ## 596 2.264280 2.600666 1.934729 ## 597 2.375484 2.692906 2.044250 ## 598 2.475539 2.799733 2.120901 ## 599 2.573074 2.923178 2.253069 ## 600 2.695740 3.012051 2.366353 ## 601 2.949144 3.247669 2.603036 ## 602 3.030920 3.386649 2.711966 ## 603 3.156247 3.500325 2.824125 ## 604 3.252666 3.573616 2.927235 ## 605 3.355745 3.699985 3.032681 ## 606 3.441701 3.793620 3.136329 ## 607 2.566284 2.879379 2.258601 ## 608 2.664728 2.982904 2.348682 ## 609 2.789259 3.114557 2.429958 ## 610 2.859198 3.194627 2.520439 ## 611 3.008007 3.332928 2.660265 ## 612 3.076479 3.413984 2.740997 ## 613 2.461195 2.802507 2.125363 ## 614 2.558500 2.900265 2.215163 ## 615 2.681636 2.995832 2.317278 ## 616 2.765622 3.102195 2.425817 ## 617 2.890012 3.231366 2.551351 ## 618 2.992397 3.312989 2.632714 ## 619 2.692678 3.025072 2.348275 ## 620 2.786231 3.124604 2.434409 ## 621 2.902153 3.227330 2.578387 ## 622 2.996921 3.356431 2.661698 ## 623 3.125946 3.454403 2.784596 ## 624 3.207854 3.530694 2.876304 ## 625 3.376673 3.719326 3.041406 ## 626 3.461090 3.810042 3.140659 ## 627 3.585948 3.923478 3.249209 ## 628 3.690565 4.033652 3.358373 ## 629 3.786201 4.114271 3.460632 ## 630 3.908415 4.207482 3.564481 ## 631 2.230628 2.568730 1.888627 ## 632 2.356521 2.667621 2.020693 ## 633 2.450961 2.785072 2.123171 ## 634 2.535782 2.871247 2.212414 ## 635 2.665591 3.010800 2.349123 ## 636 2.780494 3.121321 2.448960 ## 637 2.250316 2.570562 1.907612 ## 638 2.376387 2.711519 2.035819 ## 639 2.479494 2.795346 2.124306 ## 640 2.573112 2.897748 2.225407 ## 641 2.691526 3.034120 2.356205 ## 642 2.779382 3.090907 2.451921 ## 643 2.193765 2.559790 1.889387 ## 644 2.302802 2.641450 1.967579 ## 645 2.425211 2.728905 2.080891 ## 646 2.525776 2.871178 2.183929 ## 647 2.644037 2.990363 2.306896 ## 648 2.738635 3.083879 2.412769 ## 649 2.516339 2.843725 2.184086 ## 650 2.643139 2.970743 2.333892 ## 651 2.738278 3.089457 2.404278 ## 652 2.849663 3.197203 2.511055 ## 653 2.938828 3.282579 2.630412 ## 654 3.049234 3.374612 2.701190 ## 655 2.101220 2.432597 1.763007 ## 656 2.200841 2.533448 1.881674 ## 657 2.307326 2.620418 1.978987 ## 658 2.416335 2.737913 2.089397 ## 659 2.505433 2.843100 2.186757 ## 660 2.633505 2.968540 2.299624 ## 661 2.159332 2.489706 1.821964 ## 662 2.273478 2.598556 1.902466 ## 663 2.398528 2.708810 2.046534 ## 664 2.485916 2.813767 2.151925 ## 665 2.593280 2.908106 2.285908 ## 666 2.697079 2.989704 2.345663 ## 667 2.810239 3.154804 2.487147 ## 668 2.927233 3.239577 2.597895 ## 669 3.036761 3.370160 2.706775 ## 670 3.153442 3.481120 2.804737 ## 671 3.258906 3.592512 2.930966 ## 672 3.349924 3.681246 3.028541 ## 673 2.431693 2.765575 2.077197 ## 674 2.529228 2.851678 2.198632 ## 675 2.650256 2.979649 2.315989 ## 676 2.744092 3.055433 2.418815 ## 677 2.849837 3.186600 2.542210 ## 678 2.937209 3.281369 2.639149 ## 679 2.552630 2.855504 2.233648 ## 680 2.664998 2.977309 2.326864 ## 681 2.768211 3.098316 2.449623 ## 682 2.871471 3.205729 2.539022 ## 683 2.974391 3.311437 2.626174 ## 684 3.079971 3.398557 2.764157 ## 685 2.518177 2.827620 2.195937 ## 686 2.607186 2.934368 2.261225 ## 687 2.734657 3.050827 2.407219 ## 688 2.836666 3.177932 2.519709 ## 689 2.940944 3.290918 2.617161 ## 690 3.045365 3.382213 2.708491 ## 691 2.467950 2.816789 2.138149 ## 692 2.580839 2.927942 2.239589 ## 693 2.700290 3.027681 2.342747 ## 694 2.795738 3.118832 2.450316 ## 695 2.891275 3.218537 2.561900 ## 696 3.011040 3.343668 2.672922 ## 697 2.590948 2.925650 2.256741 ## 698 2.699001 3.014792 2.373888 ## 699 2.808318 3.133834 2.473746 ## 700 2.905767 3.225600 2.567508 ## 701 3.007873 3.347820 2.652896 ## 702 3.125370 3.454850 2.780964 ## 703 2.725293 3.085622 2.409273 ## 704 2.825738 3.149892 2.523765 ## 705 2.928611 3.277432 2.601959 ## 706 3.056558 3.383460 2.748061 ## 707 3.166556 3.491425 2.840377 ## 708 3.275274 3.591054 2.940174 ## 709 2.390695 2.714301 2.043778 ## 710 2.490805 2.826823 2.177239 ## 711 2.612440 2.930871 2.273346 ## 712 2.725817 3.050760 2.374768 ## 713 2.799216 3.147511 2.447880 ## 714 2.900429 3.277029 2.595949 ## 715 2.714553 3.041734 2.396743 ## 716 2.837736 3.148403 2.475991 ## 717 2.918837 3.275329 2.602915 ## 718 3.028940 3.359409 2.700217 ## 719 3.142650 3.492079 2.824061 ## 720 3.251820 3.578380 2.922828 ## 721 2.698727 3.038773 2.349155 ## 722 2.807192 3.112258 2.458902 ## 723 2.906335 3.243348 2.578361 ## 724 3.006653 3.314079 2.692924 ## 725 3.109469 3.450060 2.783207 ## 726 3.228976 3.533362 2.912477 ## 727 2.849999 3.191315 2.525477 ## 728 2.957343 3.293361 2.639934 ## 729 3.070936 3.416304 2.713482 ## 730 3.176325 3.494022 2.848254 ## 731 3.272585 3.603231 2.935222 ## 732 3.379025 3.735945 3.054280 ## 733 2.204841 2.548144 1.887479 ## 734 2.321131 2.647822 2.005209 ## 735 2.439808 2.791204 2.107899 ## 736 2.554640 2.886324 2.228074 ## 737 2.652959 3.001097 2.313408 ## 738 2.752510 3.072933 2.447146 ## 739 2.687835 3.032354 2.348636 ## 740 2.790350 3.119118 2.458948 ## 741 2.904158 3.240604 2.580971 ## 742 2.987020 3.338003 2.668791 ## 743 3.118574 3.420270 2.799002 ## 744 3.230201 3.568366 2.891676 ## 745 2.380759 2.713042 2.078624 ## 746 2.483407 2.825208 2.159863 ## 747 2.599520 2.904627 2.250121 ## 748 2.723141 3.044015 2.365971 ## 749 2.823895 3.155239 2.494300 ## 750 2.925154 3.247195 2.588540 ## 751 2.476709 2.807317 2.156082 ## 752 2.578625 2.927560 2.236118 ## 753 2.702669 3.034908 2.375187 ## 754 2.795686 3.135891 2.478786 ## 755 2.918476 3.246184 2.573591 ## 756 3.032600 3.336741 2.670795 ## 757 2.267948 2.588357 1.910460 ## 758 2.370647 2.679609 2.034063 ## 759 2.462158 2.797734 2.142826 ## 760 2.571180 2.891467 2.234134 ## 761 2.675549 3.015400 2.361047 ## 762 2.781799 3.111450 2.471218 ## 763 2.530127 2.855224 2.183549 ## 764 2.644301 2.962448 2.293289 ## 765 2.739225 3.065584 2.390203 ## 766 2.845855 3.174555 2.512206 ## 767 2.962739 3.297010 2.624559 ## 768 3.058727 3.379772 2.709635 ## 769 2.514083 2.855692 2.166987 ## 770 2.645155 2.980227 2.323271 ## 771 2.735718 3.060516 2.373786 ## 772 2.850765 3.181426 2.529265 ## 773 2.948503 3.293772 2.599105 ## 774 3.058884 3.390647 2.710149 ## 775 2.743266 3.075934 2.390471 ## 776 2.857034 3.181013 2.529494 ## 777 2.951969 3.279453 2.613947 ## 778 3.051773 3.381819 2.724026 ## 779 3.154120 3.474131 2.844685 ## 780 3.279011 3.594178 2.952569 ## 781 2.909536 3.236989 2.570680 ## 782 3.002879 3.335019 2.687923 ## 783 3.101481 3.454115 2.789760 ## 784 3.217915 3.563855 2.868135 ## 785 3.324038 3.641511 2.977379 ## 786 3.412846 3.747244 3.094733 ## 787 2.760523 3.077562 2.441881 ## 788 2.847730 3.200882 2.531240 ## 789 2.976638 3.292081 2.666298 ## 790 3.073735 3.390561 2.753108 ## 791 3.176508 3.511530 2.855527 ## 792 3.276902 3.639522 2.972980 ## 793 2.207875 2.546266 1.863658 ## 794 2.298911 2.659747 1.982496 ## 795 2.431076 2.745597 2.085676 ## 796 2.545126 2.867413 2.182305 ## 797 2.632257 2.980040 2.346414 ## 798 2.756868 3.081195 2.407971 ## 799 2.609171 2.960384 2.243999 ## 800 2.725719 3.043345 2.407737 ## 801 2.829016 3.147570 2.500689 ## 802 2.927396 3.255863 2.593372 ## 803 3.052145 3.416437 2.707311 ## 804 3.150416 3.487217 2.810499 ## 805 2.704058 3.041390 2.357940 ## 806 2.785507 3.129420 2.455793 ## 807 2.893781 3.215535 2.557688 ## 808 3.020962 3.358801 2.666461 ## 809 3.130632 3.459946 2.805328 ## 810 3.203249 3.531962 2.878336 ## 811 2.431501 2.754258 2.107318 ## 812 2.538908 2.862330 2.213797 ## 813 2.642833 2.951562 2.298973 ## 814 2.738703 3.082698 2.405907 ## 815 2.865514 3.186012 2.512873 ## 816 2.966722 3.296057 2.615398 ## 817 2.500198 2.812262 2.175833 ## 818 2.599968 2.925840 2.276429 ## 819 2.713573 3.053804 2.370941 ## 820 2.806323 3.154691 2.487773 ## 821 2.924476 3.255981 2.581230 ## 822 3.031384 3.360088 2.682462 ## 823 2.524032 2.880093 2.193271 ## 824 2.634737 2.986050 2.314167 ## 825 2.748732 3.087533 2.430210 ## 826 2.840897 3.203838 2.515487 ## 827 2.964592 3.282778 2.627500 ## 828 3.073705 3.397224 2.715111 ## 829 2.797815 3.113206 2.501500 ## 830 2.924186 3.256105 2.563489 ## 831 3.022897 3.347593 2.693876 ## 832 3.110762 3.454262 2.780985 ## 833 3.241847 3.546538 2.878220 ## 834 3.330022 3.678482 2.978680 ## 835 2.481272 2.801110 2.159017 ## 836 2.572207 2.898184 2.231641 ## 837 2.676689 3.015175 2.346379 ## 838 2.790899 3.129864 2.466104 ## 839 2.895474 3.215597 2.579298 ## 840 2.994103 3.328976 2.681420 ## 841 2.806650 3.114911 2.482869 ## 842 2.890399 3.221025 2.576057 ## 843 3.018645 3.354396 2.673299 ## 844 3.091312 3.426884 2.746750 ## 845 3.209563 3.543491 2.881474 ## 846 3.314743 3.630502 2.975832 ## 847 2.676528 3.002880 2.326088 ## 848 2.803493 3.126937 2.454865 ## 849 2.897467 3.230887 2.563490 ## 850 3.012998 3.324360 2.661203 ## 851 3.105989 3.437487 2.786948 ## 852 3.202121 3.522626 2.859482 ## 853 2.554705 2.877260 2.239641 ## 854 2.658469 3.013735 2.328954 ## 855 2.770713 3.106711 2.440100 ## 856 2.876393 3.213808 2.538867 ## 857 2.981635 3.334302 2.640099 ## 858 3.095009 3.418442 2.759212 ## 859 2.698743 3.005368 2.357600 ## 860 2.806162 3.124653 2.468363 ## 861 2.894807 3.228788 2.583917 ## 862 3.014464 3.343931 2.667533 ## 863 3.103541 3.434607 2.773080 ## 864 3.211063 3.521258 2.909704 ## 865 2.550850 2.894511 2.222608 ## 866 2.618125 2.979921 2.323611 ## 867 2.773352 3.098722 2.413126 ## 868 2.845219 3.182204 2.517272 ## 869 2.961451 3.282730 2.619453 ## 870 3.077661 3.399373 2.731736 ## 871 2.304752 2.637050 1.987310 ## 872 2.407255 2.759391 2.079215 ## 873 2.536576 2.873164 2.195317 ## 874 2.634826 2.965805 2.302744 ## 875 2.736935 3.060644 2.424731 ## 876 2.845858 3.147726 2.528364 ## 877 2.984851 3.301574 2.641815 ## 878 3.085591 3.419230 2.762155 ## 879 3.205037 3.539638 2.860130 ## 880 3.295835 3.644221 2.977756 ## 881 3.421387 3.722569 3.101593 ## 882 3.513736 3.845119 3.154608 ## 883 2.457851 2.797494 2.124565 ## 884 2.569306 2.903093 2.213999 ## 885 2.654524 2.983617 2.327316 ## 886 2.778405 3.119374 2.459505 ## 887 2.880001 3.232457 2.535033 ## 888 2.987413 3.324002 2.665782 ## 889 2.557393 2.871985 2.211605 ## 890 2.655421 2.980491 2.316340 ## 891 2.751693 3.104800 2.418131 ## 892 2.876982 3.190000 2.543804 ## 893 2.963931 3.284704 2.616253 ## 894 3.049142 3.411332 2.730625 ## 895 2.359540 2.679760 2.052616 ## 896 2.463318 2.810417 2.127374 ## 897 2.579201 2.915841 2.249912 ## 898 2.689648 3.023535 2.347545 ## 899 2.794152 3.131606 2.466257 ## 900 2.909023 3.222591 2.593391 ## 901 2.414142 2.738121 2.079356 ## 902 2.528934 2.852945 2.193752 ## 903 2.620757 2.965562 2.276486 ## 904 2.713752 3.046136 2.396897 ## 905 2.844611 3.171139 2.513329 ## 906 2.933029 3.269650 2.606350 ## 907 2.489182 2.815081 2.146748 ## 908 2.608001 2.932584 2.259917 ## 909 2.676677 3.000366 2.338098 ## 910 2.801420 3.163263 2.484116 ## 911 2.928111 3.237319 2.580077 ## 912 3.022610 3.374495 2.689323 ## 913 2.580990 2.912845 2.218849 ## 914 2.698641 3.020322 2.362388 ## 915 2.797377 3.129517 2.461201 ## 916 2.895524 3.207261 2.552382 ## 917 2.987928 3.338445 2.649527 ## 918 3.107360 3.435810 2.773141 ## 919 2.820514 3.133012 2.494572 ## 920 2.944978 3.271942 2.606336 ## 921 3.031934 3.362055 2.685019 ## 922 3.143202 3.470287 2.807579 ## 923 3.239904 3.573876 2.898313 ## 924 3.341819 3.651545 3.023157 ## 925 2.500718 2.841054 2.163899 ## 926 2.626777 2.973548 2.293913 ## 927 2.725510 3.070548 2.389513 ## 928 2.832030 3.170032 2.503078 ## 929 2.949880 3.267562 2.626472 ## 930 3.051806 3.389421 2.709396 ## 931 2.536314 2.842705 2.187521 ## 932 2.609816 2.944149 2.292709 ## 933 2.712208 3.042579 2.384943 ## 934 2.831777 3.174845 2.482795 ## 935 2.944753 3.305811 2.592842 ## 936 3.042087 3.370832 2.698595 ## 937 2.935573 3.284965 2.619623 ## 938 3.042052 3.388058 2.712283 ## 939 3.140068 3.495692 2.798168 ## 940 3.259421 3.600574 2.930762 ## 941 3.364452 3.702051 3.048098 ## 942 3.460395 3.831834 3.125676 ## 943 2.578469 2.896421 2.254756 ## 944 2.677663 3.008948 2.349136 ## 945 2.774514 3.122944 2.431808 ## 946 2.865006 3.210566 2.552995 ## 947 3.004226 3.334857 2.659744 ## 948 3.092788 3.447728 2.773377 ## 949 3.053537 3.381712 2.715190 ## 950 3.140380 3.465135 2.821040 ## 951 3.243314 3.587178 2.909750 ## 952 3.363331 3.680105 3.037583 ## 953 3.472748 3.787488 3.155607 ## 954 3.573952 3.908714 3.221341 ## 955 2.940290 3.282165 2.606824 ## 956 3.068780 3.389158 2.729011 ## 957 3.164983 3.515492 2.861357 ## 958 3.286650 3.633128 2.948683 ## 959 3.362050 3.686559 3.055579 ## 960 3.506166 3.853145 3.194531 ## 961 2.482405 2.816176 2.165522 ## 962 2.590373 2.916925 2.257410 ## 963 2.687588 3.026764 2.345197 ## 964 2.797130 3.169042 2.497502 ## 965 2.910051 3.251182 2.567965 ## 966 3.024134 3.356395 2.705989 ## 967 2.170114 2.508377 1.845753 ## 968 2.253935 2.581041 1.929258 ## 969 2.378151 2.693313 2.069494 ## 970 2.478900 2.825059 2.148533 ## 971 2.612400 2.948262 2.256526 ## 972 2.697234 3.025191 2.367202 ## 973 2.565247 2.883833 2.230726 ## 974 2.665073 2.982186 2.320439 ## 975 2.763162 3.086121 2.442596 ## 976 2.874937 3.233973 2.546802 ## 977 2.974694 3.302261 2.636747 ## 978 3.096110 3.419001 2.773872 ## 979 2.483670 2.796639 2.164499 ## 980 2.576094 2.924617 2.272746 ## 981 2.682014 3.035361 2.367187 ## 982 2.797231 3.108339 2.505145 ## 983 2.894027 3.234578 2.581363 ## 984 3.004537 3.318311 2.653560 ## 985 2.598941 2.898235 2.264254 ## 986 2.704780 3.023216 2.353987 ## 987 2.810019 3.148736 2.478878 ## 988 2.906906 3.244334 2.559056 ## 989 2.998338 3.329844 2.688946 ## 990 3.120288 3.482165 2.800262 ## 991 2.790166 3.120550 2.453198 ## 992 2.900048 3.228771 2.574753 ## 993 3.000854 3.325006 2.694500 ## 994 3.116843 3.421167 2.796999 ## 995 3.208467 3.536370 2.869111 ## 996 3.317559 3.677565 2.985719 ## 997 2.793886 3.145976 2.460322 ## 998 2.918636 3.250720 2.610023 ## 999 3.013185 3.339499 2.681278 ## 1000 3.126517 3.449407 2.782521 ## 1001 3.238891 3.566416 2.902921 ## 1002 3.339337 3.662695 2.987180 ## 1003 2.851741 3.165590 2.510512 ## 1004 2.965582 3.310183 2.654299 ## 1005 3.060333 3.393199 2.715407 ## 1006 3.181139 3.502902 2.841887 ## 1007 3.283934 3.622846 2.945391 ## 1008 3.386678 3.744595 3.060740 ## 1009 2.854383 3.180069 2.517233 ## 1010 2.961140 3.271155 2.644787 ## 1011 3.072497 3.400609 2.723428 ## 1012 3.177563 3.508035 2.841606 ## 1013 3.276508 3.623986 2.933681 ## 1014 3.374317 3.700466 3.047216 ## 1015 3.011980 3.338765 2.677497 ## 1016 3.098753 3.430586 2.755649 ## 1017 3.215548 3.520896 2.885764 ## 1018 3.310237 3.664160 2.958563 ## 1019 3.427291 3.767544 3.096386 ## 1020 3.507545 3.824158 3.183714 ## 1021 2.303521 2.637917 1.973795 ## 1022 2.417990 2.728403 2.072965 ## 1023 2.509325 2.859656 2.176974 ## 1024 2.635182 2.956177 2.313405 ## 1025 2.729450 3.049498 2.428713 ## 1026 2.841465 3.191182 2.521930 ## 1027 2.550715 2.903097 2.213055 ## 1028 2.652202 2.972340 2.319981 ## 1029 2.769907 3.099345 2.430935 ## 1030 2.877861 3.194519 2.554430 ## 1031 2.971403 3.297322 2.653863 ## 1032 3.082051 3.414573 2.756227 ## 1033 2.883036 3.217025 2.532029 ## 1034 2.972687 3.291472 2.650091 ## 1035 3.078914 3.412960 2.741261 ## 1036 3.191228 3.527685 2.864733 ## 1037 3.303846 3.616381 2.963655 ## 1038 3.395923 3.729683 3.040014 ## 1039 2.714564 3.022980 2.384464 ## 1040 2.820195 3.172958 2.499051 ## 1041 2.949817 3.272255 2.581150 ## 1042 3.051451 3.384078 2.711520 ## 1043 3.144392 3.488404 2.797668 ## 1044 3.266970 3.563736 2.949648 ## 1045 2.682114 3.028159 2.355079 ## 1046 2.795767 3.098714 2.467615 ## 1047 2.889401 3.214031 2.544914 ## 1048 2.996043 3.287199 2.675195 ## 1049 3.110018 3.436166 2.784676 ## 1050 3.219419 3.546510 2.877043 ## 1051 2.518366 2.853359 2.184541 ## 1052 2.626057 2.968518 2.295022 ## 1053 2.742343 3.062080 2.391599 ## 1054 2.829152 3.165662 2.500603 ## 1055 2.921257 3.251555 2.586252 ## 1056 3.048405 3.341612 2.705631 ## 1057 2.790660 3.118794 2.467856 ## 1058 2.890020 3.228993 2.555680 ## 1059 3.018295 3.339058 2.658598 ## 1060 3.117179 3.463109 2.790261 ## 1061 3.216593 3.525086 2.883254 ## 1062 3.327645 3.637919 3.001378 ## 1063 2.603826 2.955357 2.266838 ## 1064 2.730792 3.064956 2.389719 ## 1065 2.833456 3.168197 2.519043 ## 1066 2.918026 3.250367 2.602426 ## 1067 3.031919 3.372097 2.713295 ## 1068 3.141121 3.485130 2.827497 ## 1069 2.636506 2.947378 2.319340 ## 1070 2.754094 3.100264 2.450179 ## 1071 2.859813 3.178933 2.533152 ## 1072 2.963684 3.290253 2.636556 ## 1073 3.072142 3.408608 2.766361 ## 1074 3.187950 3.522580 2.836300 ## 1075 2.368053 2.708001 2.052191 ## 1076 2.468415 2.794929 2.122919 ## 1077 2.576202 2.923694 2.257990 ## 1078 2.712196 3.031516 2.362804 ## 1079 2.806328 3.127951 2.474245 ## 1080 2.896262 3.217898 2.566053 ## 1081 2.247001 2.572420 1.932034 ## 1082 2.372437 2.691342 2.023311 ## 1083 2.486686 2.823775 2.134015 ## 1084 2.573005 2.892359 2.235401 ## 1085 2.682222 3.016324 2.374889 ## 1086 2.794654 3.111344 2.456013 ## 1087 2.820157 3.144709 2.483239 ## 1088 2.933073 3.246185 2.595772 ## 1089 3.030690 3.359134 2.699985 ## 1090 3.136817 3.480904 2.819866 ## 1091 3.239474 3.576138 2.903473 ## 1092 3.364673 3.676432 3.023404 ## 1093 2.337714 2.685995 2.012664 ## 1094 2.445802 2.799306 2.084309 ## 1095 2.552867 2.894385 2.213359 ## 1096 2.668822 2.984490 2.301284 ## 1097 2.777272 3.113490 2.441113 ## 1098 2.865022 3.203100 2.531909 ## 1099 2.607884 2.975397 2.276829 ## 1100 2.715545 3.067714 2.383258 ## 1101 2.827181 3.156038 2.496855 ## 1102 2.937777 3.269479 2.596449 ## 1103 3.061894 3.380273 2.711521 ## 1104 3.151881 3.476919 2.817468 ## 1105 2.703758 3.038003 2.366119 ## 1106 2.791181 3.148419 2.461149 ## 1107 2.896988 3.234604 2.560887 ## 1108 3.003083 3.316443 2.671080 ## 1109 3.121177 3.463540 2.778423 ## 1110 3.221836 3.569296 2.895935 ## 1111 2.533141 2.869834 2.210447 ## 1112 2.650248 2.979176 2.307611 ## 1113 2.765771 3.121291 2.425658 ## 1114 2.868333 3.186028 2.539546 ## 1115 2.966909 3.298171 2.637700 ## 1116 3.067143 3.413492 2.724650 ## 1117 2.242168 2.560912 1.906385 ## 1118 2.347750 2.682658 1.996942 ## 1119 2.448917 2.784172 2.098121 ## 1120 2.548160 2.889261 2.220509 ## 1121 2.649937 2.979773 2.352844 ## 1122 2.775807 3.151723 2.416058 ## 1123 2.739868 3.078947 2.433494 ## 1124 2.856049 3.139759 2.529611 ## 1125 2.953407 3.275873 2.626556 ## 1126 3.071708 3.397024 2.728967 ## 1127 3.167780 3.514454 2.813882 ## 1128 3.285099 3.603366 2.961151 ## 1129 2.814167 3.144100 2.488497 ## 1130 2.910904 3.238546 2.582547 ## 1131 3.026754 3.364158 2.673460 ## 1132 3.110840 3.448981 2.782402 ## 1133 3.229598 3.564696 2.899774 ## 1134 3.316090 3.656619 3.012987 ## 1135 2.233443 2.553550 1.892575 ## 1136 2.340198 2.659427 2.012579 ## 1137 2.414156 2.736379 2.095969 ## 1138 2.541197 2.868530 2.217838 ## 1139 2.647090 2.986493 2.319365 ## 1140 2.757339 3.092763 2.442372 ## 1141 2.580444 2.913971 2.218201 ## 1142 2.680535 3.008232 2.352540 ## 1143 2.781402 3.089705 2.447691 ## 1144 2.876488 3.204089 2.541634 ## 1145 2.996249 3.331805 2.633138 ## 1146 3.103225 3.436851 2.742386 ## 1147 2.565767 2.891325 2.248811 ## 1148 2.656285 2.986642 2.322600 ## 1149 2.752315 3.089420 2.454618 ## 1150 2.877942 3.200670 2.555597 ## 1151 2.974272 3.322547 2.628543 ## 1152 3.105388 3.443244 2.748247 ## 1153 2.651946 2.976611 2.293112 ## 1154 2.753902 3.048383 2.416156 ## 1155 2.859570 3.181201 2.530485 ## 1156 2.965649 3.300013 2.616419 ## 1157 3.067203 3.424854 2.737678 ## 1158 3.192312 3.519593 2.865333 ## 1159 2.602320 2.929553 2.277243 ## 1160 2.698074 3.059485 2.358165 ## 1161 2.804203 3.136996 2.463304 ## 1162 2.921432 3.237509 2.591893 ## 1163 3.007864 3.347111 2.698224 ## 1164 3.125770 3.467158 2.799013 ## 1165 2.204125 2.526230 1.882579 ## 1166 2.295911 2.616260 1.975603 ## 1167 2.404964 2.747708 2.081677 ## 1168 2.530914 2.873403 2.178115 ## 1169 2.611105 2.939786 2.297559 ## 1170 2.740192 3.085064 2.400943 ## 1171 2.580628 2.887619 2.238051 ## 1172 2.693857 3.008413 2.365511 ## 1173 2.778528 3.103217 2.444786 ## 1174 2.892807 3.248718 2.555896 ## 1175 2.981339 3.336265 2.671910 ## 1176 3.111928 3.434763 2.755436 ## 1177 2.779453 3.112982 2.454598 ## 1178 2.901885 3.207139 2.571395 ## 1179 2.984399 3.309191 2.647671 ## 1180 3.099616 3.426748 2.791438 ## 1181 3.222245 3.535471 2.906495 ## 1182 3.320196 3.658812 2.975813 ## 1183 2.748323 3.078289 2.437522 ## 1184 2.874169 3.208200 2.539301 ## 1185 2.984543 3.306139 2.644063 ## 1186 3.068419 3.411506 2.744560 ## 1187 3.176473 3.499052 2.869383 ## 1188 3.292125 3.609573 2.935124 ## 1189 2.424160 2.739386 2.109329 ## 1190 2.521160 2.863233 2.177377 ## 1191 2.622988 2.953641 2.294646 ## 1192 2.737768 3.056761 2.413716 ## 1193 2.856757 3.161630 2.492916 ## 1194 2.949985 3.297231 2.635960 ## 1195 2.944304 3.275525 2.610679 ## 1196 3.057296 3.404226 2.711890 ## 1197 3.158608 3.481980 2.825040 ## 1198 3.259972 3.582518 2.918599 ## 1199 3.371059 3.705725 3.034083 ## 1200 3.464142 3.791491 3.144671 REsim(gpa_mixed) #  ## groupFctr groupID term mean median sd ## 1 student 1 (Intercept) -0.0701441251 -0.0653428740 0.08837158 ## 2 student 2 (Intercept) -0.2202304662 -0.2223898612 0.09214973 ## 3 student 3 (Intercept) 0.0873877150 0.0886630252 0.08800628 ## 4 student 4 (Intercept) -0.1807726976 -0.1820049514 0.09834002 ## 5 student 5 (Intercept) 0.0410582075 0.0422191488 0.09236427 ## 6 student 6 (Intercept) -0.3091729015 -0.3091630822 0.09999388 ## 7 student 7 (Intercept) -0.1337639899 -0.1347231012 0.09023341 ## 8 student 8 (Intercept) 0.2265688239 0.2179101442 0.07970577 ## 9 student 9 (Intercept) 0.1043975124 0.1071941216 0.09602450 ## 10 student 10 (Intercept) 0.0043864174 0.0092656073 0.09730268 ## 11 student 11 (Intercept) 0.3692521548 0.3654887222 0.09030438 ## 12 student 12 (Intercept) 0.0045298016 0.0093334667 0.10037705 ## 13 student 13 (Intercept) 0.5115834030 0.5103808539 0.09552466 ## 14 student 14 (Intercept) 0.1161275484 0.1253977994 0.09250558 ## 15 student 15 (Intercept) -0.2067345444 -0.2079706880 0.09961082 ## 16 student 16 (Intercept) -0.1238122840 -0.1258322975 0.08925757 ## 17 student 17 (Intercept) -0.2789590888 -0.2831670975 0.09239474 ## 18 student 18 (Intercept) -0.3009335474 -0.3055325027 0.09131781 ## 19 student 19 (Intercept) -0.0879324226 -0.0864119646 0.09501440 ## 20 student 20 (Intercept) 0.2313092216 0.2308897555 0.10766345 ## 21 student 21 (Intercept) -0.2982146984 -0.2965590524 0.08598868 ## 22 student 22 (Intercept) 0.1177488798 0.1135527696 0.08869661 ## 23 student 23 (Intercept) 0.0783163453 0.0781032174 0.09057507 ## 24 student 24 (Intercept) 0.0023300277 0.0041957838 0.08897154 ## 25 student 25 (Intercept) 0.3980868584 0.4001562649 0.09845324 ## 26 student 26 (Intercept) -0.2812397924 -0.2943712441 0.09150546 ## 27 student 27 (Intercept) -0.3651466011 -0.3629970138 0.09941070 ## 28 student 28 (Intercept) 0.1187415811 0.1161999825 0.09693254 ## 29 student 29 (Intercept) 0.1091307382 0.1065984050 0.09697106 ## 30 student 30 (Intercept) 0.2089919572 0.2085800962 0.09364268 ## 31 student 31 (Intercept) -0.3524261721 -0.3398861063 0.09526849 ## 32 student 32 (Intercept) 0.2624448438 0.2575619484 0.10209723 ## 33 student 33 (Intercept) -0.1289043654 -0.1291710238 0.09079781 ## 34 student 34 (Intercept) -0.2080244905 -0.2028926698 0.08995363 ## 35 student 35 (Intercept) 0.0929600500 0.0997126798 0.08868677 ## 36 student 36 (Intercept) 0.1218258846 0.1206302857 0.09232084 ## 37 student 37 (Intercept) 0.0918144807 0.0925045986 0.09477724 ## 38 student 38 (Intercept) 0.4270845259 0.4285032893 0.09227096 ## 39 student 39 (Intercept) -0.0506509995 -0.0500473346 0.09329056 ## 40 student 40 (Intercept) 0.2808241957 0.2789791459 0.09720768 ## 41 student 41 (Intercept) 0.5222427778 0.5170842236 0.07747345 ## 42 student 42 (Intercept) -0.0612245619 -0.0589505615 0.09821397 ## 43 student 43 (Intercept) 0.0712991984 0.0634072832 0.09919093 ## 44 student 44 (Intercept) 0.1853595709 0.1903546204 0.08934024 ## 45 student 45 (Intercept) 0.1646367415 0.1630317631 0.09235609 ## 46 student 46 (Intercept) -0.2833490752 -0.2786054884 0.09581029 ## 47 student 47 (Intercept) -0.2363104714 -0.2425080768 0.09727302 ## 48 student 48 (Intercept) -0.0040568964 -0.0012695224 0.10627944 ## 49 student 49 (Intercept) 0.1010177140 0.1008543513 0.08739272 ## 50 student 50 (Intercept) -0.0732538460 -0.0785328453 0.09267742 ## 51 student 51 (Intercept) -0.0316958011 -0.0316320743 0.09892249 ## 52 student 52 (Intercept) 0.1615415722 0.1633398902 0.09345008 ## 53 student 53 (Intercept) 0.0797053923 0.0708917983 0.09293485 ## 54 student 54 (Intercept) 0.1919852107 0.1898379921 0.08613061 ## 55 student 55 (Intercept) 0.1462447827 0.1363388478 0.09362994 ## 56 student 56 (Intercept) -0.2502819368 -0.2499495948 0.09683016 ## 57 student 57 (Intercept) 0.0798509191 0.0889234469 0.08928742 ## 58 student 58 (Intercept) -0.1922060107 -0.1904856464 0.09152201 ## 59 student 59 (Intercept) 0.0553145018 0.0579430337 0.08967803 ## 60 student 60 (Intercept) -0.1612881653 -0.1528606537 0.08929338 ## 61 student 61 (Intercept) 0.2635424935 0.2634652974 0.09242559 ## 62 student 62 (Intercept) -0.1065271982 -0.1161724492 0.09323082 ## 63 student 63 (Intercept) 0.2842124383 0.2717154465 0.09370265 ## 64 student 64 (Intercept) 0.1064288267 0.1084319913 0.09578379 ## 65 student 65 (Intercept) -0.2271902858 -0.2322211850 0.08901960 ## 66 student 66 (Intercept) 0.1902778111 0.1880299953 0.09202522 ## 67 student 67 (Intercept) -0.2128313531 -0.2197645924 0.09740863 ## 68 student 68 (Intercept) -0.2794255545 -0.2801771847 0.09390139 ## 69 student 69 (Intercept) -0.2725259035 -0.2694534474 0.09571048 ## 70 student 70 (Intercept) 0.0123373253 0.0183678397 0.09777276 ## 71 student 71 (Intercept) -0.0111995195 -0.0146348566 0.09575767 ## 72 student 72 (Intercept) 0.4464058490 0.4427494564 0.09108144 ## 73 student 73 (Intercept) -0.2093584600 -0.2128455487 0.08698930 ## 74 student 74 (Intercept) 0.2129124934 0.2152363106 0.09097525 ## 75 student 75 (Intercept) 0.0356707113 0.0334374491 0.09263167 ## 76 student 76 (Intercept) 0.1645026208 0.1687177835 0.09320952 ## 77 student 77 (Intercept) 0.4801422806 0.4800314156 0.09410675 ## 78 student 78 (Intercept) -0.3720704007 -0.3724224487 0.09247266 ## 79 student 79 (Intercept) -0.4750333777 -0.4708788986 0.10239928 ## 80 student 80 (Intercept) 0.2046843093 0.2039969543 0.10058781 ## 81 student 81 (Intercept) 0.0616056956 0.0628244667 0.09049684 ## 82 student 82 (Intercept) 0.3014061555 0.3005184817 0.08648285 ## 83 student 83 (Intercept) 0.2459310939 0.2474041206 0.09906224 ## 84 student 84 (Intercept) -0.5422805175 -0.5402857009 0.09043759 ## 85 student 85 (Intercept) 0.1111470004 0.1153206486 0.08975769 ## 86 student 86 (Intercept) -0.1802311434 -0.1808124554 0.09069592 ## 87 student 87 (Intercept) 0.0090507943 0.0095113827 0.09075256 ## 88 student 88 (Intercept) -0.0494293808 -0.0538339207 0.09198209 ## 89 student 89 (Intercept) 0.1481648461 0.1353111430 0.09717643 ## 90 student 90 (Intercept) -0.0647119918 -0.0652740054 0.08978581 ## 91 student 91 (Intercept) -0.3174057613 -0.3144776040 0.09740125 ## 92 student 92 (Intercept) 0.6504321257 0.6589431274 0.08894810 ## 93 student 93 (Intercept) 0.3740816004 0.3712456592 0.08918792 ## 94 student 94 (Intercept) -0.0538814411 -0.0487989412 0.08603480 ## 95 student 95 (Intercept) -0.1248807649 -0.1271020617 0.08780561 ## 96 student 96 (Intercept) 0.1563548413 0.1552185001 0.09862370 ## 97 student 97 (Intercept) -0.0560520232 -0.0546918743 0.09314826 ## 98 student 98 (Intercept) 0.1926950754 0.1897865051 0.09705696 ## 99 student 99 (Intercept) -0.1753884872 -0.1754189249 0.08997152 ## 100 student 100 (Intercept) -0.4454068115 -0.4418448353 0.08863434 ## 101 student 101 (Intercept) 0.3399038906 0.3409828077 0.09102040 ## 102 student 102 (Intercept) -0.0434914946 -0.0529236298 0.08773062 ## 103 student 103 (Intercept) -0.1444243237 -0.1451467082 0.09554499 ## 104 student 104 (Intercept) 0.0884419425 0.0885524468 0.10031758 ## 105 student 105 (Intercept) 0.7636131049 0.7599163240 0.09672496 ## 106 student 106 (Intercept) -0.3533423576 -0.3605582530 0.09380117 ## 107 student 107 (Intercept) -0.3413263801 -0.3375707555 0.09323817 ## 108 student 108 (Intercept) -0.3970496259 -0.3981794336 0.09265933 ## 109 student 109 (Intercept) -0.0737720854 -0.0720942910 0.09177485 ## 110 student 110 (Intercept) -0.5058490389 -0.5147156635 0.09819104 ## 111 student 111 (Intercept) -0.4276882562 -0.4242468427 0.09375877 ## 112 student 112 (Intercept) 0.2177594910 0.2188409328 0.09361701 ## 113 student 113 (Intercept) -0.1632398240 -0.1590234512 0.09303292 ## 114 student 114 (Intercept) -0.0442431284 -0.0444739128 0.09941273 ## 115 student 115 (Intercept) -0.0930472108 -0.0850979406 0.10053190 ## 116 student 116 (Intercept) -0.1226085204 -0.1255583130 0.09628950 ## 117 student 117 (Intercept) -0.0181652445 -0.0131486322 0.08998918 ## 118 student 118 (Intercept) 0.1322927991 0.1301987793 0.08703849 ## 119 student 119 (Intercept) -0.2163988876 -0.2104952220 0.09566085 ## 120 student 120 (Intercept) 0.1146403514 0.1084588585 0.09627142 ## 121 student 121 (Intercept) 0.0882460054 0.0952429958 0.09386717 ## 122 student 122 (Intercept) 0.2398653391 0.2420169983 0.09254130 ## 123 student 123 (Intercept) -0.3643493382 -0.3581798059 0.09533906 ## 124 student 124 (Intercept) 0.0931549117 0.1066954939 0.09910996 ## 125 student 125 (Intercept) -0.1966462141 -0.2044109774 0.09043430 ## 126 student 126 (Intercept) -0.1165706017 -0.1238693215 0.09326748 ## 127 student 127 (Intercept) -0.3454204247 -0.3491334004 0.09696113 ## 128 student 128 (Intercept) -0.0635472596 -0.0670178456 0.08426389 ## 129 student 129 (Intercept) -0.0645493015 -0.0623258064 0.08871116 ## 130 student 130 (Intercept) 0.1288228821 0.1309191151 0.09056711 ## 131 student 131 (Intercept) 0.3006764285 0.2931195081 0.09721224 ## 132 student 132 (Intercept) 0.1630088360 0.1689834768 0.09425092 ## 133 student 133 (Intercept) -0.3874131080 -0.3936745408 0.08603919 ## 134 student 134 (Intercept) 0.0095818448 0.0127760670 0.09019761 ## 135 student 135 (Intercept) 0.0794179808 0.0813987490 0.10104949 ## 136 student 136 (Intercept) -0.1829336513 -0.1846569448 0.09049691 ## 137 student 137 (Intercept) -0.0936815919 -0.0938303677 0.08688850 ## 138 student 138 (Intercept) -0.0651010689 -0.0565179865 0.10125288 ## 139 student 139 (Intercept) 0.2096873918 0.2062692817 0.08337870 ## 140 student 140 (Intercept) -0.1208675087 -0.1196935399 0.10279837 ## 141 student 141 (Intercept) 0.1868573759 0.1842000971 0.10033548 ## 142 student 142 (Intercept) 0.0855118494 0.0865542581 0.09177319 ## 143 student 143 (Intercept) -0.0370811223 -0.0397532011 0.09699113 ## 144 student 144 (Intercept) 0.0954501733 0.0974815711 0.08608155 ## 145 student 145 (Intercept) -0.0649314598 -0.0623532652 0.09582989 ## 146 student 146 (Intercept) -0.2943527778 -0.3014677247 0.09850573 ## 147 student 147 (Intercept) 0.3931339639 0.3949188125 0.09389418 ## 148 student 148 (Intercept) -0.1318487239 -0.1357467069 0.08828491 ## 149 student 149 (Intercept) -0.0599850630 -0.0529232676 0.08312354 ## 150 student 150 (Intercept) -0.2357349396 -0.2357383144 0.09219913 ## 151 student 151 (Intercept) -0.1736384993 -0.1733922935 0.09609942 ## 152 student 152 (Intercept) -0.1092123316 -0.0968007666 0.10288396 ## 153 student 153 (Intercept) -0.0257450564 -0.0182665686 0.09362914 ## 154 student 154 (Intercept) 0.2094334176 0.2116755366 0.09331615 ## 155 student 155 (Intercept) -0.0854149735 -0.0792126340 0.09858784 ## 156 student 156 (Intercept) -0.0717525804 -0.0721545508 0.09292676 ## 157 student 157 (Intercept) 0.3262693848 0.3248834172 0.09813457 ## 158 student 158 (Intercept) -0.0250980136 -0.0238847021 0.09062694 ## 159 student 159 (Intercept) 0.4481590893 0.4457090681 0.09748170 ## 160 student 160 (Intercept) 0.3746158973 0.3731431109 0.09815062 ## 161 student 161 (Intercept) -0.0949008422 -0.0858551335 0.09105678 ## 162 student 162 (Intercept) -0.4347731881 -0.4313187847 0.09947139 ## 163 student 163 (Intercept) -0.0363863465 -0.0395226411 0.08863551 ## 164 student 164 (Intercept) -0.1128092170 -0.1163854370 0.08994308 ## 165 student 165 (Intercept) -0.0161037259 -0.0110298367 0.10209537 ## 166 student 166 (Intercept) 0.1864246942 0.1813589556 0.09305685 ## 167 student 167 (Intercept) 0.2143616505 0.2200978615 0.09862017 ## 168 student 168 (Intercept) 0.2566478081 0.2626211445 0.09432605 ## 169 student 169 (Intercept) 0.2645716680 0.2631456669 0.09214512 ## 170 student 170 (Intercept) 0.3879876413 0.3965523123 0.09511184 ## 171 student 171 (Intercept) -0.2868575970 -0.2884547225 0.09508759 ## 172 student 172 (Intercept) -0.0426042112 -0.0409845551 0.09093158 ## 173 student 173 (Intercept) 0.2609751010 0.2611964441 0.09074382 ## 174 student 174 (Intercept) 0.1280574618 0.1258185986 0.09768273 ## 175 student 175 (Intercept) 0.0881718641 0.0821079089 0.08992090 ## 176 student 176 (Intercept) -0.0681650149 -0.0698379006 0.09545066 ## 177 student 177 (Intercept) 0.1742530880 0.1825738711 0.08650696 ## 178 student 178 (Intercept) 0.0206212237 0.0244687603 0.10162840 ## 179 student 179 (Intercept) 0.0451474335 0.0462921205 0.10186756 ## 180 student 180 (Intercept) -0.2270501496 -0.2251731670 0.08920794 ## 181 student 181 (Intercept) -0.3452396330 -0.3442131993 0.09356516 ## 182 student 182 (Intercept) 0.2269296373 0.2412016982 0.09367869 ## 183 student 183 (Intercept) -0.2554041473 -0.2580152281 0.09072640 ## 184 student 184 (Intercept) 0.0075882684 0.0040747457 0.09444082 ## 185 student 185 (Intercept) 0.0889309897 0.0866766503 0.10059682 ## 186 student 186 (Intercept) -0.0595674958 -0.0546590167 0.09569728 ## 187 student 187 (Intercept) -0.3550475154 -0.3553724839 0.09336050 ## 188 student 188 (Intercept) 0.1574162482 0.1597920754 0.09437906 ## 189 student 189 (Intercept) 0.2031916505 0.2089712878 0.08781871 ## 190 student 190 (Intercept) -0.3746192114 -0.3814606375 0.09607542 ## 191 student 191 (Intercept) -0.0197796846 -0.0134115392 0.08257442 ## 192 student 192 (Intercept) -0.0352233553 -0.0312633464 0.09573957 ## 193 student 193 (Intercept) 0.0442361493 0.0450273469 0.09574236 ## 194 student 194 (Intercept) 0.0009392301 0.0009879542 0.09553905 ## 195 student 195 (Intercept) -0.3949879906 -0.3881833272 0.08319630 ## 196 student 196 (Intercept) -0.0166472942 -0.0125959884 0.08853223 ## 197 student 197 (Intercept) 0.1914342614 0.1885089514 0.09660712 ## 198 student 198 (Intercept) 0.1648977111 0.1624493728 0.09097144 ## 199 student 199 (Intercept) -0.1936563421 -0.1943788764 0.08935327 ## 200 student 200 (Intercept) 0.3475358303 0.3435548539 0.09670591 plotREsim(REsim(gpa_mixed)) #  ##  re.form = NA predict(gpa_mixed, re.form=NA) %&gt;% head() ## 1 2 3 4 5 6 ## 2.599214 2.705529 2.811843 2.918157 3.024471 3.130786 ## R outputs AIC and BIC values automatically. The AICC value has to be computed as ## AICC &lt;- -2*logLik(gpa_mixed)+2*p*n/(n-p-1) 22.2.3.2 Model with random intercept and slpoe effects gpa_mixed = lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) summary(gpa_mixed) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: gpa ~ occasion + (1 + occasion | student) ## Data: gpa ## ## REML criterion at convergence: 261 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.2695 -0.5377 -0.0128 0.5326 3.1939 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## student (Intercept) 0.045193 0.21259 ## occasion 0.004504 0.06711 -0.10 ## Residual 0.042388 0.20588 ## Number of obs: 1200, groups: student, 200 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.599e+00 1.836e-02 1.990e+02 141.59 &lt;2e-16 *** ## occasion 1.063e-01 5.885e-03 1.990e+02 18.07 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## occasion -0.345 22.3 Random Slope and Intercept Model with Covariance Structure 22.3.1 Model Introduction In the random slope and intercept model introduced before, the variance-covariance matrix for the error terms has a diagonal structure. It is of the form \\(\\sigma^{2} \\mathbb{I}\\), where \\(\\mathbb{I}\\) denotes a \\(n p \\times n p\\) identity matrix, with ones on the main diagonal and zeros everywhere else. This structure assumes that the errors are independent, and thus is termed an independent structure. The random slope and intercept models with other structures for the variance-covariance matrix of the error terms can be considered. The most general is an unstructured one with an \\(n p \\times n p\\) block-diagonal matrix with \\(n\\) identical blocks of size \\(p \\times p\\) of the form: \\[ \\left[\\begin{array}{ccccc} \\sigma_{1}^{2} &amp; \\sigma_{12} &amp; \\sigma_{13} &amp; \\ldots &amp; \\sigma_{1 p} \\\\ \\sigma_{12} &amp; \\sigma_{2}^{2} &amp; \\sigma_{23} &amp; \\ldots &amp; \\sigma_{2 p} \\\\ \\sigma_{13} &amp; \\sigma_{23} &amp; \\sigma_{3}^{2} &amp; \\ldots &amp; \\sigma_{3 p} \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\sigma_{1 p} &amp; \\sigma_{2 p} &amp; \\sigma_{3 p} &amp; \\ldots &amp; \\sigma_{p}^{2} \\end{array}\\right] \\]    The first model has the blocks in the variance-covariance matrix that have constant values on each of the descending diagonals, that is, the matrix has the form: \\[ \\left[\\begin{array}{ccccc} \\sigma^{2} &amp; \\sigma_{1} &amp; \\sigma_{2} &amp; \\ldots &amp; \\sigma_{p-1} \\\\ \\sigma_{1} &amp; \\sigma^{2} &amp; \\sigma_{1} &amp; \\ldots &amp; \\sigma_{p-2} \\\\ \\sigma_{2} &amp; \\sigma_{1} &amp; \\sigma^{2} &amp; \\ldots &amp; \\sigma_{p-3} \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\sigma_{p-1} &amp; \\sigma_{p-2} &amp; \\sigma_{p-3} &amp; \\ldots &amp; \\sigma^{2} \\end{array}\\right] \\] Using this structure implies that observations that are the same number of time points apart are equally correlated. There are a total of \\(p\\) unknown parameters \\(\\sigma^{2}, \\sigma_{1}, \\ldots, \\sigma_{p-1} .\\) This model is said to have the Toeplitz covariance structure, which is sometimes referred to as an autoregressive-moving average \\(\\operatorname{ARMA}(1,1)\\) structure. Another model with useful structure of the variance-covariance matrix relies on the fact that typically as time goes on, observations become less correlated with the earlier ones. In this model, each block in the variance-covariance matrix has \\(\\sigma^{2} \\rho^{\\left|t_{i}-t_{j}\\right|}\\) in the \\(i j\\) -th cell, \\(i, j=1, \\ldots, p\\), that is, it looks like this: \\[ \\sigma^{2}\\left[\\begin{array}{ccccc} 1 &amp; \\rho^{\\left|t_{1}-t_{2}\\right|} &amp; \\rho^{\\left|t_{1}-t_{3}\\right|} &amp; \\ldots &amp; \\rho^{\\left|t_{1}-t_{p}\\right|} \\\\ \\rho^{\\left|t_{1}-t_{2}\\right|} &amp; 1 &amp; \\rho^{\\left|t_{2}-t_{3}\\right|} &amp; \\ldots &amp; \\rho^{\\left|t_{2}-t_{p}\\right|} \\\\ \\rho^{\\left|t_{1}-t_{3}\\right|} &amp; \\rho^{\\left|t_{2}-t_{3}\\right|} &amp; 1 &amp; \\ldots &amp; \\rho^{\\left|t_{3}-t_{p}\\right|} \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\rho^{\\left|t_{1}-t_{p}\\right|} &amp; \\rho^{\\left|t_{2}-t_{p}\\right|} &amp; \\rho^{\\left|t_{3}-t_{p}\\right|} &amp; \\ldots &amp; 1 \\end{array}\\right] \\] Here \\(\\sigma^{2}\\) and \\(\\rho\\) are the unknown constants, \\(|\\rho|&lt;1\\). Note that in this matrix the entries decrease as the distance between times \\(t_{i}\\) and \\(t_{j}\\) increases.  - o A special case of this model is when the times are equal to \\(\\$ 1,2,3\\), Vidots, \\(\\mathrm{p} \\$\\). Then the $p \\(\\mathrm{p}\\) $ blocks of the variance-covariance matrix become: \\[ \\sigma^{2}\\left[\\begin{array}{ccccc} 1 &amp; \\rho &amp; \\rho^{2} &amp; \\ldots &amp; \\rho^{p-1} \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\ldots &amp; \\rho^{p-2} \\\\ \\rho^{2} &amp; \\rho &amp; 1 &amp; \\ldots &amp; \\rho^{p-3} \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\rho^{p-1} &amp; \\rho^{p-2} &amp; \\rho^{p-3} &amp; \\ldots &amp; 1 \\end{array}\\right] \\] This model is said to have an autoregressive variance-covariance structure of the error terms, referring to an AR(1) model, an autoregressive time series model with lag one that has the same covariance structure. Note that the autoregressive matrix is a special case of both Toeplitz and spatial power matrices. The last model that we introduce here is the model with compound symmetric (or exchangeable) variance-covariance matrix of the error terms. In this matrix, all variances are assumed to be equal to \\(\\sigma^{2}\\), and all correlations are assumed to be equal to \\(\\rho\\). That is, the matrix has the form: \\[ \\sigma^{2}\\left[\\begin{array}{ccccc} 1 &amp; \\rho &amp; \\rho &amp; \\ldots &amp; \\rho \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\ldots &amp; \\rho \\\\ \\rho &amp; \\rho &amp; 1 &amp; \\ldots &amp; \\rho \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\rho &amp; \\rho &amp; \\rho &amp; \\ldots &amp; 1 \\end{array}\\right] \\] - () ,   22.3.2 SAS Implementation repeated / subject=id name type=covtype name r; Covariance structures in the option type= are un (unstructured), toep (Toeplitz), sp(pow)(time_name) (spatial power), ar(1) (autoregressive), or cs (compound symmetric). By specifying the option \\(r\\) we request that SAS outputs an estimated individual block of the variance-covariance matrix of the error terms, labeling it Estimated R Matrix for Subject 1 . Estimated correlation coefficient \\(\\rho\\) in the output is labeled SP(POW) (spatial power), AR (1) (autoregressive), or CS (compound symmetric). /*fitting random slope and intercept model with unstructured covariance matrix of error terms*/ proc mixed covtest; class gender; model LDL=gender age month/solution; random intercept month/subject=id type=un; repeated/subject=id type=un r; run; /*fitting random slope and intercept model with Toeplitz covariance matrix of error terms*/ proc mixed covtest; class gender; model LDL=gender age month/solution; random intercept month/subject=id type=un; repeated / subject=id type=toep r; run; 22.3.3 R Implementation A structure can be specified by adding correlation= to the syntax within thelme()function. The choices are: corSymm() (unstructured), corARMA(form= 1|id.name, p=1, q=1) (Toeplitz), corCAR1(form=1|id.name) (spatial power), corAR1(form=1|id.name) (autoregressive), corCompSymm(form=1|id.name) (compound symmetric). lme4 occasion ### Heterogeneous variance library(nlme) heterovar_res = lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, weights = varIdent(form = ~ 1 | occasion) ) summary(heterovar_res) ## Linear mixed-effects model fit by REML ## Data: gpa ## AIC BIC logLik ## 274.0536 319.8492 -128.0268 ## ## Random effects: ## Formula: ~1 | student ## (Intercept) Residual ## StdDev: 0.305992 0.3716858 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | occasion ## Parameter estimates: ## 0 1 2 3 4 5 ## 1.0000000 0.8261186 0.6272415 0.4311126 0.3484013 0.4324628 ## Fixed effects: gpa ~ occasion ## Value Std.Error DF t-value p-value ## (Intercept) 2.5987900 0.026249746 999 99.00248 0 ## occasion 0.1061401 0.004033168 999 26.31680 0 ## Correlation: ## (Intr) ## occasion -0.528 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.74415646 -0.64519742 0.02266271 0.62705166 3.05558475 ## ## Number of Observations: 1200 ## Number of Groups: 200 ### Autocorrelation corr_res = lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, correlation = corAR1(form = ~ occasion) ) summary(corr_res) ## Linear mixed-effects model fit by REML ## Data: gpa ## AIC BIC logLik ## 334.1208 359.5629 -162.0604 ## ## Random effects: ## Formula: ~1 | student ## (Intercept) Residual ## StdDev: 0.2145887 0.2732718 ## ## Correlation Structure: AR(1) ## Formula: ~occasion | student ## Parameter estimate(s): ## Phi ## 0.4182174 ## Fixed effects: gpa ~ occasion ## Value Std.Error DF t-value p-value ## (Intercept) 2.5969002 0.022951743 999 113.14610 0 ## occasion 0.1071433 0.005278835 999 20.29677 0 ## Correlation: ## (Intr) ## occasion -0.575 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.053039560 -0.612466765 0.009312407 0.607759461 2.646750929 ## ## Number of Observations: 1200 ## Number of Groups: 200 ## nlmePhiPhi 22.4 Generalized Estimating Equation 22.4.1 Introduction of GEE Alternatives to Mixed Models Fixed effects models (also panel linear models with fixed, as opposed to random, effects) Using cluster-robust standard errors Generalized estimating equations (GEE)&gt; GEEGLS/ GLM GEE In the GEE model, there are no random-effects terms. Instead, the response variable relates to the predictors via the generalized linear regression model with only fixed-effects terms, and the variance-covariance structure of the response variable itself is specified, rather than that for the error terms. GEEGEE- Suppose that for each individual \\(i, i=1, \\ldots, n\\), at time \\(t_{j}, j=1, \\ldots, p\\), the observations are \\(x_{1 i j}, \\ldots, x_{k i j}, y_{i j} .\\) Denote by \\(\\mu_{i j}=\\mathbb{E}\\left(y_{i j}\\right)\\), the mean response, and assume that \\(g\\left(\\mu_{i j}\\right)=\\beta_{0}+\\beta_{1} x_{1 i j}+\\cdots+\\beta_{k} x_{k i j}+\\beta_{k+1} t_{j}\\) where \\(g(\\cdot)\\) is the link function. Next, we write the variance of \\(y_{i j}\\) as a function of \\(\\mu_{i j}, \\operatorname{Var}\\left(y_{i j}\\right)=V\\left(\\mu_{i j}\\right)\\). The function \\(V(\\cdot)\\) is termed the variance function. Further, we model the covariance structure of correlated responses for a given individual \\(i, i=1, \\ldots, n\\) Observations between individuals are assumed independent. Let \\(\\mathbf{A}_{i}\\) denote a \\(p \\times p\\) diagonal matrix \\[ \\mathbf{A}_{i}=\\left[\\begin{array}{cccc} V\\left(\\mu_{i 1}\\right) &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; V\\left(\\mu_{i 2}\\right) &amp; \\ldots &amp; 0 \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; 0 &amp; \\ldots &amp; V\\left(\\mu_{i p}\\right) \\end{array}\\right] \\] Also, let \\(\\mathbf{R}_{i}(\\boldsymbol{\\alpha})\\) be the working correlation matrix of the repeated responses for the \\(i\\) -th individual, where \\(\\alpha\\) denotes a vector of unknown parameters which are the same for all individuals. The working covariance matrix for \\(\\mathbf{y}_{i}=\\left(y_{i 1}, y_{i 2}, \\ldots, y_{i p}\\right)^{\\prime}\\) is then \\[ \\mathbf{V}_{i}(\\boldsymbol{\\alpha})=\\mathbf{A}_{i}^{1 / 2} \\mathbf{R}_{i}(\\boldsymbol{\\alpha}) \\mathbf{A}_{i}^{1 / 2} \\] The regression coefficients \\(\\beta\\) s and the vector of parameters \\(\\boldsymbol{\\alpha}\\) are the only unknowns of the GEE model and must be estimated from the data.  \\(\\mathbf{R}_{i}(\\boldsymbol{\\alpha})\\). - unstructured \\(\\mathbf{R}_{i}(\\boldsymbol{\\alpha})=\\left[\\begin{array}{ccccc}1 &amp; \\alpha_{12} &amp; \\alpha_{13} &amp; \\ldots &amp; \\alpha_{1 p} \\\\ \\alpha_{12} &amp; 1 &amp; \\alpha_{23} &amp; \\ldots &amp; \\alpha_{2 p} \\\\ \\alpha_{13} &amp; \\alpha_{23} &amp; 1 &amp; \\ldots &amp; \\alpha_{3 p} \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\alpha_{1 p} &amp; \\alpha_{2 p} &amp; \\alpha_{3 p} &amp; \\ldots &amp; 1\\end{array}\\right]\\) - Toeplitz \\(\\mathbf{R}_{i}(\\boldsymbol{\\alpha})=\\left[\\begin{array}{ccccc}1 &amp; \\alpha_{1} &amp; \\alpha_{2} &amp; \\ldots &amp; \\alpha_{p-1} \\\\ \\alpha_{1} &amp; 1 &amp; \\alpha_{1} &amp; \\ldots &amp; \\alpha_{p-2} \\\\ \\alpha_{2} &amp; \\alpha_{1} &amp; 1 &amp; \\ldots &amp; \\alpha_{p-3} \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\alpha_{p-1} &amp; \\alpha_{p-2} &amp; \\alpha_{p-3} &amp; \\ldots &amp; 1\\end{array}\\right]\\) - autoregressive \\(\\mathbf{R}_{i}(\\boldsymbol{\\alpha})=\\left[\\begin{array}{ccccc}1 &amp; \\alpha &amp; \\alpha^{2} &amp; \\ldots &amp; \\alpha^{p-1} \\\\ \\alpha &amp; 1 &amp; \\alpha &amp; \\ldots &amp; \\alpha^{p-2} \\\\ \\alpha^{2} &amp; \\alpha &amp; 1 &amp; \\ldots &amp; \\alpha^{p-3} \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\alpha^{p-1} &amp; \\alpha^{p-2} &amp; \\alpha^{p-3} &amp; \\ldots &amp; 1\\end{array}\\right]\\) - compound symmetric or exchangeable \\(\\mathbf{R}_{i}(\\boldsymbol{\\alpha})=\\left[\\begin{array}{ccccc}1 &amp; \\alpha &amp; \\alpha &amp; \\ldots &amp; \\alpha \\\\ \\alpha &amp; 1 &amp; \\alpha &amp; \\ldots &amp; \\alpha \\\\ \\alpha &amp; \\alpha &amp; 1 &amp; \\ldots &amp; \\alpha \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\alpha &amp; \\alpha &amp; \\alpha &amp; \\alpha &amp; 1\\end{array}\\right]\\) - independent \\(\\mathbf{R}_{i}(\\boldsymbol{\\alpha})=\\left[\\begin{array}{ccccc}1 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\0 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\0 &amp; 0 &amp; 1 &amp; \\ldots &amp; 0 \\\\\\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1\\end{array}\\right]\\) The GEE estimate of \\(\\boldsymbol{\\beta}=\\left(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k+1}\\right)^{\\prime}\\) is the solution of the generalized estimating equations: \\[ \\sum_{i=1}^{n}\\left(\\frac{\\partial \\boldsymbol{\\mu}_{i}}{\\partial \\boldsymbol{\\beta}}\\right)_{(k+2) \\times p}\\left[\\mathbf{V}_{i}(\\hat{\\boldsymbol{\\alpha}})\\right]_{p \\times p}^{-1}\\left(\\mathbf{y}_{i}-\\boldsymbol{\\mu}_{i}\\right)_{p \\times 1}=\\mathbf{0}_{(k+2) \\times 1} \\] where \\(\\boldsymbol{\\mu}_{i}=\\left(\\mu_{i 1}, \\ldots, \\mu_{i p}\\right)^{\\prime}\\) is the vector of mean responses, and the estimator \\(\\hat{\\boldsymbol{\\alpha}}\\) is the method of moments estimator of the vector of parameters. 22.4.2 SAS Implementation proc genmod data=longform data name; class &lt;list of categorical predictors&gt;; model response name=&lt;list of x predictors&gt; time name/ dist=normal link=identity; output out=outdata name p=predicted response name; repeated subject=id name/type=corrtype name corrw covb; run; The types of the working correlation matrix are un for unstructured, mdep(m) for Toeplitz with m repeated observations for each individual, ar for autoregressive, cs or exch for compound symmetric (or exchangeable), and ind for independent. Specifying the option corrw produces the estimated working correlation matrix for a single individual. data cholesterol; input id gender$ age LDL0 LDL6 LDL9 LDL24 @@; cards; 1 M 50 73 71 80 85 2 F 72 174 164 139 112 3 M 46 85 86 82 90 4 F 71 172 150 139 127 5 F 75 186 177 153 145 6 F 68 184 169 153 138 7 F 63 196 188 163 155 8 M 73 137 137 132 104 9 M 59 135 120 106 106 10 M 60 111 110 100 76 11 F 59 127 126 106 99 12 M 46 88 87 84 80 13 F 67 176 150 156 153 14 F 52 155 135 128 120 15 M 65 142 117 114 97 16 F 75 158 143 145 135 17 F 57 148 131 138 102 18 M 58 125 111 118 124 19 M 48 76 65 94 98 20 M 47 116 108 94 107 21 F 53 191 185 162 113 22 F 73 167 165 162 140 23 M 62 109 104 93 94 24 F 77 167 164 155 155 25 M 55 103 94 75 78 26 F 74 122 126 105 111 27 F 79 203 204 178 145 ; proc genmod; class id gender; model ldl=gender age month/dist=normal link=identity; repeated subject=id/type=un corrw; run; 22.4.3 R Implementation Function geeglm() in package geepack may be employed to fit a GEE model with the specified underlying distribution. R doesnt automatically fit models with the Toeplitz correlation matrix. The choices for the correlation structures are unstructured, ar1, exchangeable, and independence. #fitting GEE model with unstructured working correlation matrix summary(un.fitted.model&lt;- geeglm(LDL ~ gender.rel + age + month, data=longform.data, id=id, family=gaussian(link=&quot;identity&quot;), corstr=&quot;unstructured&quot;)) QIC(un.fitted.model) 22.5 Hierarchical Regression Model for Normal Response 22.5.1 Introduction These models incorporate potential correlation among observations at each hierarchical level.  Suppose data are recorded at times \\(t_{1}, \\ldots, t_{p}\\) on each of \\(n\\) individuals, who are grouped into \\(c\\) clusters. For each individual, there are \\(k\\) predictor variables and one normally distributed response. An observation at time \\(j\\) for individual \\(i\\) in cluster \\(m\\) is \\(\\left(x_{1 i j m}, \\ldots, x_{k i j m}, y_{i j m}\\right)\\), where \\(i=1, \\ldots, n, j=1, \\ldots, p\\), and \\(m=1, \\ldots, c .\\) Some of the \\(x\\) variables may be characteristics of times (level 1 ), or of individuals (level 2), or of clusters (level 3). A general form of the three level hierarchical model (also termed three-stage hierarchical regression model or multilevel model or model for clustered data) for a normal response is: \\[ y_{i j m}=\\beta_{0}+\\beta_{1} x_{1 i j m}+\\cdots+\\beta_{k} x_{k i j m}+\\beta_{k+1} t_{j}+u_{1 i m}+u_{2 i m} t_{j}+\\tau_{1 m}+\\tau_{2 m} t_{j}+\\varepsilon_{i j m} \\] where \\[ \\begin{array}{c} u_{1 i m} \\stackrel{i i d}{\\sim} \\mathcal{N}\\left(0, \\sigma_{u_{1}}^{2}\\right), u_{2 i m} \\stackrel{i i d}{\\sim} \\mathcal{N}\\left(0, \\sigma_{u_{2}}^{2}\\right), \\operatorname{Cov}\\left(u_{1 i m}, u_{2 i m}\\right)=\\sigma_{u_{1} u_{2}} \\\\ \\operatorname{Cov}\\left(u_{1 i m}, u_{2 i^{\\prime} m^{\\prime}}\\right)=0 \\text { for } i \\neq i^{\\prime} \\end{array} \\] These two random variables are the level-2 random slope and intercept, respectively. Also, \\(\\tau_{1 m} \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}\\left(0, \\sigma_{\\tau_{1}}^{2}\\right), \\tau_{2 m} \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}\\left(0, \\sigma_{\\tau_{2}}^{2}\\right), \\mathbb{C o v}\\left(\\tau_{1 m}, \\tau_{2 m}\\right)=\\sigma_{\\tau_{1} \\tau_{2}}\\), and \\(\\mathbb{C} \\operatorname{cov}\\left(\\tau_{1 m}, \\tau_{1 m^{\\prime}}\\right)=0\\) for \\(m \\neq m^{\\prime} .\\) These variables are, respectively, the level-3 random slope and intercept. The random errors \\(\\varepsilon_{i j m}\\) s are independent with \\(\\mathcal{N}\\left(0, \\sigma^{2}\\right)\\) distribution. In addition, all \\(u\\) s are independent of \\(\\tau\\) s, and both are independent of \\(\\varepsilon\\) s. As defined in the above formula, the index \\(i\\) for individuals (or, more generally, level- 2 variable) ranges between 1 and \\(n\\). It is also possible to enumerate individuals only within each cluster: \\(i=1, \\ldots, n_{m}\\) where \\(\\sum_{m=1}^{c} n_{m}=n\\) 22.5.2 SAS Implementation To accommodate the hierarchical structure, random statements should be included for variables defining levels 2 and 3 of the model. proc mixed data=longform data name covtest; class cluster name individual name &lt;list of categorical predictors&gt;; model response name=&lt;list of predictors&gt; time name/solution outpm=outdata name; random intercept time name/subject=cluster name type=un; random intercept time name/subject=individual name(cluster name) type=un; run; data dyads; input family individual relation$ depression1 depression2 depression3 qol1 qol2 qol3 @@; cards; 1 1 M 1 1 1 4.0 4.1 4.9 1 2 D 1 1 0 2.5 3.2 4.2 2 1 M 1 1 1 2.6 2.8 4.1 2 2 D 1 1 1 2.8 3.1 4.2 3 1 M 1 1 1 2.5 3.8 4.0 3 2 D 1 1 1 2.4 5.1 3.3 4 1 M 1 0 0 2.1 3.3 4.6 4 2 D 1 0 0 3.7 3.1 4.4 5 1 M 1 0 0 2.9 4.2 3.4 5 2 D 1 0 0 2.4 2.6 2.7 6 1 M 1 1 0 3.3 4.2 4.7 6 2 D 1 1 0 2.7 4.0 4.1 7 1 M 1 1 0 3.7 4.3 3.8 7 2 D 1 1 0 2.8 3.2 3.6 8 1 M 1 0 0 3.5 4.1 4.3 8 2 D 1 1 0 1.6 2.6 3.5 9 1 M 1 0 0 4.0 4.4 3.6 9 2 D 1 0 1 1.8 2.5 3.1 10 1 M 1 1 1 3.0 3.7 4.3 10 2 D 1 1 1 2.2 2.0 3.3 11 1 M 1 1 1 4.3 5.0 3.7 11 2 D 1 1 1 3.3 2.5 3.2 12 1 M 1 1 1 3.5 5.4 4.7 12 2 D 1 1 1 3.5 3.6 4.2 13 1 M 1 0 0 4.1 4.5 3.2 13 2 D 1 0 0 3.7 4.2 3.5 14 1 M 1 1 0 5.0 4.2 3.6 14 2 D 1 0 0 3.3 4.6 3.0 15 1 M 1 1 0 1.8 2.2 2.3 15 2 D 1 0 0 2.4 3.5 3.6 16 1 M 1 0 0 3.1 2.5 3.9 16 2 D 1 1 0 2.0 2.9 2.4 17 1 M 1 1 0 3.4 5.5 4.7 17 2 D 1 0 1 3.2 4.3 3.7 18 1 M 1 0 0 3.4 5.3 4.1 18 2 D 1 1 0 1.8 3.4 3.1 19 1 M 1 0 0 3.5 3.3 5.1 19 2 D 1 0 0 2.8 4.3 3.4 20 1 M 1 0 0 3.5 3.3 5.1 20 2 D 1 0 0 3.2 4.9 3.6 21 1 M 1 0 0 2.9 2.7 3.5 21 2 D 1 0 0 4.3 3.7 2.5 22 1 M 1 0 0 4.8 4.3 4.3 22 2 D 1 0 0 4.5 3.8 3.3 23 1 M 1 1 0 3.6 3.9 3.7 23 2 D 1 0 0 3.7 3.7 3.5 23 3 D 1 1 0 2.5 1.8 2.3 24 1 M 1 1 0 5.0 4.4 4.2 24 2 D 1 1 0 4.9 3.2 2.5 24 3 D 1 0 0 2.7 3.1 4.1 24 4 D 1 0 1 3.0 3.5 3.6 ; data longform; set dyads; array d[3] depression1-depression3; array q[3] qol1-qol3; do visit=1 to 3; depression=d[visit]; qol_score=q[visit]; output; end; keep family individual relation depression visit qol_score; run; ## Check the Goodness-of-Fit Tests for Normal Distribution ## The tests support normality, and the histogram shows a roughly bell-shaped curve. proc univariate; var qol_score; histogram/normal; run; ## Fit the model proc mixed covtest; class family individual relation(ref=&#39;D&#39;) depression; model qol_score=relation depression visit/solution; random intercept visit/subject=family type=un; random intercept visit/subject=individual(family) type=un; run; 22.5.3 R Implementation library(&quot;lme4&quot;) load(&#39;./01_Datasets/nurses.RData&#39;) nurses_hierarchical = lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + (1 | hospital) + (1 | hospital:ward), data = nurses ) summary(nurses_hierarchical) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: stress ~ age + sex + experience + treatment + wardtype + hospsize + ## (1 | hospital) + (1 | hospital:ward) ## Data: nurses ## ## REML criterion at convergence: 1640 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.62082 -0.69562 -0.00134 0.66513 2.75800 ## ## Random effects: ## Groups Name Variance Std.Dev. ## hospital:ward (Intercept) 0.3366 0.5801 ## hospital (Intercept) 0.1194 0.3455 ## Residual 0.2174 0.4662 ## Number of obs: 1000, groups: hospital:ward, 100; hospital, 25 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 5.379585 0.184686 46.055617 29.128 &lt; 2e-16 *** ## age 0.022114 0.002200 904.255527 10.053 &lt; 2e-16 *** ## sexFemale -0.453184 0.034991 906.006740 -12.952 &lt; 2e-16 *** ## experience -0.061653 0.004475 908.841522 -13.776 &lt; 2e-16 *** ## treatmentTraining -0.699851 0.119767 72.776723 -5.843 1.34e-07 *** ## wardtypespecial care 0.050807 0.119767 72.775457 0.424 0.67266 ## hospsizemedium 0.489400 0.201547 21.963644 2.428 0.02382 * ## hospsizelarge 0.901527 0.274824 22.015667 3.280 0.00342 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) age sexFml exprnc trtmnT wrdtyc hspszm ## age -0.172 ## sexFemale -0.142 -0.014 ## experience -0.002 -0.817 0.028 ## trtmntTrnng -0.326 0.005 -0.004 0.000 ## wrdtypspclc -0.324 -0.007 -0.002 0.008 0.000 ## hospsizemdm -0.624 0.001 -0.002 0.000 0.000 0.000 ## hospsizelrg -0.458 -0.001 0.004 0.002 0.000 0.000 0.419 01_Datasets "]]
