[["logistic-regression.html", "Chapter 16 Logistic Regression 16.1 Introduction 16.2 Logit Modell 16.3 Illustration in SAS 16.4 Probit Modell 16.5 Complementary log-log-Modell 16.6 Multi-category logit model 16.7 Ordinal Cumulative Logit Model 16.8 Adjacent Categories Model 16.9 Continuation Ratio Model", " Chapter 16 Logistic Regression 16.1 Introduction Use probability-based linear models to predict qualitative response variables, three methods: Logistic regression Linear discriminant analysis Multivariate adaptive regression spline 16.1.1 Violation of assumptions of Ordinary least squares (OLS) The basic assumptions of OLS regression \\(y_{i}=\\alpha+\\beta x_{i}+\\varepsilon_{i} \\mid\\) \\(\\mathrm{E}\\left(\\varepsilon_{i}\\right)=0\\) \\(\\operatorname{var}\\left(\\varepsilon_{i}\\right)=\\sigma^{2}\\) \\(\\operatorname{cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0\\) \\(\\varepsilon_{i} \\sim\\) Normal Normal residuals assumption Assuming y is a dichotomy, the possible values are 1 or 0. Assume yi = 1. Then hypothesis 1 means i = 1xi. On the other hand, if yi = 0, we have i = xi. Since i can only take two values, it is impossible to have a normal distribution Consistant variance assumption \\[E\\left(y_{i}\\right)=1 \\times \\operatorname{Pr}\\left(y_{i}=1\\right)+0 \\times \\operatorname{Pr}\\left(y_{i}=0\\right)\\] If we define \\(p i=\\operatorname{Pr}(y i=1)\\),Then \\[E\\left(y_{i}\\right)=p_{i}\\] \\[ \\begin{array}{c} E\\left(y_{i}\\right)=E\\left(\\alpha+\\beta x_{i}+\\varepsilon_{i}\\right) =E(\\alpha)+E\\left(\\beta x_{i}\\right)+E\\left(\\varepsilon_{i}\\right) =\\alpha+\\beta x_{i} \\end{array} \\] Putting these two results together, we get \\[ \\begin{array}{c} p_{i}=\\alpha+\\beta x_{i} \\\\ \\operatorname{var}\\left(\\varepsilon_{i}\\right)=p_{i}\\left(1-p_{i}\\right)=\\left(\\alpha+\\beta x_{i}\\right)\\left(1-\\alpha-\\beta x_{i}\\right) \\end{array} \\] For different observations, the variance of \\(_i\\) must be different, especially as it changes with changes in x. When pi = 0.5, the disturbance variance is the largest, and when pi is close to 1 or 0, the disturbance variance becomes smaller. Problems If the sample is quite large, the normality assumption is not required. The central limit theorem assures us that even if  is not normally distributed, the coefficient estimates will have an approximately normal distribution. This means that we can still use ordinary tables to calculate p-values and confidence intervals. However, if the sample is small, these approximations may be poor. Violation of the homoscedasticity assumption has two undesirable consequences. First, the coefficient estimates are no longer effective. In statistical terms, this means that there are other selection methods with smaller standard errors. the standard error estimates are no longer consistent estimates of the true standard errors. That means that the estimated standard errors could be biased (either upward or downward) to unknown degrees. And because the standard errors are used in calculating test statistics, the test statistics could also be problematic. Heteroscedasticity consistent covariance estimator sandwich Even if the homogeneity assumption is violated, this method will produce a consistent estimate of the standard error. To implement this method in PROC REG, just put the option HCC on the MODEL statement PROC REG DATA=penalty; MODEL death=blackd whitvic serious / HCC; RUN; 16.1.2 More fundamental problem outside [0,1] For Linear probability model \\(p_{i}=\\alpha+\\beta x_{i}\\), If x has no upper or lower limit, then for any value of , there is a value of x whose pi is greater than 1 or less than 0. 16.1.3 Logistic Regression Model Probability is bounded by 0 and 1, while linear functions are inherently unbounded. The solution is to convert the probability so that it is no longer restricted. Converting probabilities to odds eliminates the upper limit. For k explanatory variables \\[\\log \\left[\\frac{p_{i}}{1-p_{i}}\\right]=\\alpha+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\ldots+\\beta_{k} x_{i k}\\] \\[p_{i}=\\frac{\\exp \\left(\\alpha+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\ldots+\\beta_{k} x_{i k}\\right)}{1+\\exp \\left(\\alpha+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\ldots+\\beta_{k} x_{i k}\\right)}\\] \\[p_{i}=\\frac{1}{1+\\exp \\left(-\\alpha-\\beta_{1} x_{i 1}-\\beta_{2} x_{i 2}-\\ldots-\\beta_{k} x_{i k}\\right)}\\] There is no random disturbance term in the equation of the logic model. This does not mean that the model is deterministic, because there is still room for random variation in the probability relationship between pi and yi. 16.1.4 Estimation of the Logistic Model ordinary least squares, weighted least squares, maximum likelihood. Pii OLSPlogitlog [P/1-P] WLS ML  ML  1. ML 2. ML.  12 Consistency means that as the sample size gets larger the probability that the estimate is within some small distance of the true value also gets larger. No matter how small the distance or how high the specified probability, there is always a sample size that yields an even higher probability that the estimator is within that distance of the true value. One implication of consistency is that the ML estimator is approximately unbiased in large samples. Asymptotic efficiency means that, in large samples, the estimates will have standard errors that are, approximately, at least as small as those for any other estimation method. And, finally, the sampling distribution of the estimates will be approximately normal in large samples, which means that you can use the normal and chi-square distributions to compute confidence intervals and p-values. All these approximations get better as the sample size gets larger. The fact that these desirable properties have only been proven for large samples does not mean that ML has bad properties for small samples. It simply means that we usually dont know exactly what the small-sample properties are. And in the absence of attractive alternatives, researchers routinely use ML estimation for both large and small samples. Maximum Likelihood Estimation We have data for n individuals (i = 1, , n), and these individuals are considered statistically independent. For each i, the data consists of yi and xi, where yi is a random variable with possible values 0 and 1, and xi = [1 xi1xik] is a vector of explanatory variables (1 is the intercept).) Let pi The probability of yi = 1 \\[p_{i}=\\frac{1}{1+e^{-\\boldsymbol{\\beta} \\mathbf{x}_{i}}}\\] The likelihood of observing the values of \\(y\\) for all the observations can be written as \\[L=\\operatorname{Pr}\\left(y_{1}, y_{2,} \\ldots, y_{n}\\right)\\] Because we are assuming that observations are independent, the overall probability of observing all the \\(y_{i}, \\mathrm{~s}\\) can be factored into the product of the individual probabilities: \\[ L=\\operatorname{Pr}\\left(y_{1}\\right) \\operatorname{Pr}\\left(y_{2}\\right) \\ldots \\operatorname{Pr}\\left(y_{n}\\right)=\\prod_{i=1}^{n} \\operatorname{Pr}\\left(y_{i}\\right) \\] By definition, \\(\\operatorname{Pr}\\left(y_{i}=1\\right)=p_{i}\\) and \\(\\operatorname{Pr}\\left(y_{i}=0\\right)=1-p_{i} .\\) That implies that we can write \\[ \\begin{array}{c} \\operatorname{Pr}\\left(y_{i}\\right)=p_{i}^{y_{i}}\\left(1-p_{i}\\right)^{1-y_{i}} \\\\ L=\\prod_{i=1}^{n} p_{i}^{y_{i}}\\left(1-p_{i}\\right)^{1-y_{i}}=\\prod_{i=1}^{n}\\left(\\frac{p_{i}}{1-p_{i}}\\right)^{y_{i}}\\left(1-p_{i}\\right) . \\end{array} \\] At this point we take the logarithm of both sides of the equation to get \\[ \\log L=\\sum_{i} y_{i} \\log \\left(\\frac{p_{i}}{1-p_{i}}\\right)+\\sum_{i} \\log \\left(1-p_{i}\\right) \\] And for equation \\[ \\log L=\\sum_{i} \\boldsymbol{\\beta} \\mathbf{x}_{i} y_{i}-\\sum_{i} \\log \\left(1+e^{\\boldsymbol{\\beta} \\mathbf{x}_{i}}\\right) \\] Taking the derivative of equation and setting it equal to 0 gives us: \\[ \\begin{aligned} \\frac{\\partial \\log L}{\\partial \\boldsymbol{\\beta}} &amp;=\\sum_{i} \\mathbf{x}_{i} y_{i}-\\sum_{i} \\mathbf{x}_{i}\\left(1+e^{-\\boldsymbol{\\beta} \\mathbf{x}_{i}}\\right)^{-1} \\\\ &amp;=\\sum_{i} \\mathbf{x}_{i} y_{i}-\\sum_{i} \\mathbf{x}_{i} \\hat{y}_{i}=0 \\end{aligned} \\] \\[\\hat{y}_{i}=\\frac{1}{1+e^{-\\beta \\mathbf{x}_{i}}}\\] Newton-Raphson iterative methods There is no clear solution to the equation. Instead, we must rely on iterative methods, which are equivalent to successive approximations to the solution until the approximation converges to the solution. Until the approximation converges to the correct value. Again, there are many different ways to do this. All methods produce the same solution, but they differ in factors such as convergence speed, sensitivity to initial values, and computational difficulty of each iteration. The Newton-Raphson algorithm is one of the most widely used iterative methods. \\[ \\begin{array}{l} \\mathbf{U}(\\boldsymbol{\\beta})=\\frac{\\partial \\log L}{\\partial \\boldsymbol{\\beta}}=\\sum_{i} \\mathbf{x}_{i} y_{i}-\\sum_{i} \\mathbf{x}_{i} \\hat{y}_{i} \\\\ \\mathbf{I}(\\boldsymbol{\\beta})=\\frac{\\partial^{2} \\log L}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^{\\prime}}=-\\sum_{i} \\mathbf{x}_{i} \\mathbf{x}_{i}^{\\prime} \\hat{y}_{i}\\left(1-\\hat{y}_{i}\\right) \\end{array} \\] The Newton-Raphson algorithm is then \\[\\boldsymbol{\\beta}_{j+1}=\\boldsymbol{\\beta}_{j}-\\mathbf{I}^{-1}\\left(\\boldsymbol{\\beta}_{j}\\right) \\mathbf{U}\\left(\\boldsymbol{\\beta}_{j}\\right)\\] We need a set of initial values \\(\\beta_0\\). PROC LOGISTIC starts by setting all slope coefficients to 0. Set the intercept to be equal to log [p /(1-p)], where p is the total proportion of events. These initial values are substituted into the right side of the equation, resulting in the result of the first iteration \\(\\beta_1\\). Then substitute these values into the right side, recalculate the first and second derivatives, and the result is \\(\\beta_2\\) Repeat this process until you get convergence. This means that what is inserted on the right is obtained on the left. In fact, you will never get exactly the same thing, so it is necessary to adopt a convergence criterion to judge whether the proximity is close enough. But since every successful run of PROC LOGISTIC reports Convergence criterion (GCONV=1E-8) satisfied, \\[\\frac{\\mathbf{U}\\left(\\boldsymbol{\\beta}_{j}\\right)^{\\prime} \\mathbf{I}^{-1}\\left(\\boldsymbol{\\beta}_{j}\\right) \\mathbf{U}\\left(\\boldsymbol{\\beta}_{j}\\right)}{\\left|\\log L\\left(\\boldsymbol{\\beta}_{j}\\right)\\right|+.000001}\\] If the number is less than .00000001, convergence is declared and the algorithm stops. 16.1.5 Convergence Problems 10 LOGISTIC25LOGISTICMODELMAXITER =25 Quasi-complete separation of data points detected. 100 Recode the problem variables Collapse categories Exclude cases from the model Retain the model with quasi-complete separation but use likelihood-ratio tests. The reported standard error and Walds chi-square of this variable are also of no avail. Nevertheless, it is still possible to obtain a valid likelihood ratio test, Profile Likelihood confidence interval. PROC LOGISTIC DATA=penalty; WHERE blackd=0; CLASS culp /PARAM=REF; MODEL death(EVENT=&#39;1&#39;) = culp serious / CLPARM=PL ALPHA=.01; RUN; ########################################## Parameter Estimate 99% Confidence Limits culp 1 -15.5467 . -3.2344 16.1.6 Use exact methods. p   Fisher p p PROC LOGISTIC DATA=penalty; WHERE blackd=0; CLASS culp /PARAM=REF; MODEL death(EVENT=&#39;1&#39;) = culp serious; EXACT culp serious / ESTIMATE=BOTH; RUN; These tests are conditional in the sense that they are based on the conditional distribution of the sufficient statistic for each parameter, conditioning on the sufficient statistics for all the other parameters. (The sufficient statistics are the sums of cross products for each x and the binary dependent variable y.) The tests are exact in the same sense that t-statistics are exact in normal-theory linear regression. That is, they are not large sample approximations, and they give the correct probability of getting a result that is at least as extreme as the one observed in the sample, under the null hypothesis that a variable has no effect. 16.1.7 Use penalized likelihood Firth1993Firth   HeinzeSchemper2002  -3.7U \\[\\mathbf{U}(\\boldsymbol{\\beta})=\\frac{\\partial \\log L}{\\partial \\boldsymbol{\\beta}}=\\sum_{i} \\mathbf{x}_{i} y_{i}-\\sum_{i} \\mathbf{x}_{i} \\hat{y}_{i}-\\sum_{i} h_{i} \\mathbf{x}_{i}\\left(.5-\\hat{y}_{i}\\right)\\] In PROC LOGISTIC, the method is implemented with the FIRTH option on the MODEL statement PROC LOGISTIC DATA=penalty; WHERE blackd=0; CLASS culp /PARAM=REF; MODEL death(EVENT=&#39;1&#39;) = culp serious / FIRTH CLPARM=PL; RUN; 16.2 Logit Modell 16.2.1 Introduction The probability \\(\\pi{i}=\\mathrm{P}\\left(y{i}=1 \\mid x{i 1}, \\ldots, x{i k}\\right)\\) and the linear predictor \\(\\eta_{i}=\\beta_{0}+\\beta_{1} x_{i 1}+\\ldots+\\beta_{k} x_{i k}=\\boldsymbol{x}_{i}^{\\prime} \\boldsymbol{\\beta}\\) are linked by a response function ${i}=h({i}) $: For Logit-Modell: \\[\\pi=\\frac{\\exp (\\eta)}{1+\\exp (\\eta)} \\Longleftrightarrow \\log \\frac{\\pi}{1-\\pi}=\\eta\\] For Complementary log-log model: \\[\\pi=1-\\exp (-\\exp (\\eta)) \\quad \\Longleftrightarrow \\quad \\log (-\\log (1-\\pi))=\\eta\\] Interpretation \\[\\frac{\\mathrm{P}\\left(y_{i}=1 \\mid \\boldsymbol{x}_{i}\\right)}{\\mathrm{P}\\left(y_{i}=0 \\mid \\boldsymbol{x}_{i}\\right)}=\\exp \\left(\\beta_{0}\\right) \\cdot \\exp \\left(x_{i 1} \\beta_{1}\\right) \\cdot \\ldots \\cdot \\exp \\left(x_{i k} \\beta_{k}\\right)\\] \\[\\frac{\\mathrm{P}\\left(y_{i}=1 \\mid x_{i 1}, \\ldots\\right)}{\\mathrm{P}\\left(y_{i}=0 \\mid x_{i 1}, \\ldots\\right)} / \\frac{\\mathrm{P}\\left(y_{i}=1 \\mid x_{i 1}+1, \\ldots\\right)}{\\mathrm{P}\\left(y_{i}=0 \\mid x_{i 1}+1, \\ldots\\right)}=\\exp \\left(\\beta_{1}\\right)\\] \\[ \\begin{array}{l} \\beta_{1}&gt;0: \\text {Chance} $\\mathrm{P}\\left(y_{i}=1\\right) / \\mathrm{P}\\left(y_{i}=0\\right) \\text {wird größer},\\\\ \\beta_{1}&lt;0: \\text { Chance } \\mathrm{P}\\left(y_{i}=1\\right) / \\mathrm{P}\\left(y_{i}=0\\right) \\text { wird kleiner, } \\\\ \\beta_{1}=0: \\text { Chance } \\mathrm{P}\\left(y_{i}=1\\right) / \\mathrm{P}\\left(y_{i}=0\\right) \\text { bleibt gleich. } \\end{array} \\] 16.2.2 R Implementation Compare models anova(fit.model1, fit.model2, test = \"Chisq\") Change Referenz relevel(factor(school2$RANK),ref=4) Interpret coefficients Odds exp(coef(fit.reduced)) Predict using new datasets predict(fit.model, newdata = testdata, type = \"response\") CIs using profiled log-likelihood confint(fit.model) CIs using standard errors confint.default(fit.model) VIF statistics library(car), vif(fit.model) Wald Test library(aod) ,wald.test() Log likelihood ratio test logLik() Marginal effects library(mfx), logitmfx ## get summary statistics data(Affairs, package = &quot;AER&quot;) summary(Affairs) ## affairs gender age yearsmarried children ## Min. : 0.000 female:315 Min. :17.50 Min. : 0.125 no :171 ## 1st Qu.: 0.000 male :286 1st Qu.:27.00 1st Qu.: 4.000 yes:430 ## Median : 0.000 Median :32.00 Median : 7.000 ## Mean : 1.456 Mean :32.49 Mean : 8.178 ## 3rd Qu.: 0.000 3rd Qu.:37.00 3rd Qu.:15.000 ## Max. :12.000 Max. :57.00 Max. :15.000 ## religiousness education occupation rating ## Min. :1.000 Min. : 9.00 Min. :1.000 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:14.00 1st Qu.:3.000 1st Qu.:3.000 ## Median :3.000 Median :16.00 Median :5.000 Median :4.000 ## Mean :3.116 Mean :16.17 Mean :4.195 Mean :3.932 ## 3rd Qu.:4.000 3rd Qu.:18.00 3rd Qu.:6.000 3rd Qu.:5.000 ## Max. :5.000 Max. :20.00 Max. :7.000 Max. :5.000 table(Affairs$affairs) ## ## 0 1 2 3 7 12 ## 451 34 17 19 42 38 ## ( /),affairsynaffair ## create binary outcome variable Affairs$ynaffair[Affairs$affairs &gt; 0] &lt;- 1 Affairs$ynaffair[Affairs$affairs == 0] &lt;- 0 Affairs$ynaffair &lt;- factor(Affairs$ynaffair, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;)) table(Affairs$ynaffair) ## ## No Yes ## 451 150 # fit full model fit.full &lt;- glm(ynaffair ~ gender + age + yearsmarried + children + religiousness + education + occupation + rating, data = Affairs, family = &quot;binomial&quot;) summary(fit.full) ## ## Call: ## glm(formula = ynaffair ~ gender + age + yearsmarried + children + ## religiousness + education + occupation + rating, family = &quot;binomial&quot;, ## data = Affairs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5713 -0.7499 -0.5690 -0.2539 2.5191 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.37726 0.88776 1.551 0.120807 ## gendermale 0.28029 0.23909 1.172 0.241083 ## age -0.04426 0.01825 -2.425 0.015301 * ## yearsmarried 0.09477 0.03221 2.942 0.003262 ** ## childrenyes 0.39767 0.29151 1.364 0.172508 ## religiousness -0.32472 0.08975 -3.618 0.000297 *** ## education 0.02105 0.05051 0.417 0.676851 ## occupation 0.03092 0.07178 0.431 0.666630 ## rating -0.46845 0.09091 -5.153 2.56e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 675.38 on 600 degrees of freedom ## Residual deviance: 609.51 on 592 degrees of freedom ## AIC: 627.51 ## ## Number of Fisher Scoring iterations: 4 # p()(0) # fit reduced model fit.reduced &lt;- glm(ynaffair ~ age + yearsmarried + religiousness + rating, data = Affairs, family = binomial()) summary(fit.reduced) ## ## Call: ## glm(formula = ynaffair ~ age + yearsmarried + religiousness + ## rating, family = binomial(), data = Affairs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6278 -0.7550 -0.5701 -0.2624 2.3998 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.93083 0.61032 3.164 0.001558 ** ## age -0.03527 0.01736 -2.032 0.042127 * ## yearsmarried 0.10062 0.02921 3.445 0.000571 *** ## religiousness -0.32902 0.08945 -3.678 0.000235 *** ## rating -0.46136 0.08884 -5.193 2.06e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 675.38 on 600 degrees of freedom ## Residual deviance: 615.36 on 596 degrees of freedom ## AIC: 625.36 ## ## Number of Fisher Scoring iterations: 4 # compare models anova(fit.reduced, fit.full, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: ynaffair ~ age + yearsmarried + religiousness + rating ## Model 2: ynaffair ~ gender + age + yearsmarried + children + religiousness + ## education + occupation + rating ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 596 615.36 ## 2 592 609.51 4 5.8474 0.2108 # (p=0.21)  #  # interpret coefficients coef(fit.reduced) ## (Intercept) age yearsmarried religiousness rating ## 1.93083017 -0.03527112 0.10062274 -0.32902386 -0.46136144 exp(coef(fit.reduced)) ## (Intercept) age yearsmarried religiousness rating ## 6.8952321 0.9653437 1.1058594 0.7196258 0.6304248 ## CIs using profiled log-likelihood confint(fit.reduced) ## 2.5 % 97.5 % ## (Intercept) 0.75404303 3.150622807 ## age -0.07006400 -0.001854759 ## yearsmarried 0.04388142 0.158562400 ## religiousness -0.50637196 -0.155156981 ## rating -0.63741235 -0.288566411 ## CIs using standard errors confint.default(fit.reduced) ## 2.5 % 97.5 % ## (Intercept) 0.73463085 3.127029497 ## age -0.06928747 -0.001254761 ## yearsmarried 0.04337199 0.157873491 ## religiousness -0.50434371 -0.153703999 ## rating -0.63547499 -0.287247895 ##   ## Globale Test library(aod) wald.test(b = coef(fit.reduced), Sigma = vcov(fit.reduced), Terms = 1:4) ## Wald test: ## ---------- ## ## Chi-squared test: ## X2 = 27.4, df = 4, P(&gt; X2) = 1.7e-05 ## the three terms for the levels of rank. wald.test(b = coef(fit.reduced), Sigma = vcov(fit.reduced), Terms = 1:3) ## Wald test: ## ---------- ## ## Chi-squared test: ## X2 = 26.7, df = 3, P(&gt; X2) = 6.9e-06 ## likelihood ratio test ## An indicator to measure the degree of model fit. The test statistic is the residual deviation between the model with predictor variables and the zero model. ## The degree of freedom of the chi-square distribution of the test statistic is equal to the degree of freedom difference between the current model and the zero model (that is, the number of predictors in the model) with(fit.reduced, null.deviance - deviance) # 41.46 ## [1] 60.01915 with(fit.reduced, df.null - df.residual) # degrees of freedom ## [1] 4 with(fit.reduced, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)) # the p-value ## [1] 2.874106e-12 ## called a likelihood ratio test (the deviance residual is -2*log likelihood). logLik(fit.reduced) ## &#39;log Lik.&#39; -307.6789 (df=5) #  #  1~5 # calculate probability of extramariatal affair by marital ratings testdata &lt;- data.frame(rating = c(1, 2, 3, 4, 5), age = mean(Affairs$age), yearsmarried = mean(Affairs$yearsmarried), religiousness = mean(Affairs$religiousness)) testdata$prob &lt;- predict(fit.reduced, newdata = testdata, type = &quot;response&quot;) testdata ## rating age yearsmarried religiousness prob ## 1 1 32.48752 8.177696 3.116473 0.5302296 ## 2 2 32.48752 8.177696 3.116473 0.4157377 ## 3 3 32.48752 8.177696 3.116473 0.3096712 ## 4 4 32.48752 8.177696 3.116473 0.2204547 ## 5 5 32.48752 8.177696 3.116473 0.1513079 # calculate probabilites of extramariatal affair by age testdata &lt;- data.frame(rating = mean(Affairs$rating), age = seq(17, 57, 10), yearsmarried = mean(Affairs$yearsmarried), religiousness = mean(Affairs$religiousness)) testdata$prob &lt;- predict(fit.reduced, newdata = testdata, type = &quot;response&quot;) testdata ## rating age yearsmarried religiousness prob ## 1 3.93178 17 8.177696 3.116473 0.3350834 ## 2 3.93178 27 8.177696 3.116473 0.2615373 ## 3 3.93178 37 8.177696 3.116473 0.1992953 ## 4 3.93178 47 8.177696 3.116473 0.1488796 ## 5 3.93178 57 8.177696 3.116473 0.1094738 16.2.3 SAS Implementation PROC LOGISTIC DATA=penalty; MODEL death(EVENT=&#39;1&#39;)=blackd whitvic serious; RUN; MODELEVENT =1 LOGISTICEVENT =1DEATH0EVENT=11 DEATH DESCENDINGLOGISTICDEATHEVENT =1 *** For Multiplicative Terms in the MODEL Statement; MODEL y = x|x|x; MODEL y = x x*x x*x*x; class CLASSMODELLOGISTICCLASS01CLASSCLASSCLASSCLASS PROC LOGISTIC DATA=penalty; CLASS culp /PARAM=REF; MODEL death(EVENT=&#39;1&#39;) = blackd whitvic culp ; RUN; Change the default reference category (5 in this example) and hope it is the minimum value of CULP instead of the maximum value CLASS culp / PARAM=REF DESCENDING; Particular value, say 3 CLASS culp(REF='3') / PARAM=REF; Confidence Intervals Wald CI: CLPARM = WALD Profile likelihood CI: Can produce better approximations, especially in smaller samples using CLPARM = PL Two confidence intervals: PROC LOGISTIC DATA=penalty; MODEL death(EVENT=&#39;1&#39;) = blackd whitvic culp / CLPARM=BOTH; RUN; Marginal effect For each variable, we obtain the predicted change in the probability of death penalty for each additional unit of the variable according to the predicted probability of the person. Get them easily with PROC QLIM PROC LOGISTIC DATA=penalty; MODEL death(EVENT=&#39;1&#39;)=blackd whitvic serious; RUN; PROC QLIM DATA=penalty; ENDOGENOUS death~DISCRETE(DIST=LOGISTIC); MODEL death = blackd whitvic serious; OUTPUT OUT=a MARGINAL; PROC PRINT DATA=a(OBS=10); VAR meff_p2_blackd meff_p2_whitvic meff_p2_serious; RUN; 16.2.4 Multicollinearity   When none of the individual variables is significant but the entire set is significant, multicollinearity is a likely culprit. How to diagnose multicollinearity PROC CORR PROC REGTOLVIFCOLLINOINTPROC LOGISTIC logitPROC REG PROC REG DATA=penalty; MODEL death = blackd whitvic serious serious2 / TOL VIF; RUN; \\[ \\begin{array}{|l|r|r|r|r|r|r|r|} \\hline \\text { Variable } &amp; \\text { DF } &amp; \\text {Parameter} &amp; \\text { Standard } &amp; \\text { t Value } &amp; \\text { Pr }&gt;\\mid \\text { |t| }&amp; \\text { Tolerance }&amp; \\text { Variance } \\\\ &amp; \\text { } &amp; \\text {Estimate } &amp; \\text {Error } &amp; &amp; &amp; &amp; \\text { Inflation } \\\\ \\hline \\text { Intercept } &amp; 1 &amp; -0.14164 &amp; 0.18229 &amp; -0.78 &amp; 0.4384 &amp; &amp; 0 \\\\ \\hline \\text { blackd } &amp; 1 &amp; 0.12093 &amp; 0.08242 &amp; 1.47 &amp; 0.1445 &amp; 0.85428 &amp; 1.17058 \\\\ \\hline \\text { whitvic } &amp; 1 &amp; 0.05739 &amp; 0.08451 &amp; 0.68 &amp; 0.4982 &amp; 0.84548 &amp; 1.18276 \\\\ \\hline \\text { serious } &amp; 1 &amp; 0.01924 &amp; 0.03165 &amp; 0.61 &amp; 0.5442 &amp; 0.14290 &amp; 6.99788 \\\\ \\hline \\text { serious2 } &amp; 1 &amp; 0.07044 &amp; 0.10759 &amp; 0.65 &amp; 0.5137 &amp; 0.14387 &amp; 6.95081 \\\\ \\hline \\end{array} \\] Davis1986 Weight matrix PROC LOGISTIC DATA=penalty; MODEL death(EVENT=&#39;1&#39;) = blackd whitvic serious serious2; OUTPUT OUT=a PRED=phat; DATA b; SET a; w = phat*(1-phat); PROC REG DATA=b; WEIGHT w; MODEL death = blackd whitvic serious1 serious2 / TOL VIF; RUN; OUTPUTMODELPHATPHATDATAWW  16.2.5 Goodness-of-Fit Statistics Pearson deviance AICSC-2 LogL2 Log L-2 Log L Akaikes Information Criterion (AIC) \\(A I C=-2 \\log L+2 k\\) Schwarz Criterion (SC), also known as the Bayesian Information Criterion (BIC) \\(S C=-2 \\log L+k \\log n\\) deviance deviance is a goodness-of-fit statistic for a statistical model; it is often used for statistical hypothesis testing. It is a generalization of the idea of using the sum of squares of residuals (RSS) in ordinary least squares to cases where model-fitting is achieved by maximum likelihood. \\[{\\displaystyle D(y,{\\hat {\\mu }})=2{\\Big (}\\log {\\big (}p(y\\mid {\\hat {\\theta }}_{s}){\\big )}-\\log {\\big (}p(y\\mid {\\hat {\\theta }}_{0}){\\big )}{\\Big )}.\\,}\\] LOGISTICAGGREGATESCALE, AGGREGATELOGISTIC SCALESCALE = NONELOGISTIC PROC LOGISTIC DATA=penalty; MODEL death(EVENT=&#39;1&#39;) = blackd whitvic culp / AGGREGATE SCALE=NONE; RUN; Saturated mode   PROC LOGISTIC DATA=penalty; CLASS culp; MODEL death(EVENT=&#39;1&#39;) = blackd whitvic culp blackd*whitvic blackd*culp whitvic*culp blackd*whitvic*culp ; RUN; *** The MODEL statement could also be abbreviated; MODEL death(EVENT=&#39;1&#39;) = blackd|whitvic|culp; 16.2.6 Hosmer and Lemeshow Goodness-of-Fit Test Pearson SCALEAGGREGATE   HosmerLemeshow2000 MODELLACKFITLOGISTIC  PROC LOGISTIC DATA=penalty; MODEL death(EVENT=&#39;1&#39;) = blackd whitvic culp / LACKFIT; RUN; Hosmer-LemeshowHL10Pearson2p/ 16.2.7 Statistics Measuring Predictive Power \\(R^2\\) 16.2.7.1 Generalized \\(R^{2}\\) which is the statistic reported by LOGISTIC under the heading Testihg Global Null Hypothesis: BETA=0. If we denote that statistic by \\(L^{2}\\) and let \\(n\\) be the sample size, the generalized \\(R^{2}\\) is \\[ R^{2}=1-\\exp \\left\\{-\\frac{L^{2}}{n}\\right\\} \\] RSQMODEL, \\(\\mathrm{LOGISTIC}\\) R2 , , CoxR2 , R2: - Its based on the quantity being maximized, namely the log-likelihood. ,  - Its invariant to grouping. You get the same \\(\\mathrm{R} 2\\) whether youre working with grouped or individual-level data (as long as cases are only grouped together if theyre identical on all the independent variables). , R2 - Its readily obtained with virtually all computer programs because the loglikelihood is nearly always reported by default. ,  - It never diminishes when you add variables to a model. ,  - The calculated values are usually quite similar to the \\(\\mathrm{R} 2\\) obtained from fitting a linear probability model to dichotomous data by ordinary least squares. R2 R2R2R2 1, LOGISTIC Max-rescaled Rsquare\", R2 (Nagelkerke 1991)  16.2.7.2 McFadden \\(R^{2}\\) McFadden \\(R^{2}\\) ,  \\(L^{2}\\) 0 , \\(\\ell_{0}\\)  \\[ R_{M c F}^{2}=\\frac{L^{2}}{-2 \\log \\ell_{0}} \\] R2, McFadden \\(R^{2}\\) 1McFadden \\(R^{2}\\)  \\(R^{2}\\) ,  MLR2,   ()   16.2.7.3 Tjurs \\(R^{2}\\) Tjur (2009) model free \\(R^{2}\\) ,   \\(\\mathrm{y}=1\\)  \\(\\mathrm{y}=\\) 0Tjur \\(R^{2}\\)  \\(R^{2}\\)  Tjur \\(R^{2}\\) 1. PROC LOGISTIC DATA=penalty; MODEL death(EVENT=&#39;1&#39;)=culp whitvic blackd; OUTPUT OUT=a PRED=yhat; PROC MEANS; CLASS death; VAR yhat; RUN; Whenever you run binary (or ordinal) logistic regression, PROC LOGISTIC reports four such metrics by default. \\[ \\begin{array}{|l|c|l|l|} \\hline \\text { Percent Concordant } &amp; 88.3 &amp; \\text { Somers&#39; } \\mathbf{D} &amp; 0.800 \\\\ \\hline \\text { Percent Discordant } &amp; 8.4 &amp; \\text { Gamma } &amp; 0.827 \\\\ \\hline \\text { Percent Tied } &amp; 3.3 &amp; \\text { Tau-a } &amp; 0.361 \\\\ \\hline \\text { Pairs } &amp; 4850 &amp; \\mathrm{C} &amp; 0.900 \\\\ \\hline \\end{array} \\] 147147146/ 2 = 10731 588110 4850101 0    CDTN  If the answer is yes, we call that pair concordant. If no, the pair is discordant. If the two cases have the same predicted value, we call it a tie. Let C be the number of concordant pairs, D the number of discordant pairs, T the number of ties, and N the total number of pairs (before eliminating any). \\[ \\begin{array}{c} \\text { Tau }-a=\\frac{C-D}{N} \\\\ G a m m a=\\frac{C-D}{C+D} \\\\ \\text { Somer }^{\\prime} s D=\\frac{C-D}{C+D+T} \\\\ c=.5(1+\\text { Somer&#39;s } D) \\end{array} \\] 01 Tau-aR2 cROC 16.2.8 ROC Curves Another way to evaluate the predictive ability of a model for binary outcomes is the ROC curve. ROC is the abbreviation of Receiver Operating Characteristic. Let pi be the predicted probability of yi = 1 for individual i. If you want to use the predicted probability to generate an actual forecast for y = 1, you need some pointcut values. The natural choice is .5. If i p.5, we predict yi =1. If i p &lt;.5, we predict yi = 0. Then we can build the following classification table: \\[ \\begin{array}{l|c|c|c} &amp; \\hat{p}_{i} \\geq .5 &amp; \\hat{p}_{i}&lt;.5 &amp; \\text { Total } \\\\ y_{i}=1 &amp; 35 &amp; 15 &amp; 50 \\\\ y_{i}=0 &amp; 10 &amp; 87 &amp; 97 \\\\ \\hline \\end{array} \\] &gt; 35/50 = .7087/97 = .90ROC110 PROC LOGISTIC DATA=penalty PLOTS(ONLY)=ROC; MODEL death(EVENT=&#39;1&#39;)=blackd whitvic culp ; RUN; The 45-degree line represents the expected ROC curve of a model with only intercept (ie, no predictive power). The farther the curve is from the 45-degree line, the greater the predictive power. The standard statistic for summarizing deviations is the area under the curve 45ROC45 ROC PLOTSPLOTSONLY= ROCID = CUTPOINT (#fig:ROC logistic)Figure: ROC PLOTS(ONLY)= ROC(ID = CUTPOINT) Compare the ROC curve and c statistics of different models. In the following code, I request the ROC curve and c statistics of the basic model and three different sub-models PROC LOGISTIC DATA=penalty; MODEL death(EVENT=&#39;1&#39;)=blackd whitvic culp; ROC &#39;omit culp&#39; blackd whitvic; ROC &#39;omit blackd&#39; whitvic culp; ROC &#39;omit whitvic&#39; blackd culp; ROCCONTRAST / ESTIMATE=ALLPAIRS; RUN; 16.2.9 Predicted Values, Residuals, and Influence Statistics PROC LOGISTIC can use the OUTPUT statement to generate a large number of case statistics, which writes the selected diagnostic statistics into the SAS data set. Here are some statistics that can be selected: Linear predictor- Predicted log-odds for each case. In matrix notation, this is \\(\\mathbf{x} \\boldsymbol{\\beta}\\), so its commonly referred to as XBETA. Standard error of linear predictor-Used in generating confidence intervals. Predicted values-Predicted probability of the event, based on the estimated model and values of the explanatory variables. For grouped data, this is the expected number of events. Confidence intervals for predicted values -Confidence intervals are first calculated for the linear predictor by adding and subtracting an appropriate multiple of the standard error. Then, to get confidence intervals around the predicted values, the upper and lower bounds on the linear predictor are substituted into \\(1 /\\left(1+e^{-x}\\right)\\), where \\(x\\) is either an upper or a lower bound. Deviance residuals - Contribution of each observation to the deviance chi-square. Pearson residuals - Contribution of each observation to the Pearson chi-square. OUTPUT,  , PROC REG ,  LOGISTIC DFBETAS-, DIFDEV- DIFCHISQ \\(\\mathrm{C}\\) and \\(\\mathrm{CBAR}\\) ,  LEVERAGE- OUTPUTPROC PLOTSODS,  INFLUENCEDFBETAS, PHATDPC,   LEVERAGE,  Here is the example PROC LOGISTIC DATA=penalty PLOTS(UNPACK LABEL)= (INFLUENCE DFBETAS PHAT DPC LEVERAGE); MODEL death(EVENT=&#39;1&#39;)=blackd whitvic culp ; RUN; 16.2.10 Unobserved Heterogeneity  is the coefficient disturbance term . The random disturbance term can be regarded as representing all omitted explanatory variables that have nothing to do with the measured x variable, usually referred to as unobserved heterogeneity \\[\\log \\left[\\frac{\\operatorname{Pr}(y=1 \\mid \\varepsilon)}{\\operatorname{Pr}(y=0 \\mid \\varepsilon)}\\right]=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\ldots+\\beta_{k} x_{k}+\\sigma \\varepsilon\\] We assume that  has nothing to do with x and has a standard logarithmic distribution, (Allison 1987) the result is approximately \\[\\log \\left[\\frac{\\operatorname{Pr}(y=1)}{\\operatorname{Pr}(y=0)}\\right]=\\beta_{0}^{*}+\\beta_{1}^{*} x_{1}+\\beta_{2}^{*} x_{2}+\\ldots+\\beta_{k}^{*} x_{k}\\] \\[\\beta_{i}^{*}=\\frac{\\beta_{j}}{\\sqrt{1+\\sigma^{2}}}\\]   logit logit First, in interpreting the results of a logistic regression, we should keep in mind that the magnitudes of the coefficients and the corresponding odds ratios are likely to be conservative to some degree. Second, to keep such shrinkage to a minimum, its always desirable to include important explanatory variables in the modeleven if we think those variables are not correlated with the variables already in the model. This advice is especially relevant to randomized experiments where there is a tendency to ignore covariates and look only at the effect of the treatment on the response. That may be okay for linear models, but logit models yield superior estimates of the treatment effect when all relevant covariates are included. A third problem is that differential heterogeneity can confound comparisons of logit coefficients across different groups. If, for example, you want to compare logit coefficients estimated separately for men and women, you must implicitly assume that the degree of unobserved heterogeneity is the same in both groups. Adjusting for differential heterogeneity using HETERO in Proc QLIM PROC QLIM DATA=promo; ENDOGENOUS promo~DISCRETE(DIST=LOGISTIC); MODEL promo = female dur dur*dur select arts prestige arts*female; HETERO promo~female / NOCONST; run; #################################################### Parameter DF Estimate StandardError t Value Approx Pr &gt; |t| _H.FEMALE 1 0.354838 0.325341 1.09 0.2754 FEMALEp.28 0exp.354838= 1.43 43 16.3 Illustration in SAS PLOTS &lt;(global-plot-options)&gt; =(plot-request &lt;(options)&gt; &lt;plot-request &lt;(options)&gt;&gt;) controls the plots produced through ODS Graphics. When you specify only one plot-request, you can omit the parentheses from around the plot-request. For example: PLOTS = ALL PLOTS = (ROC EFFECT INFLUENCE(UNPACK)) PLOTS(ONLY) = EFFECT(CLBAR SHOWOBS) If the INFLUENCE or IPLOTS option is specified in the MODEL statement, then the INFLUENCE plots are produced unless the MAXPOINTS= cutoff is exceeded. If you specify ROC statements, then an overlaid plot of the ROC curves for the model (or the selected model if a SELECTION= method is specified) and for all the ROC statement models is displayed. If you specify the CLODDS= option in the MODEL statement or if you specify the ODDSRATIO statement, then a plot of the odds ratios and their confidence limits is displayed. 16.3.1 Effects of Predictor Variables  EFFECTPLOT  EFFECTPLOTARTS CLASSARTS / LINKEFFECTPLOT 01  ***  PROC LOGISTIC DATA=promo; MODEL promo(EVENT=&#39;1&#39;) = female dur select arts; EFFECTPLOT FIT(X=arts); run; PROC LOGISTIC DATA=promo; MODEL promo(EVENT=&#39;1&#39;) = female dur select arts arts*arts; EFFECTPLOT FIT(X=arts); run; ## SLICEFIT option to produce curves for men and women: PROC LOGISTIC DATA=promo; MODEL promo(EVENT=&#39;1&#39;) = female dur select arts prestige arts*female ; EFFECTPLOT SLICEFIT(X=arts SLICEBY=female=0 1); run; 16.3.2 Odds ratio plot ods graphics on; proc logistic data=prostate plots(only)=(effect oddsratio (type=horizontalstat)); model capsule (event=&quot;1&quot;) = psa /clodds=both; unit psa = 10; run; ods graphics off; 16.4 Probit Modell 16.4.1 Introduction The latent variable model we examined in the last section should help to motivate these models. The equation for the continuous latent variable z was \\[z=\\alpha_{0}+\\alpha_{1} x_{1}+\\alpha_{2} x_{2}+\\ldots+\\alpha_{k} x_{k}+\\sigma \\varepsilon\\] * logisticyxlogit * yx * Gumbellog-log The probability \\(\\pi{i}=\\mathrm{P}\\left(y{i}=1 \\mid x{i 1}, \\ldots, x{i k}\\right)\\) and the linear predictor \\(\\eta_{i}=\\beta_{0}+\\beta_{1} x_{i 1}+\\ldots+\\beta_{k} x_{i k}=\\boldsymbol{x}_{i}^{\\prime} \\boldsymbol{\\beta}\\) are linked by a response function ${i}=h({i}) $: For Logit-Modell: \\[\\pi=\\frac{\\exp (\\eta)}{1+\\exp (\\eta)} \\Longleftrightarrow \\log \\frac{\\pi}{1-\\pi}=\\eta\\] For Probit-Modell: \\[\\pi=\\Phi(\\eta) \\Longleftrightarrow \\Phi^{-1}(\\pi)=\\eta\\] \\[\\pi=\\Phi(\\eta)=\\Phi\\left(\\boldsymbol{x}^{\\prime} \\boldsymbol{\\beta}\\right)\\] So the probit model is often written as \\[\\Phi^{-1}\\left(p_{i}\\right)=\\beta_{0}+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\ldots+\\beta_{k} x_{i k}\\] 16.4.2 R Implemetation mydata &lt;- read.csv(&quot;./01_Datasets/binary.csv&quot;) mydata$rank &lt;- factor(mydata$rank) xtabs(~rank + admit, data = mydata) ## admit ## rank 0 1 ## 1 28 33 ## 2 97 54 ## 3 93 28 ## 4 55 12 myprobit &lt;- glm(admit ~ gre + gpa + rank, family = binomial(link = &quot;probit&quot;), data = mydata) summary(myprobit) ## ## Call: ## glm(formula = admit ~ gre + gpa + rank, family = binomial(link = &quot;probit&quot;), ## data = mydata) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6163 -0.8710 -0.6389 1.1560 2.1035 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.386836 0.673946 -3.542 0.000398 *** ## gre 0.001376 0.000650 2.116 0.034329 * ## gpa 0.477730 0.197197 2.423 0.015410 * ## rank2 -0.415399 0.194977 -2.131 0.033130 * ## rank3 -0.812138 0.208358 -3.898 9.71e-05 *** ## rank4 -0.935899 0.245272 -3.816 0.000136 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 458.41 on 394 degrees of freedom ## AIC: 470.41 ## ## Number of Fisher Scoring iterations: 4 confint(myprobit) ## 2.5 % 97.5 % ## (Intercept) -3.7201050682 -1.076327713 ## gre 0.0001104101 0.002655157 ## gpa 0.0960654793 0.862610221 ## rank2 -0.7992113929 -0.032995019 ## rank3 -1.2230955861 -0.405008112 ## rank4 -1.4234218227 -0.459538829 ## Wald test l &lt;- cbind(0, 0, 0, 1, -1, 0) wald.test(b = coef(myprobit), Sigma = vcov(myprobit), L = l) ## Wald test: ## ---------- ## ## Chi-squared test: ## X2 = 5.6, df = 1, P(&gt; X2) = 0.018 wald.test(b = coef(myprobit), Sigma = vcov(myprobit), Terms = 4:6) ## Wald test: ## ---------- ## ## Chi-squared test: ## X2 = 21.4, df = 3, P(&gt; X2) = 8.9e-05 16.4.3 SAS Implementation PROC LOGISTIC DATA=penalty; MODEL death(EVENT=&#39;1&#39;) = culp blackd whitvic / LINK=PROBIT STB; RUN; For logit and probit models, the chi-square, p-value, and standardized coefficient are usually very close, although they will never be exactly the same. On the other hand, compared with the logit model, the parameter estimates and standard errors of the probit model are much lower. 16.5 Complementary log-log-Modell The complementary log-log model has important applications in the field of survival analysis. The probability \\(\\pi{i}=\\mathrm{P}\\left(y{i}=1 \\mid x{i 1}, \\ldots, x{i k}\\right)\\) and the linear predictor \\(\\eta_{i}=\\beta_{0}+\\beta_{1} x_{i 1}+\\ldots+\\beta_{k} x_{i k}=\\boldsymbol{x}_{i}^{\\prime} \\boldsymbol{\\beta}\\) are linked by a response function ${i}=h({i}) $: For Logit-Modell: \\[\\pi=\\frac{\\exp (\\eta)}{1+\\exp (\\eta)} \\Longleftrightarrow \\log \\frac{\\pi}{1-\\pi}=\\eta\\] For Complementary log-log model: \\[\\pi=1-\\exp (-\\exp (\\eta)) \\quad \\Longleftrightarrow \\quad \\log (-\\log (1-\\pi))=\\eta\\] \\[\\log \\left[-\\log \\left(1-p_{i}\\right)\\right]=\\beta_{0}+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\ldots+\\beta_{k} x_{i k}\\] \\[p_{i}=1-\\exp \\left\\{-\\exp \\left[\\beta_{0}+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\ldots+\\beta_{k} x_{i k}\\right]\\right\\}\\] logitprobitlogitprobitp = .50log-logS10 CoxCox 1972 Weibull  \\[\\log h(t)=\\alpha(t)+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\ldots+\\beta_{k} x_{i k}\\] where \\(h(t)\\) is something called the hazard of an event at time \\(t\\) and \\(\\alpha(t)\\) is an unspecified function of time. 16.5.1 R Implemetation 16.6 Multi-category logit model 16.6.1 Multinomialverteilung \\[ f(\\boldsymbol{y} \\mid \\boldsymbol{\\pi})=\\pi_{1}^{y_{1}} \\cdot \\ldots \\cdot \\pi_{q}^{y_{q}}\\left(1-\\pi_{1}-\\ldots-\\pi_{q}\\right)^{1-y_{1}-\\ldots-y_{q}} \\] given in generalization of the Bernoulli distribution. For m independent repetitions, in generalization to the binomial distribution,\\(y r, r=1, \\ldots, c\\) now the number of repetitions in which the category r occurred. Then \\(\\boldsymbol{y}=\\left(y 1, \\ldots, y_{q}\\right)^{\\prime}\\) has the probability function \\[ \\begin{aligned} f(\\boldsymbol{y} \\mid \\boldsymbol{\\pi}) &amp;=\\frac{m !}{y_{1} ! \\cdot \\ldots \\cdot y_{q} !\\left(m-y_{1}-\\ldots-y_{q}\\right) !} \\pi_{1}^{y_{1}} \\cdot \\ldots \\cdot \\pi_{q}^{y_{q}}\\left(1-\\pi_{1}-\\ldots-\\pi_{q}\\right)^{1-y_{1}-\\ldots-y_{q}} \\\\ &amp;=\\frac{m !}{y_{1} ! \\cdot \\ldots \\cdot y_{c} !} \\pi_{1}^{y_{1}} \\cdot \\ldots \\cdot \\pi_{c}^{y_{c}} \\end{aligned} \\] a multinomial distribution, in short \\[ \\boldsymbol{y} \\sim M(m, \\boldsymbol{\\pi}) \\] mit den Parametern \\(\\mathrm{m}\\) und \\(\\boldsymbol{\\pi}=(\\pi 1, \\ldots, \\pi q)^{\\prime}\\). Für die ersten beiden Momente ergibt sich \\[ \\mathrm{E}(\\boldsymbol{y})=m \\boldsymbol{\\pi}=\\left(\\begin{array}{c} m \\pi_{1} \\\\ \\vdots \\\\ m \\pi_{q} \\end{array}\\right), \\quad \\operatorname{Cov}(\\boldsymbol{y})=\\left(\\begin{array}{ccc} m \\pi_{1}\\left(1-\\pi_{1}\\right) &amp; \\cdots &amp; -\\pi_{1} \\pi_{q} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ -\\pi_{q} \\pi_{1} &amp; \\cdots &amp; m \\pi_{q}\\left(1-\\pi_{q}\\right) \\end{array}\\right) \\] ### Mehrkategoriales Logit-Modell Multicategory logit models are an extension of logistic regression models in which the response can be classified into three or more categories. When the categories have no natural ordering (e.g. car style preference), a baseline-category logit model is appropriate. To simplify the explanation, let us consider a single predictor. The model takes the following form, where the response can be classified into one of \\(J\\), categories: \\(\\operatorname{lng}\\left(\\frac{\\pi_{j}}{\\pi_{J}}\\right)=\\alpha+\\beta_{j} x\\) where \\(j=0,1, \\ldots J-1\\). (The reference category is typically taken to be category \\(J\\) ). The logit link for this model is then \\(\\log \\left(\\frac{\\pi_{j}}{\\pi_{J}}\\right)\\). Given that the response is classified in category \\(j\\) or category \\(J\\), this represents the log odds that the response is \\(j\\). Let us consider a total of 4 possible response levels from 0 to 3 . For \\(J=4\\), the model uses: \\[ \\log \\left(\\frac{\\pi_{0}}{\\pi_{3}}\\right), \\log \\left(\\frac{\\pi_{1}}{\\pi_{3}}\\right) \\text {, and } \\log \\left(\\frac{\\pi_{2}}{\\pi_{3}}\\right) \\] Odds are then calculated for each response level using the highest category as the reference category (in the example this would be category 4\\()\\). For each response level, separate parameter estimates are provided for each predictor in the model. When several predictors are used, the number of parameters in the model can grow large and become difficult to manage. More details: \\[\\mathrm{P}\\left(Y_{i}=r \\mid \\boldsymbol{x}_{i}\\right)=\\pi_{i r}=\\frac{\\exp \\left(\\boldsymbol{x}_{i}^{\\prime} \\boldsymbol{\\beta}_{r}\\right)}{1+\\sum_{s=1}^{q} \\exp \\left(\\boldsymbol{x}_{i}^{\\prime} \\boldsymbol{\\beta}_{s}\\right)} \\quad r=1, \\ldots, q\\] \\[\\log \\frac{\\pi_{i r}}{\\pi_{i c}}=\\boldsymbol{x}_{i}^{\\prime} \\boldsymbol{\\beta}_{r} \\quad b z w \\cdot \\frac{\\pi_{i r}}{\\pi_{i c}}=\\exp \\left(\\boldsymbol{x}_{i}^{\\prime} \\boldsymbol{\\beta}_{r}\\right), \\quad r=1, \\ldots, c\\] 16.6.2 R Implementation library(foreign) ml &lt;- read.dta(&quot;./01_Datasets/hsbdemo.dta&quot;) with(ml, table(ses, prog)) ## prog ## ses general academic vocation ## low 16 19 12 ## middle 20 44 31 ## high 9 42 7 with(ml, do.call(rbind, tapply(write, prog, function(x) c(M = mean(x), SD = sd(x))))) ## M SD ## general 51.33333 9.397775 ## academic 56.25714 7.943343 ## vocation 46.76000 9.318754 ## Multinomial logistic regression library(nnet) ml$prog2 &lt;- relevel(ml$prog, ref = &quot;academic&quot;) test &lt;- multinom(prog2 ~ ses + write, data = ml) ## # weights: 15 (8 variable) ## initial value 219.722458 ## iter 10 value 179.982880 ## final value 179.981726 ## converged summary(test) ## Call: ## multinom(formula = prog2 ~ ses + write, data = ml) ## ## Coefficients: ## (Intercept) sesmiddle seshigh write ## general 2.852198 -0.5332810 -1.1628226 -0.0579287 ## vocation 5.218260 0.2913859 -0.9826649 -0.1136037 ## ## Std. Errors: ## (Intercept) sesmiddle seshigh write ## general 1.166441 0.4437323 0.5142196 0.02141097 ## vocation 1.163552 0.4763739 0.5955665 0.02221996 ## ## Residual Deviance: 359.9635 ## AIC: 375.9635 ## extract the coefficients from the model and exponentiate exp(coef(test)) ## (Intercept) sesmiddle seshigh write ## general 17.32582 0.5866769 0.3126026 0.9437172 ## vocation 184.61262 1.3382809 0.3743123 0.8926116 ## does not include the calculation of the p-value of the regression coefficient, so the Wald test (here z-test) is used to calculate the p-value ## 2-tailed z test z &lt;- summary(test)$coefficients/summary(test)$standard.errors p &lt;- (1 - pnorm(abs(z), 0, 1)) * 2 # Use a fitting function to calculate the probability of each result level head(fitted(test)) ## academic general vocation ## 1 0.1482764 0.3382454 0.5134781 ## 2 0.1202017 0.1806283 0.6991700 ## 3 0.4186747 0.2368082 0.3445171 ## 4 0.1726885 0.3508384 0.4764731 ## 5 0.1001231 0.1689374 0.7309395 ## 6 0.3533566 0.2377976 0.4088458 Interpretation \\[\\begin{aligned} ln\\left(\\frac{P(prog=general)}{P(prog=academic)}\\right) = b_{10} + b_{11}(ses=2) + b_{12}(ses=3) + b_{13}write\\\\ ln\\left(\\frac{P(prog=vocation)}{P(prog=academic)}\\right) = b_{20} + b_{21}(ses=2) + b_{22}(ses=3) + b_{23}write \\end{aligned}\\] ### SAS Implementation The unordered multinomial model is invoked by the LINK=GLOGIT option. If this option is omitted, LOGISTIC estimates the cumulative logit, which assumes that the response levels are ordered. PROC LOGISTIC DATA=wallet; MODEL wallet = male business punish explain / LINK=GLOGIT; RUN; There are three types of wallet, (1) keep the wallet and the money, (2) keep the money and return the wallet, or (3) return both the wallet and the money. The default setting category 3 is the reference category. You can specify category 2 as the reference category MODEL wallet(REF='2') = male business punish explain / LINK=GLOGIT; Test the null hypothesis using TEST PROC LOGISTIC DATA=wallet; MODEL wallet = male business punish explain / LINK=GLOGIT; TEST male_1=male_2, business_1=business_2, punish_1=punish_2, explain_1=explain_2; RUN; Goodness-of-fit statistics using the AGGREGATE and SCALE=NONE options PROC LOGISTIC DATA=wallet; MODEL wallet = male business punish explain / LINK=GLOGIT AGGREGATE SCALE=NONE; OUTPUT OUT=predicted PREDPROBS=I; RUN; OUTPUTPREDICTED 195 PREDPROBS = IIP_11IP_22IP_33  PROC TABULATE PROC TABULATE DATA=predicted; CLASS male business punish explain; var IP_1 IP_2 IP_3; TABLE male*business*punish*explain, IP_1 IP_2 IP_3; RUN; (#fig:Logistic AGGREGATE)Figure: Goodness-of-fit statistics Visualizing the effect of a variable NOLIMITS option suppresses the 95% confidence bands around the plots PROC LOGISTIC DATA=wallet; MODEL wallet = male business punish explain / LINK=GLOGIT; EFFECTPLOT FIT(X=punish) / NOOBS NOLIMITS GRIDSIZE=3; RUN; 16.7 Ordinal Cumulative Logit Model When there is a natural ordering to the response levels, a cumulative logit model is appropriate. An example of a natural ordering would be a score on a 5-point score range or the educational attainment of adults ranging from high school through post-graduate. This logit models the curnulative probability that a response ( \\(Y\\) can be classified at or below a given category. In its simplest form, it assumes that the same proportionality constant applies for each parameter estimate to each cumulative level (known as a cumulative logit model with proportional odds). As such, it assumes that, controlling for all other predictors in the model, the change in odds between a predictor value of \\((x+1)\\) versus \\(x\\) is the same at each response level. This difference is proportional to the distance between the explanatory variables, and the difference is the same no matter which response function is used (independent of the response level considered). This is also known as the equal slopes assumption or the parallel lines assumption (Derr 2013). This model has the following form: \\[ \\operatorname{logit}[P(Y \\leq j)]=\\alpha_{j}+\\beta x, \\text { where } \\mathrm{j}=0 \\text { to J - } 1 \\] The cumulative logit link is the following: \\[\\operatorname{logit}[P(Y \\leq j)]=\\log \\left[\\frac{P(Y \\leq j)}{1-P(Y \\leq j)}\\right]=\\log \\left[\\frac{\\pi_{0}+\\ldots \\pi_{j}}{\\pi_{j+1}+\\ldots \\pi_{j}}\\right]\\] where \\(\\mathrm{j}=0\\) to J \\(-1\\) Note that the highest category is not in the model, as the cumulative probability of the highest category is always 1 . Proportional odds assumption The proportional odds assumption is that the number added to each of these logarithms to get the next is the same in every case. In other words, these logarithms form an arithmetic sequence.[2] The model states that the number in the last column of the tablethe number of times that that logarithm must be addedis some linear combination of the other observed variables. Ordinal logistic regression assumes that the coefficients that describe the relationship between the lowest versus all higher categories of the response variable 16.7.1 R Implementation require(foreign) require(ggplot2) require(MASS) require(Hmisc) require(reshape2) dat &lt;- read.dta(&quot;./01_Datasets/ologit.dta&quot;) ## three way cross tabs (xtabs) and flatten the table ftable(xtabs(~ public + apply + pared, data = dat)) ## pared 0 1 ## public apply ## 0 unlikely 175 14 ## somewhat likely 98 26 ## very likely 20 10 ## 1 unlikely 25 6 ## somewhat likely 12 4 ## very likely 7 3 ## gpa ggplot(dat, aes(x = apply, y = gpa)) + geom_boxplot(size = .75) + geom_jitter(alpha = .5) + facet_grid(pared ~ public, margins = TRUE) + theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))+ theme_bw() ## fit ordered logit model and store results &#39;m&#39; ## proportional odds logistic regression, m &lt;- polr(apply ~ pared + public + gpa, data = dat, Hess=TRUE) ## Hess = TRUEHessian ##  summary(m) ## Call: ## polr(formula = apply ~ pared + public + gpa, data = dat, Hess = TRUE) ## ## Coefficients: ## Value Std. Error t value ## pared 1.04769 0.2658 3.9418 ## public -0.05879 0.2979 -0.1974 ## gpa 0.61594 0.2606 2.3632 ## ## Intercepts: ## Value Std. Error t value ## unlikely|somewhat likely 2.2039 0.7795 2.8272 ## somewhat likely|very likely 4.2994 0.8043 5.3453 ## ## Residual Deviance: 717.0249 ## AIC: 727.0249 ## calculate and store p values ctable &lt;- coef(summary(m)) p &lt;- pnorm(abs(ctable[, &quot;t value&quot;]), lower.tail = FALSE) * 2 ctable &lt;- cbind(ctable, &quot;p value&quot; = p) ## combined table ctable ## Value Std. Error t value p value ## pared 1.04769010 0.2657894 3.9418050 8.087072e-05 ## public -0.05878572 0.2978614 -0.1973593 8.435464e-01 ## gpa 0.61594057 0.2606340 2.3632399 1.811594e-02 ## unlikely|somewhat likely 2.20391473 0.7795455 2.8271792 4.696004e-03 ## somewhat likely|very likely 4.29936315 0.8043267 5.3452947 9.027008e-08 ## KI und odds ratios ci &lt;- confint(m) ## default method gives profiled CIs confint.default(m) ## CIs assuming normality ## 2.5 % 97.5 % ## pared 0.5267524 1.5686278 ## public -0.6425833 0.5250119 ## gpa 0.1051074 1.1267737 exp(coef(m)) ## pared public gpa ## 2.8510579 0.9429088 1.8513972 exp(cbind(OR = coef(m), ci)) ## OR 2.5 % 97.5 % ## pared 2.8510579 1.6958376 4.817114 ## public 0.9429088 0.5208954 1.680579 ## gpa 1.8513972 1.1136247 3.098490 ## Predict and marginal effects ## predicted probabilities ## Use &quot;probs&quot; for predicted probabilities m1.pred &lt;- predict(m, type=&quot;probs&quot;) summary(m1.pred) ## unlikely somewhat likely very likely ## Min. :0.2294 Min. :0.2205 Min. :0.04192 ## 1st Qu.:0.5220 1st Qu.:0.3053 1st Qu.:0.06839 ## Median :0.5806 Median :0.3378 Median :0.08160 ## Mean :0.5498 Mean :0.3498 Mean :0.10040 ## 3rd Qu.:0.6263 3rd Qu.:0.3767 3rd Qu.:0.10123 ## Max. :0.7376 Max. :0.4807 Max. :0.29242 ## marginal effects library(erer) m.marginal &lt;- ocME(m, rev.dum =TRUE) m.marginal ## effect.unlikely effect.somewhat likely effect.very likely ## pared -0.255 0.137 0.117 ## public 0.015 -0.010 -0.005 ## gpa -0.152 0.101 0.051 ## want t and p-values m.marginal$out ## $ME.unlikely ## effect error t.value p.value ## pared -0.255 0.060 -4.237 0.000 ## public 0.015 0.073 0.198 0.843 ## gpa -0.152 0.064 -2.364 0.019 ## ## $`ME.somewhat likely` ## effect error t.value p.value ## pared 0.137 0.029 4.820 0.000 ## public -0.010 0.049 -0.196 0.844 ## gpa 0.101 0.044 2.305 0.022 ## ## $`ME.very likely` ## effect error t.value p.value ## pared 0.117 0.039 3.025 0.003 ## public -0.005 0.024 -0.201 0.841 ## gpa 0.051 0.022 2.301 0.022 ## ## $ME.all ## effect.unlikely effect.somewhat likely effect.very likely ## pared -0.255 0.137 0.117 ## public 0.015 -0.010 -0.005 ## gpa -0.152 0.101 0.051 16.7.2 SAS Implementation cumulative logit modelalso known as the ordered logit or ordinal logit model. It is the most widely used method and the easiest model to use in SAS (default). In PROC LOGISTIC, specifying LINK=CLOGIT produces a model which utilizes a cumulative logit link. Using the default settings, the procedure conducts a Score Test for the Proportional Odds Assumption. If it rejects the null hypothesis that proportional odds may be assumed, the user has two additional modeling techniques utilizing the cumulative logit link. A general cumulative logit model generates separate parameter estimates (effects) for each predictor across all \\(J-1\\) response levels (see Agresti, 2007). This model replaces \\(\\beta\\) in the proportional odds model with \\(\\beta_{j}\\) and uses the same cumulative logit link: \\[ \\operatorname{logit}[P(Y \\leq j)]=\\alpha_{j}+\\beta_{j} x, \\text { where } \\mathrm{j}=0 \\text { to J - } 1 \\] The model allows for probability distribution curves that climb or fall at different rates for different response levels. This raises the possibility of these curves crossing each other at certain predictor values. This is inappropriate, because this violates the order that cumulative probabilities must have: \\(P(Y \\leq X) \\leq P(Y \\leq X+1)\\) for all values of the predictors This result could lead to negative predicted probabilities at individual response levels. Therefore, such a model can only fit adequately over a narrow range of predictor values. Using the proportional odds form of model ensures that the cumulative probabilities have the proper order for all predictor values. If a model of this form is desirable, PROC LOGISTIC will generate a general cumulative logit model if the option UNEQUALSLOPES is specified in the options for the MODEL statement. PROC LOGISTIC DATA=wallet; MODEL wallet = male business punish explain; RUN; Score Test for the Proportional Odds Assumption This is the Chi-Square Score Test for the Proportional Odds Assumption. Since the ordered logit model estimates one equation over all levels of the dependent variable (as compared to the multinomial logit model, which models, assuming low ses is our referent level, an equation for medium ses versus low ses, and an equation for high ses versus low ses), the test for proportional odds tests whether our one-equation model is valid. If we were to reject the null hypothesis, we would conclude that ordered logit coefficients are not equal across the levels of the outcome and we would fit a less restrictive model (i.e., multinomial logit model). If we fail to reject the null hypothesis, we conclude that the assumption holds. For our model, the Proportional Odds Assumption appears to have held. 16.8 Adjacent Categories Model  \\(p_{i j}\\)  \\(i\\)  \\(j\\)   \\(j=1, \\ldots, J\\)  \\(j\\)  \\(j+1\\)  logit: \\[ \\begin{array}{c} \\log \\left(\\frac{p_{i, j+1}}{p_{i j}}\\right)=\\alpha_{j}+\\boldsymbol{\\beta}_{j} \\mathbf{x}_{i} \\quad j=1, \\ldots, J-1 \\\\ \\boldsymbol{\\beta}_{j} \\mathbf{x}_{i}=\\beta_{j 1} x_{i 1}+\\ldots+\\beta_{j k} x_{i k} \\end{array} \\]  \\(\\beta_{j}=\\beta\\) for all \\(j\\) ,  logitPROC LOGISTIC PROC CATMODCATMOD  CATMODFREQWEIGHTALOGITRESPONSE RESPONSEMODELCATMOD , CATMODCLASSLOGISTIC, CATMODPARAM = REF . LOGISTIC, CATMOD,  MARRIEDCLASS10.  PROC CATMOD DATA=happy; WEIGHT count; RESPONSE ALOGIT; MODEL happy = _RESPONSE_ married year / PARAM=REF; RUN; 16.9 Continuation Ratio Model when the ordered categories represent a progression through stages, so that individuals must pass through each lower stage before they go on to higher stages.   Formalize the model for J categories on the dependent variable  \\(A_{i j}\\)  \\(j+1\\)  \\[ \\begin{array}{c} A_{i j}=\\operatorname{Pr}\\left(y_{i}&gt;j \\mid y_{i} \\geq j\\right) \\\\ \\log \\left(\\frac{A_{i j}}{1-A_{i j}}\\right)=\\alpha_{j}+\\boldsymbol{\\beta} \\mathbf{x}_{i} \\quad j=1, \\ldots, J-1 \\end{array} \\] where \\(\\boldsymbol{\\beta} \\mathbf{x}_{i}=\\beta_{1} x_{i 1}+\\ldots+\\beta_{k} x_{i k}\\). \\[ \\log \\left(\\frac{A_{i j}}{1-A_{i j}}\\right)=\\log \\left[\\frac{\\sum_{m=j+1}^{J} p_{i m}}{p_{i j}}\\right] \\] logit   logit \\[ \\begin{array}{lcc|ccc} \\hline &amp; &amp; \\text { Father&#39;s } &amp; \\text { Grammar } &amp; \\text { Some High } &amp; \\text { High School } \\\\ \\text { Race } &amp; \\text { Age } &amp; \\text { Education* } &amp; \\text { School } &amp; \\text { School } &amp; \\text { Graduate } \\\\ \\hline \\text { White } &amp; &lt;22 &amp; 1 &amp; 39 &amp; 29 &amp; 8 \\\\ &amp; &amp; 2 &amp; 4 &amp; 8 &amp; 1 \\\\ &amp; &amp; 3 &amp; 11 &amp; 9 &amp; 6 \\\\ &amp; &amp; 4 &amp; 48 &amp; 17 &amp; 8 \\\\ &amp; \\geq 22 &amp; 1 &amp; 231 &amp; 115 &amp; 51 \\\\ &amp; &amp; 2 &amp; 17 &amp; 21 &amp; 13 \\\\ &amp; &amp; 3 &amp; 18 &amp; 28 &amp; 45 \\\\ &amp; &amp; 4 &amp; 197 &amp; 111 &amp; 35 \\\\ \\text { Black } &amp; &lt;22 &amp; 1 &amp; 19 &amp; 40 &amp; 19 \\\\ &amp; &amp; 2 &amp; 5 &amp; 17 &amp; 7 \\\\ &amp; &amp; 3 &amp; 2 &amp; 14 &amp; 3 \\\\ &amp; \\geq 22 &amp; 4 &amp; 49 &amp; 79 &amp; 24 \\\\ &amp; &amp; 1 &amp; 110 &amp; 133 &amp; 103 \\\\ &amp; &amp; 2 &amp; 18 &amp; 38 &amp; 25 \\\\ &amp; &amp; 3 &amp; 11 &amp; 25 &amp; 18 \\\\ &amp; &amp; 4 &amp; 178 &amp; 206 &amp; 81 \\\\ \\hline \\end{array} \\] DATA afqt; INPUT white old faed ed count @@; DATALINES; 1 0 1 1 39 1 0 1 2 29 1 0 1 3 8 1 0 2 1 4 1 0 2 2 8 1 0 2 3 1 1 0 3 1 11 1 0 3 2 9 1 0 3 3 6 1 0 4 1 48 1 0 4 2 17 1 0 4 3 8 1 1 1 1 231 1 1 1 2 115 1 1 1 3 51 1 1 2 1 17 1 1 2 2 21 1 1 2 3 13 1 1 3 1 18 1 1 3 2 28 1 1 3 3 45 1 1 4 1 197 1 1 4 2 111 1 1 4 3 35 0 0 1 1 19 0 0 1 2 40 0 0 1 3 19 0 0 2 1 5 0 0 2 2 17 0 0 2 3 7 0 0 3 1 2 0 0 3 2 14 0 0 3 3 3 0 0 4 1 49 0 0 4 2 79 0 0 4 3 24 0 1 1 1 110 0 1 1 2 133 0 1 1 3 103 0 1 2 1 18 0 1 2 2 38 0 1 2 3 25 0 1 3 1 11 0 1 3 2 25 0 1 3 3 18 0 1 4 1 178 0 1 4 2 206 0 1 4 3 81 ; ## first stage DATA first; SET afqt; stage=1; advance = ed GE 2; RUN; ## second stage DATA second; SET afqt; stage=2; IF ed=1 THEN DELETE; advance = ed EQ 3; RUN; ## concatenated into a single set DATA concat; SET first second; RUN; ## Alternatively DATA combined; SET afqt; stage=1; advance = ed GE 2; OUTPUT; stage=2; IF ed=1 THEN DELETE; advance = ed EQ 3; OUTPUT; RUN; ## estimate the model with PROC LOGISTIC: PROC LOGISTIC DATA=combined; FREQ count; CLASS faed / PARAM=REF; MODEL advance(EVENT=&#39;1&#39;)=stage white old faed / AGGREGATE SCALE=NONE; RUN;    --(probit or complementary log-log functions)Cox "]]
