[["glmm-and-gam.html", "Chapter 20 GLMM and GAM 20.1 Genaralised linear model 20.2 Generalized Linear (Mixed) Model 20.3 Generalized Linear Model 20.4 Generalized Additive Models", " Chapter 20 GLMM and GAM 20.1 Genaralised linear model 20.1.1 Review Probability distribution for GLM is \\[ f_{Y}(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\tau)=h(\\mathbf{y}, \\tau) \\exp \\left(\\frac{\\mathbf{b}(\\boldsymbol{\\theta})^{\\mathrm{T}} \\mathbf{T}(\\mathbf{y})-A(\\boldsymbol{\\theta})}{d(\\tau)}\\right) \\] Expectation and variance \\[ \\begin{aligned} E(Y) &amp;=\\mu=b^{\\prime}(\\theta) \\\\ \\operatorname{Var}(Y) &amp;=V(\\mu) a(\\psi)=b^{\\prime \\prime}(\\theta) a(\\psi) \\end{aligned} \\]  \\(b(\\theta)\\)  \\(b^{\\prime}(\\theta)\\),  \\(\\mu=G(\\eta)=b^{\\prime}(\\theta)\\)  \\(\\theta\\)  \\(\\theta(\\mu)\\)  There are two assumptions Distribution assumption \\[ \\qquad f\\left(y{i} \\mid \\theta{i}\\right)=\\exp \\left(\\frac{y{i} \\theta{i}-b\\left(\\theta{i}\\right)}{\\phi} \\omega{i}-c\\left(y{i}, \\phi, \\omega{i}\\right)\\right) \\] and for \\(\\mathrm{E}\\left(y{i} \\mid \\boldsymbol{x}{i}\\right)=\\mu{i}\\) and \\(\\operatorname{Var}\\left(y{i} \\mid \\boldsymbol{x}_{i}\\right)\\) there are \\[ \\mathrm{E}\\left(y_{i} \\mid \\boldsymbol{x}_{i}\\right)=\\mu_{i}=b^{\\prime}\\left(\\theta_{i}\\right), \\quad \\operatorname{Var}\\left(y_{i} \\mid \\boldsymbol{x}_{i}\\right)=\\sigma_{i}^{2}=\\phi b^{\\prime \\prime}\\left(\\theta_{i}\\right) / \\omega_{i} \\] Structural assumption: The (conditional) expected value \\(\\mu i\\) is with the linear predictor \\(\\eta i=\\boldsymbol{x} i^{\\prime} \\boldsymbol{\\beta }=\\beta _{0}+\\beta _{1} x_ {i 1}+\\ldots+\\beta_{k} x_{i k}\\) by \\[ \\mu_{i}=h\\left(\\eta_{i}\\right)=h\\left(\\mathbf{x}_{i}^{\\prime} \\mathbf{\\beta}\\right) \\quad b z w . \\quad \\eta_{i}=g\\left(\\mu_{i}\\right) \\] linked, where h is a (one-to-one and twice differentiable) response function and g is the link function, i.e.Â the inverse function \\(g=h^{-1}\\) of \\(h\\). 20.1.2 Exponential Families Figure 20.1: Figure: 20.1.3 Testing linear Hypotheses For testing linear hypotheses: \\[ H_{0}: \\boldsymbol{C} \\boldsymbol{\\beta}=\\boldsymbol{d} \\quad \\text { against } \\quad H_{1}: \\boldsymbol{C} \\boldsymbol{\\beta } \\neq \\ boldsymbol{d} \\] test statistics 1. Likelihood-ratio statistic: \\(l q=-2 l(\\tilde{\\mathbf{\\beta }})-l(\\hat{\\mathbf{\\beta }})\\). 2. Forest statistic: \\(W=(\\boldsymbol{C} \\hat{\\boldsymbol{\\beta}}-\\boldsymbol{d})^{\\prime}\\left[\\boldsymbol{C} \\boldsymbol{F} ^{-1}(\\hat{\\boldsymbol{\\beta}}) \\boldsymbol{C}^{\\prime}\\right]^{-1}(\\boldsymbol{C} \\hat{\\boldsymbol{\\beta } }-\\boldsymbol{d})\\) 3. Score statistics: \\(u=\\mathbf{s}^{\\prime}(\\tilde{\\mathbf{\\beta }}) \\mathbf{F}^{-1}(\\tilde{\\mathbf{\\beta }} }}) \\mathbf{s}(\\tilde{\\mathbf{\\beta}})\\). Here \\(\\tilde{\\mathbf{\\beta}}\\) is the ML estimator under the restriction \\(H_{0}\\). Test decisions For large n under \\(H_{0}\\) approximately: \\[ l q, w, u \\stackrel{a}{\\sim} \\chi_{r}^{2} \\] where r is the rank of \\(C \\cdot H_{0}\\) is rejected if \\[ l q, w, u&gt;\\chi_{r}^{2}(1-) \\] 20.1.4 Maximum Likelihood Estimation in GLM The ML estimator \\(\\hat{\\beta}\\) maximizes the (log) likelihood and is used as the zero \\[ \\boldsymbol{s}(\\hat{\\boldsymbol{\\beta }})=\\mathbf{0} \\] for the score function \\[ \\boldsymbol{s}(\\boldsymbol{\\beta})=\\sum \\boldsymbol{x}_{i} \\frac{d_{i}}{\\sigma _{i}^{2}}\\left(y_{i }-\\mu_{i}\\right)=\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{\\Sigma }^{-1}(\\boldsymbol{y}-\\boldsymbol{\\mu } ) \\] The Fisher matrix is \\[ \\boldsymbol{F}(\\boldsymbol{\\beta })=\\sum \\boldsymbol{x}_{i} \\boldsymbol{x}_{i}^{\\prime} w_{i}=\\boldsymbol{X}^ {\\prime} \\boldsymbol{W} \\boldsymbol{X} \\] with \\(w{i}=d{i}^{2} / \\sigma{i}^{2}\\) and the weight matrix \\(\\boldsymbol{W}=\\operatorname{diag}\\left(w{1}, \\ldots, w_{n}\\right)\\) ML-estimator \\(\\hat{\\boldsymbol{\\beta}}\\) is calculated iteratively by Fisher scoring in the form of an iteratively weighted KQ-estimate \\[ \\hat{\\boldsymbol{\\beta}}^{(k+1)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{W}^{(k)} \\boldsymbol{X}\\right )^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{W}^{(k)} \\tilde{\\boldsymbol{y}}^{(k)}, \\quad k=0.1 ,2, \\ldots \\] With the vactor \\(\\tilde{\\boldsymbol{y}}^{(k)}=\\left(\\ldots, \\tilde{y}{i}\\left(\\hat{\\boldsymbol{\\beta}}^{(k)}\\right), \\ldots\\right)^{\\prime}\\) \\[\\tilde{y}{i}\\left(\\hat{\\boldsymbol{\\beta}}^{(k)}\\right)=\\boldsymbol{x}{i}^{\\prime} \\hat{\\boldsymbol{\\beta}}^{(k)}+d{i}^{-1}\\left(\\hat{\\boldsymbol{\\beta}}^{(k)}\\right)\\left(y{i}-\\hat{\\mu}{i}\\left(\\hat{\\boldsymbol{\\beta}}^{(k)}\\right)\\right)\\] and \\[\\text{und} \\ \\ \\ \\boldsymbol{W}^{(k)}=\\boldsymbol{W}\\left(\\hat{\\boldsymbol{\\beta}}^{(k)}\\right)\\] 20.1.5 IRLS Algorithm for Estimating GLM When looking at GLMs from a historical context, there are three important data-fitting procedures which are closely connected: Newton-Raphson Fisher Scoring Iteratively Reweighted Least Squares (IRLS) () Least Squares \\[ \\begin{gathered} f(\\theta)=\\frac{1}{2}\\|X \\theta-Y\\|^{2} \\\\ f(\\theta)=\\frac{1}{2}\\left(\\theta^{T} X^{T} X \\theta-\\theta^{T} X^{T} Y-Y^{T} X \\theta+Y^{T} Y\\right) \\\\ \\frac{\\partial f(\\theta))}{\\partial \\theta}=X^{T} X \\theta-X^{T} Y=0 \\\\ \\theta=\\left(X^{T} X\\right)^{-1} X^{T} Y \\end{gathered} \\] Weighted least squares: .  \\[ W=\\left[\\begin{array}{cccc} w_{11} &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; w_{22} &amp; \\ldots &amp; 0 \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; 0 &amp; \\ldots &amp; w_{m m} \\end{array}\\right] \\] : \\(f(\\theta)=\\frac{1}{2}\\|W(X \\theta-Y)\\|^{2}\\) \\[ f(\\theta)=\\frac{1}{2}\\left(W \\theta^{T} X^{T} A \\theta-W \\theta^{T} A^{T} Y-W Y^{T} A \\theta+W Y^{T} Y\\right) \\] \\[ \\frac{\\partial f(\\theta)}{\\partial \\theta}=-X^{T} W^{T} W Y+X^{T} W^{T} W X \\theta=0 \\] \\[ \\theta=\\left(X^{T} W^{T} W X\\right)^{-1} X^{T} W^{T} W Y \\] IRLS IRLSrobust  Burrus C S. Iterative re-weighted least squares[J]. Comm.pure Appl.math, 2009, 44(6):1-9. 20.2 Generalized Linear (Mixed) Model 20.2.1 logistic model with fixed and random effectss A plain ANOVA is inappropriate with a categorical response variable. The model assumptions are violated (variance is heteroscedastic, whereas ANOVA assumes homoscedasticity). This leads to invalid results (spurious null results and significances). An ANOVA can perform poorly even if transformations of the response are performed. At any rate, there is no reason to use this technique: cheap computing makes use of a transformed ANOVA unnecessary.  ANOVA  ANOVA ANOVA  ANOVA  ## Speaker Modality Verb SemanticClass LengthOfRecipient ## S1104 : 40 spoken :2360 give :1666 a:1433 Min. : 1.000 ## S1083 : 30 written: 903 pay : 207 c: 405 1st Qu.: 1.000 ## S1151 : 30 sell : 206 f: 59 Median : 1.000 ## S1139 : 29 send : 172 p: 228 Mean : 1.842 ## S1019 : 28 cost : 169 t:1138 3rd Qu.: 2.000 ## (Other):2203 tell : 128 Max. :31.000 ## NA&#39;s : 903 (Other): 715 ## AnimacyOfRec DefinOfRec PronomOfRec LengthOfTheme ## animate :3024 definite :2775 nonpronominal:1229 Min. : 1.000 ## inanimate: 239 indefinite: 488 pronominal :2034 1st Qu.: 2.000 ## Median : 3.000 ## Mean : 4.272 ## 3rd Qu.: 5.000 ## Max. :46.000 ## ## AnimacyOfTheme DefinOfTheme PronomOfTheme RealizationOfRecipient ## animate : 74 definite : 929 nonpronominal:2842 NP:2414 ## inanimate:3189 indefinite:2334 pronominal : 421 PP: 849 ## ## ## ## ## ## AccessOfRec AccessOfTheme ## accessible: 615 accessible:1742 ## given :2302 given : 502 ## new : 346 new :1019 ## ## ## ## ## , , AnimacyOfTheme = animate ## ## AnimacyOfRec ## RealizationOfRecipient animate inanimate ## NP 21 1 ## PP 47 5 ## ## , , AnimacyOfTheme = inanimate ## ## AnimacyOfRec ## RealizationOfRecipient animate inanimate ## NP 2278 114 ## PP 678 119 20.3 Generalized Linear Model 20.4 Generalized Additive Models 20.4.1 Concept In essence, a GAM is a GLM. What distinguishes it from the ones you know is that, unlike a standard GLM, it is composed of a sum of smooth functions of covariates instead of or in addition to the standard linear covariate effects. For the GAM, we can specify it generally as follows: \\[y = f(x) + \\epsilon\\] \\[y = f(x) + \\epsilon = \\sum_{j=1}^{d}B_j(x)\\gamma_j + \\epsilon\\] Above, each \\(B_j\\) is a basis function that is the transformed \\(x\\) depending on the type of basis considered, and the \\(y\\) are the corresponding regression coefficients. ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Overall ~ s(Income, bs = &quot;cr&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 470.444 4.082 115.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Income) 6.895 7.741 16.67 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.7 Deviance explained = 73.9% ## GCV = 1053.7 Scale est. = 899.67 n = 54 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
