[["knn.html", "Chapter 32 KNN 32.1 Introduction 32.2 KNN 32.3 Application", " Chapter 32 KNN 32.1 Introduction : :   KKNN SVM  .    KKNNSVM.    K(k-Nearest NeighborKNN)k()kk = 55 5 k k k  AB  KNNk(weight)K  K Support Vector Machine, SVMsupervised learninggeneralized linear classifiermaximum-margin hyperplane  SVM    SVM SVM SVM . SVM:     xixjc:    sigmod 32.1.1 Idee KNN. KNN KNNK k()kk = 555 KNNK 32.1.2  (weight) K  K 32.1.3 KNN KNN k   () 32.1.3.1 k k kK kK 32.1.3.2   AB  \\[D(x,y) =|x_1-y_1| + |x_2-y_2| + ... + |x_n-y_n| =\\sum\\limits_{i=1}^{n}|x_i-y_i|\\] (Minkowski Distance) \\[D(x,y) =\\sqrt[p]{(|x_1-y_1|)^p + (|x_2-y_2|)^p + ... + (|x_n-y_n|)^p} =\\sqrt[p]{\\sum\\limits_{i=1}^{n}(|x_i-y_i|)^p}\\] p=2p=1 32.1.4  KNN     KNNKNN  KNN   KD  KNN 32.2 KNN (brute-force)KD(KDTree)(BallTree)BBFMVP https://www.cnblogs.com/pinard/p/6061661.html 32.2.1 Brute-force kk  : KD 32.2.2 KD KDKDKDKKKNNKKNNKKKDKn    32.2.2.1 KD KDmnnk\\[n_k\\]\\[n_k\\]\\[n_{vk}\\]\\[k\\]\\[n_{vk}\\]\\[k\\]\\[n_{vk}\\]KD 6{(2,3)(5,4)(9,6)(4,7)(8,1)(7,2)}kd 16xy6.975.37x1 27,2x6()77,27,2x=7 3 x=7x&lt;=73={(2,3),(5,4),(4,7)}2={(9,6)(8,1)} 4{(2,3),(5,4),(4,7)}{(9,6)(8,1)}KD KD Figure 32.1: KD tree 32.2.2.2 KD KDKD, KD (2,4.5): 7,25,4y = 4y4.54,7&lt;(7,2)(5,4)(4,7)&gt; 4,73.2025,43.0415,4 24.53.041y = 45,42,3&lt;(7,2)(2,3)&gt; 2,32,32,4.55,4231.5 5,47,22,4.51.5x = 72,31.5 32.2.2.3 KD KDkKKNNKKNNK 32.2.3  KDKNN,  KD 32.2.3.1  KD 1)  2) KD 3)2).  KDKDKD 32.2.3.2  KD  KD 32.3 Application 32.3.1 Data Preparation 5328 1Yes/No 30 95%     2010  20124900  532MASSRPima.trPima.te  : npreg glu bpmm Hg skinmm bmi ped age type/ data(Pima.tr) str(Pima.tr) ## &#39;data.frame&#39;: 200 obs. of 8 variables: ## $ npreg: int 5 7 5 0 0 5 3 1 3 2 ... ## $ glu : int 86 195 77 165 107 97 83 193 142 128 ... ## $ bp : int 68 70 82 76 60 76 58 50 80 78 ... ## $ skin : int 28 33 41 43 25 27 31 16 15 37 ... ## $ bmi : num 30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 ... ## $ ped : num 0.364 0.163 0.156 0.259 0.133 ... ## $ age : int 24 55 35 26 23 52 25 24 63 31 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 1 1 2 ... data(Pima.te) str(Pima.te) ## &#39;data.frame&#39;: 332 obs. of 8 variables: ## $ npreg: int 6 1 1 3 2 5 0 1 3 9 ... ## $ glu : int 148 85 89 78 197 166 118 103 126 119 ... ## $ bp : int 72 66 66 50 70 72 84 30 88 80 ... ## $ skin : int 35 29 23 32 45 19 47 38 41 35 ... ## $ bmi : num 33.6 26.6 28.1 31 30.5 25.8 45.8 43.3 39.3 29 ... ## $ ped : num 0.627 0.351 0.167 0.248 0.158 0.587 0.551 0.183 0.704 0.263 ... ## $ age : int 50 31 21 26 53 51 31 33 27 29 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 2 2 1 1 2 ... pima &lt;- rbind(Pima.tr, Pima.te) str(pima) ## &#39;data.frame&#39;: 532 obs. of 8 variables: ## $ npreg: int 5 7 5 0 0 5 3 1 3 2 ... ## $ glu : int 86 195 77 165 107 97 83 193 142 128 ... ## $ bp : int 68 70 82 76 60 76 58 50 80 78 ... ## $ skin : int 28 33 41 43 25 27 31 16 15 37 ... ## $ bmi : num 30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 ... ## $ ped : num 0.364 0.163 0.156 0.259 0.133 ... ## $ age : int 24 55 35 26 23 52 25 24 63 31 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 1 1 2 ... ## &quot;type&quot;ID melt() ## facet_wrap() pima.melt &lt;- melt(pima, id.var = &quot;type&quot;) ggplot(data = pima.melt, aes(x = type, y = value)) + geom_boxplot() + facet_wrap(~ variable, ncol = 2) ## ,  Y  ## Rscale()01 , scale() as.data.frame() ##  type pima.scale &lt;- data.frame(scale(pima[, -8])) #scale.pima = as.data.frame(scale(pima[,1:7], byrow=FALSE)) #do not create own function str(pima.scale) ## &#39;data.frame&#39;: 532 obs. of 7 variables: ## $ npreg: num 0.448 1.052 0.448 -1.062 -1.062 ... ## $ glu : num -1.13 2.386 -1.42 1.418 -0.453 ... ## $ bp : num -0.285 -0.122 0.852 0.365 -0.935 ... ## $ skin : num -0.112 0.363 1.123 1.313 -0.397 ... ## $ bmi : num -0.391 -1.132 0.423 2.181 -0.943 ... ## $ ped : num -0.403 -0.987 -1.007 -0.708 -1.074 ... ## $ age : num -0.708 2.173 0.315 -0.522 -0.801 ... pima.scale$type &lt;- pima$type pima.scale.melt &lt;- melt(pima.scale, id.var = &quot;type&quot;) ggplot(data=pima.scale.melt, aes(x = type, y = value)) + geom_boxplot() + facet_wrap(~ variable, ncol = 2) ## Interpretation:  typeage ## npreg/ageskin/bmi  cor(pima.scale[-8]) ## npreg glu bp skin bmi ped ## npreg 1.000000000 0.1253296 0.204663421 0.09508511 0.008576282 0.007435104 ## glu 0.125329647 1.0000000 0.219177950 0.22659042 0.247079294 0.165817411 ## bp 0.204663421 0.2191779 1.000000000 0.22607244 0.307356904 0.008047249 ## skin 0.095085114 0.2265904 0.226072440 1.00000000 0.647422386 0.118635569 ## bmi 0.008576282 0.2470793 0.307356904 0.64742239 1.000000000 0.151107136 ## ped 0.007435104 0.1658174 0.008047249 0.11863557 0.151107136 1.000000000 ## age 0.640746866 0.2789071 0.346938723 0.16133614 0.073438257 0.071654133 ## age ## npreg 0.64074687 ## glu 0.27890711 ## bp 0.34693872 ## skin 0.16133614 ## bmi 0.07343826 ## ped 0.07165413 ## age 1.00000000 ##  YesNo  21 table(pima.scale$type) ## ## No Yes ## 355 177 ## 2170/30 set.seed(502) ind &lt;- sample(2, nrow(pima.scale), replace = TRUE, prob = c(0.7, 0.3)) train &lt;- pima.scale[ind == 1, ] test &lt;- pima.scale[ind == 2, ] str(train) ## &#39;data.frame&#39;: 385 obs. of 8 variables: ## $ npreg: num 0.448 0.448 -0.156 -0.76 -0.156 ... ## $ glu : num -1.42 -0.775 -1.227 2.322 0.676 ... ## $ bp : num 0.852 0.365 -1.097 -1.747 0.69 ... ## $ skin : num 1.123 -0.207 0.173 -1.253 -1.348 ... ## $ bmi : num 0.4229 0.3938 0.2049 -1.0159 -0.0712 ... ## $ ped : num -1.007 -0.363 -0.485 0.441 -0.879 ... ## $ age : num 0.315 1.894 -0.615 -0.708 2.916 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 2 1 1 1 ... str(test) ## &#39;data.frame&#39;: 147 obs. of 8 variables: ## $ npreg: num 0.448 1.052 -1.062 -1.062 -0.458 ... ## $ glu : num -1.13 2.386 1.418 -0.453 0.225 ... ## $ bp : num -0.285 -0.122 0.365 -0.935 0.528 ... ## $ skin : num -0.112 0.363 1.313 -0.397 0.743 ... ## $ bmi : num -0.391 -1.132 2.181 -0.943 1.513 ... ## $ ped : num -0.403 -0.987 -0.708 -1.074 2.093 ... ## $ age : num -0.7076 2.173 -0.5217 -0.8005 -0.0571 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 2 1 2 1 1 1 ... 32.3.2 KNN Modelling KNNkKk caretk2201 expand.grid()seq()caretKNN .k grid1 &lt;- expand.grid(.k = seq(2, 20, by = 1)) ## controlcaret trainControl() control = trainControl(method = &quot;cv&quot;) ## , carettrain()k ## train()(knn), k set.seed(123) knn.train &lt;- train(type ~ ., data = train, method = &quot;knn&quot;, trControl = control, tuneGrid = grid1) ## k17: The final value used for the model was k = 17. knn.train ## k-Nearest Neighbors ## ## 385 samples ## 7 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 345, 347, 347, 346, 347, 347, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 2 0.7291262 0.3579438 ## 3 0.7661606 0.4218716 ## 4 0.7688596 0.4318872 ## 5 0.7660324 0.4220202 ## 6 0.7657659 0.4140541 ## 7 0.7791161 0.4501050 ## 8 0.7713563 0.4371057 ## 9 0.7816835 0.4559734 ## 10 0.7868792 0.4661169 ## 11 0.7841835 0.4627038 ## 12 0.7736572 0.4406084 ## 13 0.7789845 0.4408541 ## 14 0.7712888 0.4150304 ## 15 0.7790520 0.4401016 ## 16 0.7869467 0.4552805 ## 17 0.7739238 0.4262446 ## 18 0.7686606 0.4104614 ## 19 0.7738596 0.4303754 ## 20 0.7791228 0.4437383 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 16. ## Interpretation ## Kappa  #  # Kappa KKappa  Kappa = ( )/(1 )  Kappa Kappa1 ## : Kappa knn.test &lt;- knn(train[, -8], test[, -8], train[, 8], k = 17) table(knn.test, test$type) ## ## knn.test No Yes ## No 77 26 ## Yes 16 28 ## :  (77+28)/147 ## [1] 0.7142857 ## calculate Kappa prob.agree &lt;- (77+28)/147 prob.chance &lt;- ((77+26)/147) * ((77+16)/147) prob.chance ## [1] 0.4432875 kappa &lt;- (prob.agree - prob.chance) / (1 - prob.chance) kappa ## [1] 0.486783 ## Kappa: 0.20 ; 0.21 ~ 0.40 ; 0.41 ~ 0.60 ; 0.61 ~ 0.80 ; 0.81 ~ 1.00  32.3.3     kknntrain.kknn() train.kknn()LOOCVK  Kkknn   , kknn10 retangulartriangularepanechnikovbiweighttriweightconsine inversiongaussianrankoptimal  01triangular1 epanechnikov3/4(1 ) ## triangular, epanechnikov ## kknn(), kkmaxdistance12kernel set.seed(123) kknn.train &lt;- train.kknn(type ~ ., data = train, kmax = 25, distance = 2, kernel = c(&quot;rectangular&quot;, &quot;triangular&quot;, &quot;epanechnikov&quot;)) ## plotXkY plot(kknn.train) ##  kknn.train ## ## Call: ## train.kknn(formula = type ~ ., data = train, kmax = 25, distance = 2, kernel = c(&quot;rectangular&quot;, &quot;triangular&quot;, &quot;epanechnikov&quot;)) ## ## Type of response variable: nominal ## Minimal misclassification: 0.212987 ## Best kernel: rectangular ## Best k: 19 ##   kknn.pred &lt;- predict(kknn.train, newdata = test) table(kknn.pred, test$type) ## ## kknn.pred No Yes ## No 76 27 ## Yes 17 27 "]]
