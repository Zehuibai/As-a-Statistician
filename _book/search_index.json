[["cluster-analysis.html", "Chapter 37 Cluster Analysis 37.1 Hierarchical Clustering 37.2 K-means Clustering 37.3 Gowers coefficient and PAM 37.4 BIRCH Clustering 37.5 Application", " Chapter 37 Cluster Analysis  k.  K   37.1 Hierarchical Clustering 37.1.1 Introduction   Agglomerative clustering    n 1 n 2  : Ward,  Complete linkage),  Centroid linkage),  dist(): ,   0 1  (Hierarchical Clustering) k k make a decision on the dissimilarity measure to use.  Each linkage method has different systematic tendencies (or biases) in the way it groups observations and can result in significantly different results.  Ward- not need to pre-specify the number of clusters, we often still need to decide where to cut the dendrogram in order to obtain the final clusters to use. 37.1.2 Hierarchical clustering algorithms Hierarchical clustering can be divided into two main types: Agglomerative clustering: Commonly referred to as AGNES (AGglomerative NESting) works in a bottom-up manner. That is, each observation is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are a member of just one single big cluster (root). The result is a tree that can be displayed using a dendrogram. Divisive hierarchical clustering: Commonly referred to as DIANA (DIvise ANAlysis) works in a top-down manner. DIANA is like the reverse of AGNES. It begins with the root, in which all observations are included in a single cluster. At each step of the algorithm, the current cluster is split into two clusters that are considered most heterogeneous. The process is iterated until all observations are in their own cluster. AGNES  leaf  nodes  root DIANADIvise ANAlysis DIANAAGNES    37.1.3 Measure the dissimilarity between two clusters of observations k; cluster agglomeration methods Linkage Maximum or complete linkage clustering: Computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters. Minimum or single linkage clustering: Computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long, loose clusters. Mean or average linkage clustering: Computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters. Can vary in the compactness of the clusters it creates. Centroid linkage clustering: Computes the dissimilarity between the centroid for cluster 1 (a mean vector of length p, one element for each variable) and the centroid for cluster 2. Wards minimum variance method: Minimizes the total within-cluster variance. At each step the pair of clusters with the smallest between-cluster distance are merged. Tends to produce more compact clusters. 12 12 12 12 p  Ward 37.2 K-means Clustering kkkKNN K-MeansKNNKNNkkK-Meansk(nearest neighbors) K-MeansK \\((C_1,C_2,...C_k)\\) \\(E\\) \\[ E = \\sum\\limits_{i=1}^k\\sum\\limits_{x \\in C_i} ||x-\\mu_i||_2^2 \\] \\(u_i\\)\\(C_i\\) \\[ \\mu_i = \\frac{1}{|C_i|}\\sum\\limits_{x \\in C_i}x \\] \\(k=2\\)k 37.2.1 Algorithm K k  : 1. :(k) 2. :k 3. : ()k; ;  1  K-Meanskkkkk ()kk  :  \\(D=\\left\\{x_{1}, x_{2}, \\ldots x_{m}\\right\\}\\),k,N  \\(C=\\left\\{C_{1}, C_{2}, \\ldots C_{k}\\right\\}\\) Dkk \\(\\left\\{\\mu_{1}, \\mu_{2}, \\ldots, \\mu_{k}\\right\\}\\)  \\(n=1,2, \\ldots, N\\) C \\(C_{t}=\\varnothing t=1,2 \\ldots k\\)  \\(i=1,2 \\ldots\\),  \\(x_{i}\\)  \\(\\mu_{j}(j=1,2, \\ldots k)\\) : \\(d_{i j}=\\left\\|x_{i}-\\mu_{j}\\right\\|_{2}^{2},\\)  \\(x_{i}\\)  \\(d_{i j}\\)  \\(\\lambda_{i_{\\circ}}\\)   \\(C_{\\lambda_{i}}=C_{\\lambda_{i}} \\cup\\left\\{x_{i}\\right\\}\\)  \\(j=1,2, \\ldots, k,\\)  \\(C_{j}\\)  \\(\\mu_{j}=\\frac{1}{\\left|C_{j}\\right|} \\sum_{x \\in C_{j}} x\\) k, 3)  \\(C=\\left\\{C_{1}, C_{2}, \\ldots C_{k}\\right\\}\\) 37.2.2 K-Means++ kK-Means++K-Means K-Means++  \\(\\mu_{1}\\)  \\(x_{i},\\)  \\[ D\\left(x_{i}\\right)=\\arg \\min \\left\\|x_{i}-\\mu_{r}\\right\\|_{2}^{2} r=1,2, \\ldots k_{\\text {selected }} \\] , : \\(D(x)\\) ,  bck kK-Means 37.2.3 elkan K-Means K-Meanselkan K-Means elkan K-Means,,   \\(x\\)  \\(\\mu_{j_{1}}, \\mu_{j_{2} \\circ}\\)  \\(D\\left(j_{1}, j_{2}\\right),\\)  \\(2 D\\left(x, j_{1}\\right) \\leq D\\left(j_{1}, j_{2}\\right)\\), \\(D\\left(x, j_{1}\\right) \\leq D\\left(x, j_{2}\\right)\\)  \\(D\\left(x, j_{2}\\right)\\),  \\(x\\)  \\(\\mu_{j_{1}}, \\mu_{j_{2} \\circ}\\)  \\(D\\left(x, j_{2}\\right) \\geq \\max \\left\\{0, D\\left(x, j_{1}\\right)-D\\left(j_{1}, j_{2}\\right)\\right\\}_{\\circ}\\)  elkan K-MeansK-Means 37.2.4 Mini Batch K-Means K-Means10100K-Meanselkan K-MeansMini Batch K-Means Mini BatchK-Means Mini Batch K-Meansbatch sizebatch sizeK-Meansbatch sizeMini Batch K-Means 37.3 Gowers coefficient and PAM 37.3.1 Gowers coefficient  K()    ,   PAMK, PAMK,  1. PAM 2. PAM  ij \\[S_{ij} = sum(W_{ijk} * S_{ijk}) / sum(W_{ijk})\\] \\(S_{ijk}\\)kk\\(W_{ijk}\\)10 37.3.2  () M 37.3.2.1  a=matrix(rnorm(15,0,1),c(3,5)) a ## [,1] [,2] [,3] [,4] [,5] ## [1,] -1.1234296 -0.2182115 -0.02421158 0.5951353 -0.40230683 ## [2,] 0.6779165 -1.2521680 -0.67549420 0.8304810 0.77204471 ## [3,] -1.0334835 -0.1518533 1.20846267 -0.7415417 0.03816229 dist(a,p=2) ## 1 2 ## 2 2.484466 ## 3 1.874219 3.270889 ## 2.1747103.966592 37.3.2.2  8(x1,y1)(x2,y2) a=matrix(rnorm(15,0,1),c(3,5)) a ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.7522117 -0.08803036 1.2171604 -2.08185053 0.7085528 ## [2,] -2.1251094 -0.02846483 -0.2718205 -0.68150294 0.2334100 ## [3,] 0.4746535 -0.88115223 2.4823226 0.09474935 -1.1422181 dist(a,&quot;maximum&quot;) ## 1 2 ## 2 2.877321 ## 3 2.176600 2.754143 37.3.2.3  (City Block distance) aa=matrix(rnorm(15,0,1),c(3,5)) dist(aa,&quot;manhattan&quot;) ## 1 2 ## 2 6.587407 ## 3 5.221523 2.187232 37.3.2.4   aa=matrix(rnorm(15,0,1),c(3,5)) dist(aa, method = &quot;canberra&quot;) ## 1 2 ## 2 3.023238 ## 3 2.786321 3.833127 37.3.3 PAM () K55 PAM: 1. k; 2. ; 3. ; 4. ; 5. (2)~(4) PAMRclusterdaisy() pam() 37.4 BIRCH Clustering 37.4.1 BIRCH Introduction BIRCHBalanced Iterative Reducing and Clustering Using Hierarchies. BIRCHK. BIRCHB+(Clustering Feature TreeCF Tree)(Clustering FeatureCF)CFCF 37.4.2 CFCF Tree CFCFNLSSSNCFLSCFSSCF CF TreeCF5(3,4), (2,6), (4,5), (4,7), (3,8)N=5 \\(LS=(3+2+4+4+3, 4+6+5+7+8) = (16,30)\\)$ , $\\(SS=(3^2+2^2+4^2 +4^2+3^2 + 4^2+6^2+5^2 +7^2+8^2) = (54 + 190) = 244\\) CF \\(CF1+CF2 = (N_1+N_2, LS_1+LS_2, SS_1 +SS_2)\\) CF TreeCF(N,LS,SS)CF CF TreeCFBCFLCFCFTCFT 37.4.3 CF Tree CF CFTCF3. CFLCFCFCF4 CFCFCF 37.4.4 BIRCH CF TreeBIRCHCFBIRCHCF Tree() CF Tree CF TreeCF K-MeansCFCF Tree.CF CF TreeCFCF Tree 37.5 Application 37.5.1 Data preparation library(cluster) # conduct cluster analysis library(compareGroups) # build descriptive statistic tables library(HDclassif) # contains the dataset library(NbClust) # cluster validity measures library(sparcl) # colored dendrogram ## HDclassif ## 17813Class  data(wine) str(wine) ## &#39;data.frame&#39;: 178 obs. of 14 variables: ## $ class: int 1 1 1 1 1 1 1 1 1 1 ... ## $ V1 : num 14.2 13.2 13.2 14.4 13.2 ... ## $ V2 : num 1.71 1.78 2.36 1.95 2.59 1.76 1.87 2.15 1.64 1.35 ... ## $ V3 : num 2.43 2.14 2.67 2.5 2.87 2.45 2.45 2.61 2.17 2.27 ... ## $ V4 : num 15.6 11.2 18.6 16.8 21 15.2 14.6 17.6 14 16 ... ## $ V5 : int 127 100 101 113 118 112 96 121 97 98 ... ## $ V6 : num 2.8 2.65 2.8 3.85 2.8 3.27 2.5 2.6 2.8 2.98 ... ## $ V7 : num 3.06 2.76 3.24 3.49 2.69 3.39 2.52 2.51 2.98 3.15 ... ## $ V8 : num 0.28 0.26 0.3 0.24 0.39 0.34 0.3 0.31 0.29 0.22 ... ## $ V9 : num 2.29 1.28 2.81 2.18 1.82 1.97 1.98 1.25 1.98 1.85 ... ## $ V10 : num 5.64 4.38 5.68 7.8 4.32 6.75 5.25 5.05 5.2 7.22 ... ## $ V11 : num 1.04 1.05 1.03 0.86 1.04 1.05 1.02 1.06 1.08 1.01 ... ## $ V12 : num 3.92 3.4 3.17 3.45 2.93 2.85 3.58 3.58 2.85 3.55 ... ## $ V13 : int 1065 1050 1185 1480 735 1450 1290 1295 1045 1045 ... names(wine) &lt;- c(&quot;Class&quot;, &quot;Alcohol&quot;, &quot;MalicAcid&quot;, &quot;Ash&quot;, &quot;Alk_ash&quot;, &quot;magnesium&quot;, &quot;T_phenols&quot;, &quot;Flavanoids&quot;, &quot;Non_flav&quot;, &quot;Proantho&quot;, &quot;C_Intensity&quot;, &quot;Hue&quot;, &quot;OD280_315&quot;, &quot;Proline&quot;) names(wine) ## [1] &quot;Class&quot; &quot;Alcohol&quot; &quot;MalicAcid&quot; &quot;Ash&quot; &quot;Alk_ash&quot; ## [6] &quot;magnesium&quot; &quot;T_phenols&quot; &quot;Flavanoids&quot; &quot;Non_flav&quot; &quot;Proantho&quot; ## [11] &quot;C_Intensity&quot; &quot;Hue&quot; &quot;OD280_315&quot; &quot;Proline&quot; df &lt;- as.data.frame(scale(wine[, -1])) str(df) ## &#39;data.frame&#39;: 178 obs. of 13 variables: ## $ Alcohol : num 1.514 0.246 0.196 1.687 0.295 ... ## $ MalicAcid : num -0.5607 -0.498 0.0212 -0.3458 0.2271 ... ## $ Ash : num 0.231 -0.826 1.106 0.487 1.835 ... ## $ Alk_ash : num -1.166 -2.484 -0.268 -0.807 0.451 ... ## $ magnesium : num 1.9085 0.0181 0.0881 0.9283 1.2784 ... ## $ T_phenols : num 0.807 0.567 0.807 2.484 0.807 ... ## $ Flavanoids : num 1.032 0.732 1.212 1.462 0.661 ... ## $ Non_flav : num -0.658 -0.818 -0.497 -0.979 0.226 ... ## $ Proantho : num 1.221 -0.543 2.13 1.029 0.4 ... ## $ C_Intensity: num 0.251 -0.292 0.268 1.183 -0.318 ... ## $ Hue : num 0.361 0.405 0.317 -0.426 0.361 ... ## $ OD280_315 : num 1.843 1.11 0.786 1.181 0.448 ... ## $ Proline : num 1.0102 0.9625 1.3912 2.328 -0.0378 ... table(wine$Class) ## ## 1 2 3 ## 59 71 48 37.5.2 Hierarchical Clustering Rstatshclust() :dist() hclust() 305CHDudaCindexGamma Bealegap RNbClustNbClust()23 MiliganCooper5gap  numComplete26  Hubert:  ;.33 Dindex: numComplete &lt;- NbClust(df, distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 6, method = &quot;complete&quot;, index = &quot;all&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 1 proposed 2 as the best number of clusters ## * 11 proposed 3 as the best number of clusters ## * 6 proposed 5 as the best number of clusters ## * 5 proposed 6 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* ##   ## (KL)5(CH)3 numComplete$Best.nc ## KL CH Hartigan CCC Scott Marriot TrCovW ## Number_clusters 5.0000 3.0000 3.0000 5.000 3.0000 3.000000e+00 3.00 ## Value_Index 14.2227 48.9898 27.8971 1.148 340.9634 6.872632e+25 22389.83 ## TraceW Friedman Rubin Cindex DB Silhouette Duda ## Number_clusters 3.0000 3.0000 5.0000 3.0000 6.0000 3.0000 5.0000 ## Value_Index 256.4861 10.6941 -0.1489 0.3551 1.6018 0.2038 0.8856 ## PseudoT2 Beale Ratkowsky Ball PtBiserial Frey McClain ## Number_clusters 5.0000 5.0000 3.0000 3.0000 6.0000 1 2.0000 ## Value_Index 6.3314 1.1253 0.3318 462.0304 0.5877 NA 0.7687 ## Dunn Hubert SDindex Dindex SDbw ## Number_clusters 6.0000 0 6.0000 0 6.0000 ## Value_Index 0.1892 0 0.9951 0 0.5031 ## 3 dis &lt;- dist(df, method = &quot;euclidean&quot;) hc &lt;- hclust(dis, method = &quot;complete&quot;) ## plothang=-1  ## ()  plot(hc, hang = -1,labels = FALSE, main = &quot;Complete-Linkage&quot;) ## sparcl cutree() : comp3 &lt;- cutree(hc, 3) ColorDendrogram(hc, y = comp3, main = &quot;Complete&quot;, branchlength = 50) ## branchlength=50  table(comp3) ## comp3 ## 1 2 3 ## 69 58 51 table(comp3, wine$Class) ## ## comp3 1 2 3 ## 1 51 18 0 ## 2 8 50 0 ## 3 0 3 48 ## 84% (51+50+48)/178 ## [1] 0.8370787 ## Wardmethod Ward.D2 numWard &lt;- NbClust(df, diss = NULL, distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 6, method= &quot;ward.D2&quot;, index = &quot;all&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 2 proposed 2 as the best number of clusters ## * 18 proposed 3 as the best number of clusters ## * 2 proposed 6 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* hcWard &lt;- hclust(dis, method = &quot;ward.D2&quot;) plot(hcWard, hang = -1, labels = FALSE, main = &quot;Ward&#39;s-Linkage&quot;) ## 3  ward3 &lt;- cutree(hcWard, 3) table(ward3, wine$Class) ## ## ward3 1 2 3 ## 1 59 5 0 ## 2 0 58 0 ## 3 0 8 48 ##  table(comp3, ward3) ## ward3 ## comp3 1 2 3 ## 1 53 11 5 ## 2 11 47 0 ## 3 0 0 51 ##   par(mfrow = c(1, 2)) boxplot(wine$Proline ~ comp3, main = &quot;Proline by Complete Linkage&quot;) boxplot(wine$Proline ~ ward3, main = &quot;Proline by Ward&#39;s Linkage&quot;) ## Ward  37.5.3 K-means Clustering ## methodkmeans15 numKMeans &lt;- NbClust(df, min.nc = 2, max.nc = 15, method = &quot;kmeans&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 2 proposed 2 as the best number of clusters ## * 19 proposed 3 as the best number of clusters ## * 1 proposed 14 as the best number of clusters ## * 1 proposed 15 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* ## 3 set.seed(1234) km &lt;- kmeans(df, 3, nstart = 25) table(km$cluster) ## ## 1 2 3 ## 62 65 51 ##  K : km$centers ## Alcohol MalicAcid Ash Alk_ash magnesium T_phenols ## 1 0.8328826 -0.3029551 0.3636801 -0.6084749 0.57596208 0.88274724 ## 2 -0.9234669 -0.3929331 -0.4931257 0.1701220 -0.49032869 -0.07576891 ## 3 0.1644436 0.8690954 0.1863726 0.5228924 -0.07526047 -0.97657548 ## Flavanoids Non_flav Proantho C_Intensity Hue OD280_315 ## 1 0.97506900 -0.56050853 0.57865427 0.1705823 0.4726504 0.7770551 ## 2 0.02075402 -0.03343924 0.05810161 -0.8993770 0.4605046 0.2700025 ## 3 -1.21182921 0.72402116 -0.77751312 0.9388902 -1.1615122 -1.2887761 ## Proline ## 1 1.1220202 ## 2 -0.7517257 ## 3 -0.4059428 37.5.4 Gowers coefficient and PAM ##  :/ wine$Alcohol &lt;- as.factor(ifelse(df$Alcohol &gt; 0, &quot;High&quot;, &quot;Low&quot;)) ## clusterdaisy() disMatrix &lt;- daisy(wine[, -1], metric = &quot;gower&quot;) set.seed(123) pamFit &lt;- pam(disMatrix, k = 3) table(pamFit$clustering) ## ## 1 2 3 ## 62 71 45 table(pamFit$clustering, wine$Class) ## ## 1 2 3 ## 1 57 5 0 ## 2 2 64 5 ## 3 0 2 43 wine$cluster &lt;- pamFit$clustering ## compareGroups group &lt;- compareGroups(cluster ~ ., data = wine) clustab &lt;- createTable(group) clustab ## ## --------Summary descriptives table by &#39;cluster&#39;--------- ## ## _________________________________________________________ ## 1 2 3 p.overall ## N=62 N=71 N=45 ## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ ## Class 1.08 (0.27) 2.04 (0.31) 2.96 (0.21) &lt;0.001 ## Alcohol: &lt;0.001 ## High 62 (100%) 1 (1.41%) 29 (64.4%) ## Low 0 (0.00%) 70 (98.6%) 16 (35.6%) ## MalicAcid 2.00 (0.83) 1.95 (0.91) 3.41 (1.09) &lt;0.001 ## Ash 2.42 (0.27) 2.28 (0.30) 2.44 (0.18) 0.002 ## Alk_ash 17.2 (2.75) 20.2 (3.20) 21.6 (2.26) &lt;0.001 ## magnesium 105 (11.7) 95.6 (16.8) 99.1 (10.8) 0.001 ## T_phenols 2.83 (0.36) 2.20 (0.56) 1.71 (0.37) &lt;0.001 ## Flavanoids 2.96 (0.42) 1.99 (0.76) 0.81 (0.31) &lt;0.001 ## Non_flav 0.29 (0.07) 0.36 (0.12) 0.46 (0.12) &lt;0.001 ## Proantho 1.89 (0.43) 1.60 (0.60) 1.18 (0.43) &lt;0.001 ## C_Intensity 5.44 (1.29) 3.17 (1.01) 7.51 (2.36) &lt;0.001 ## Hue 1.07 (0.13) 1.03 (0.21) 0.69 (0.13) &lt;0.001 ## OD280_315 3.12 (0.36) 2.75 (0.58) 1.70 (0.26) &lt;0.001 ## Proline 1070 (279) 535 (167) 635 (119) &lt;0.001 ## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ # export2csv(clustab,file = &quot;wine_clusters.csv&quot;) # export2pdf(clustab, file = &quot;wine_clusters.pdf&quot;) "]]
